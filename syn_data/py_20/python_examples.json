{"label":{"0":"remove","1":"append","2":"append","3":"append","4":"remove","5":"append","6":"remove","7":"append","8":"remove","9":"append","10":"append","11":"append","12":"append","13":"append","14":"append","15":"append","16":"append","17":"append","18":"append","19":"append","20":"append","21":"remove","22":"remove","23":"append","24":"append","25":"remove","26":"append","27":"append","28":"append","29":"append","30":"append","31":"append","32":"append","33":"append","34":"append","35":"append","36":"remove","37":"append","38":"append","39":"append","40":"append","41":"remove","42":"append","43":"append","44":"append","45":"append","46":"append","47":"append","48":"append","49":"append","50":"append","51":"append","52":"append","53":"append","54":"append","55":"append","56":"append","57":"append","58":"remove","59":"append","60":"remove","61":"remove","62":"remove","63":"append","64":"remove","65":"append","66":"remove","67":"remove","68":"append","69":"remove","70":"remove","71":"append","72":"append","73":"append","74":"append","75":"append","76":"append","77":"append","78":"append","79":"append","80":"remove","81":"remove","82":"append","83":"remove","84":"append","85":"append","86":"append","87":"append","88":"append","89":"append","90":"append","91":"append","92":"remove","93":"remove","94":"append","95":"remove","96":"append","97":"append","98":"append","99":"append","100":"append","101":"append","102":"remove","103":"append","104":"remove","105":"append","106":"append","107":"remove","108":"append","109":"remove","110":"append","111":"remove","112":"append","113":"append","114":"append","115":"append","116":"remove","117":"append","118":"append","119":"append","120":"append","121":"append","122":"remove","123":"append","124":"remove","125":"append","126":"remove","127":"remove","128":"remove","129":"append","130":"remove","131":"remove","132":"remove","133":"append","134":"append","135":"append","136":"append","137":"append","138":"remove","139":"append","140":"append","141":"append","142":"append","143":"append","144":"append","145":"append","146":"append","147":"append","148":"append","149":"remove","150":"append","151":"remove","152":"append","153":"append","154":"append","155":"remove","156":"append","157":"remove","158":"append","159":"remove","160":"remove","161":"remove","162":"remove","163":"append","164":"append","165":"append","166":"append","167":"remove","168":"append","169":"remove","170":"remove","171":"append","172":"append","173":"append","174":"append","175":"append","176":"append","177":"append","178":"append","179":"append","180":"append","181":"remove","182":"append","183":"append","184":"append","185":"append","186":"append","187":"append","188":"append","189":"append","190":"append","191":"remove","192":"append","193":"append","194":"append","195":"append","196":"append","197":"remove","198":"append","199":"append","200":"remove","201":"remove","202":"append","203":"remove","204":"append","205":"remove","206":"remove","207":"remove","208":"append","209":"remove","210":"append","211":"remove","212":"remove","213":"remove","214":"remove","215":"append","216":"append","217":"append","218":"append","219":"append","220":"append","221":"append","222":"remove","223":"remove","224":"append","225":"remove","226":"append","227":"append","228":"append","229":"remove","230":"append","231":"remove","232":"append","233":"append","234":"append","235":"remove","236":"append","237":"append","238":"append","239":"append","240":"remove","241":"append","242":"append","243":"remove","244":"append","245":"append","246":"remove","247":"remove","248":"append","249":"remove","250":"remove","251":"append","252":"remove","253":"remove","254":"remove","255":"append","256":"append","257":"append","258":"append","259":"append","260":"append","261":"append","262":"append","263":"append","264":"append","265":"remove","266":"remove","267":"remove","268":"remove","269":"remove","270":"remove","271":"remove","272":"append","273":"append","274":"remove","275":"append","276":"append","277":"append","278":"append","279":"append","280":"append","281":"remove","282":"append","283":"append","284":"append","285":"append","286":"remove","287":"remove","288":"append","289":"append","290":"append","291":"append","292":"remove","293":"remove","294":"append","295":"remove","296":"append","297":"remove","298":"append","299":"append","300":"append","301":"append","302":"remove","303":"append","304":"remove","305":"remove","306":"remove","307":"remove","308":"append","309":"append","310":"append","311":"remove","312":"append","313":"append","314":"remove","315":"append","316":"append","317":"append","318":"append","319":"append","320":"append","321":"remove","322":"remove","323":"remove","324":"append","325":"append","326":"remove","327":"remove","328":"append","329":"append","330":"append","331":"append","332":"append","333":"remove","334":"remove","335":"append","336":"append","337":"append","338":"append","339":"remove","340":"remove","341":"append","342":"append","343":"append","344":"append","345":"append","346":"append","347":"remove","348":"append","349":"append","350":"remove","351":"append","352":"append","353":"append","354":"append","355":"append","356":"append","357":"append","358":"remove","359":"append","360":"remove","361":"remove","362":"append","363":"remove","364":"append","365":"remove","366":"append","367":"remove","368":"append","369":"append","370":"append","371":"append","372":"append","373":"append","374":"append","375":"append","376":"append","377":"append","378":"append","379":"append","380":"append","381":"append","382":"remove","383":"remove","384":"append","385":"remove","386":"remove","387":"remove","388":"append","389":"append","390":"append","391":"append","392":"append","393":"append","394":"append","395":"remove","396":"remove","397":"append","398":"remove","399":"remove","400":"remove","401":"remove","402":"remove","403":"append","404":"remove","405":"append","406":"append","407":"append","408":"append","409":"remove","410":"append","411":"append","412":"remove","413":"remove","414":"remove","415":"remove","416":"append","417":"append","418":"append","419":"remove","420":"append","421":"remove","422":"append","423":"append","424":"append","425":"append","426":"append","427":"append","428":"append","429":"remove","430":"append","431":"append","432":"append","433":"append","434":"append","435":"append","436":"remove","437":"append","438":"append","439":"append","440":"append"},"code":{"0":"def decorate_method(cls, func):\n        \"\"\"\n        :param func:       func to be decorated\n        :return:           func that is now decorated\n        \"\"\"\n        func_args = [arg for arg in function_arguments(func) if arg != 'self']\n        method_return_types = \\\n            Endpoint._parse_function_return_types_from_doc(func.__doc__)\n        name = '%s.%s' % (cls.path, func.__name__)\n\n        @wraps(func)\n        def method_decorator(self, *args, **kwargs):\n            for i in range(len(args)):\n                kwargs[func_args[i]] = args[i]\n\n            api_call = self.connection._pre_process_call(\n                name, endpoint_params=kwargs)\n            try:\n                data = func(**kwargs)\n            except RestException as e:\n                api_call.error = e\n                raise e\n\n            except Exception as e:\n                call_queue = self.connection._call_queue.get(\n                    self.connection._get_thread_id(), [])\n                if api_call in call_queue:\n                    call_queue.remove(api_call)\n                e = RestException(original_error=e,\n                                  stack=traceback.format_exc())\n                log.error('ApiCall Exception: %s' % e, exc_info=True)\n                raise e\n            return self.connection._post_process_call(\n                api_call, data, method_return_types)\n\n        method_decorator.rest_method = True\n        return method_decorator","1":"def main():\n    \"\"\"Run playbook\"\"\"\n    for flag in ('--check',):\n        if flag not in sys.argv:\n            sys.argv.append(flag)\n    obj = PlaybookCLI(sys.argv)\n    obj.parse()\n    obj.run()","2":"def set_substitution(self, word, substitution):\n        \"\"\"\n        Add a word substitution\n        :param word: The word to replace\n        :type  word: str\n\n        :param substitution: The word's substitution\n        :type  substitution: str\n        \"\"\"\n        # Parse the word and its substitution\n        raw_word = re.escape(word)\n        raw_substitution = substitution\n\n        case_word = re.escape(normalize(word, preserve_case=True))\n        case_substitution = normalize(substitution, preserve_case=True)\n\n        word = re.escape(normalize(word))\n        substitution = normalize(substitution)\n\n        # Compile and group the regular expressions\n        raw_sub = (re.compile(r'\\b{word}\\b'.format(word=raw_word), re.IGNORECASE), raw_substitution)\n        case_sub = (re.compile(r'\\b{word}\\b'.format(word=case_word), re.IGNORECASE), case_substitution)\n        sub = (re.compile(r'\\b{word}\\b'.format(word=word), re.IGNORECASE), substitution)\n\n        sub_group = (sub, case_sub, raw_sub)\n\n        # Make sure this substitution hasn't already been processed and add it to the substitutions list\n        if sub_group not in self._substitutions:\n            self._log.info('Appending new word substitution: \"{word}\" => \"{sub}\"'.format(word=word, sub=substitution))\n            self._substitutions.append(sub_group)","3":"def call_fn(self, what, *args, **kwargs):\n        \"\"\" Lazy call init_adapter then call the function \"\"\"\n        logger.debug('f_{0}:{1}{2}({3})'.format(\n            self.call_stack_level,\n            ' ' * 4 * self.call_stack_level,\n            what,\n            arguments_as_string(args, kwargs)))\n        port, fn_name = self._what(what)\n        if port not in self['_initialized_ports']:\n            self._call_fn(port, 'init_adapter')\n            self['_initialized_ports'].append(port)\n        return self._call_fn(port, fn_name, *args, **kwargs)","4":"def get_mentions(self, message):\n        \"\"\" Remove duplicates in a case-insensitive way while preserving the original order\n            Return all mentions in lower case *without* their prefixes. (So return ['clyde'], not ['@clyde'])\n        >>> BaseBot().get_mentions(\"This is a @user\")\n        ['user']\n        >>> BaseBot().get_mentions(\"This is empty\")\n        []\n        >>> BaseBot().get_mentions(\"title case and end of string @mention @ChangeTip. and @\")\n        ['mention', 'changetip']\n        >>> BaseBot().get_mentions(\"@ This one has an empty one and two @mention-69 @changetip.\")\n        ['mention-69', 'changetip']\n        >>> BaseBot().get_mentions(\"This one has a dupe @mention-69 @changetip and @mention-69.\")\n        ['mention-69', 'changetip']\n        \"\"\"\n        mentions = re.findall(re.escape(self.prefix) + '([\\w-]+)', message)\n        mentions_set = set([m.lower() for m in mentions])\n\n        deduped_mentions = []\n        for m in mentions:\n            m = m.lower()\n            if m in mentions_set:\n                mentions_set.remove(m)\n                deduped_mentions.append(m)\n\n        return deduped_mentions","5":"def join(self, channel):\n        \"\"\"Add this user to the channel's user list and add the channel to this\n        user's list of joined channels.\n        \"\"\"\n        \n        if channel not in self.channels:\n            channel.users.add(self.nick)\n            self.channels.append(channel)","6":"def part(self, channel):\n        \"\"\"Remove this user from the channel's user list and remove the channel\n        from this user's list of joined channels.\n        \"\"\"\n        \n        if channel in self.channels:\n            channel.users.remove(self.nick)\n            self.channels.remove(channel)","7":"def add_if_unique(self, name):\n        \"\"\"\n        Returns ``True`` on success.\n\n        Returns ``False`` if the name already exists in the namespace.\n        \"\"\"\n        with self.lock:\n            if name not in self.names:\n                self.names.append(name)\n                return True\n        return False","8":"def _validate_optional_key(key, missing, value, validated, optional):\n    \"\"\"Validate an optional key.\"\"\"\n    try:\n        validated[key] = optional[key](value)\n    except NotValid as ex:\n        return ['%r: %s' % (key, arg) for arg in ex.args]\n    if key in missing:\n        missing.remove(key)\n    return []","9":"def de_duplicate(items):\n    \"\"\"Remove any duplicate item, preserving order\n\n    >>> de_duplicate([1, 2, 1, 2])\n    [1, 2]\n    \"\"\"\n    result = []\n    for item in items:\n        if item not in result:\n            result.append(item)\n    return result","10":"def remove_children(list_of_abspath):\n        \"\"\"Remove all dir path that being children path of other dir path.\n        \n        **\u4e2d\u6587\u6587\u6863**\n        \n        \u53bb\u9664list_of_abspath\u4e2d\u6240\u6709\u76ee\u5f55\u7684\u5b50\u76ee\u5f55, \u4fdd\u8bc1\u5176\u4e2d\u7684\u6240\u6709\u5143\u7d20\u4e0d\u53ef\u80fd\u4e3a\u53e6\u4e00\u4e2a\n        \u5143\u7d20\u7684\u5b50\u76ee\u5f55\u3002\n        \"\"\"\n        sorted_list_of_abspath = list(list_of_abspath)\n        sorted_list_of_abspath.sort()\n        sorted_list_of_abspath.append(\"\")  \n        res = list()\n        temp = sorted_list_of_abspath[0]\n        for abspath in sorted_list_of_abspath:\n            if temp not in abspath:\n                res.append(temp)\n                temp = abspath         \n        return res","11":"def create_migration(initial=False):\n    \"\"\"Create a South migration for this project\"\"\"\n\n    settings = DjangoSettings()\n    if 'south' not in (name.lower() for name in settings.INSTALLED_APPS):\n        print(\"Temporarily adding 'south' into INSTALLED_APPS.\")\n        settings.INSTALLED_APPS.append('south')\n\n    kwargs = dict(initial=True) if initial else dict(auto=True)\n    run_django_cmd('schemamigration', package['name'], **kwargs)","12":"def WhatActorsArePresent(self):\n        '''Return a list of URI's of all the actor attributes found in\n        the header.  The special actor \"next\" is ignored.\n        '''\n        results = []\n        for E in self.header_elements:\n            a = _find_actor(E)\n            if a not in [ None, SOAP.ACTOR_NEXT ]: results.append(a)\n        return results","13":"def bind(self, func, etype):\n        '''\n        Register @func for execution when events with `.type` of @etype \n        or meta-events with `.utype` of @etype are handled. @func will be \n        called with self, self.gstate, and the event as arguments.\n        '''\n        self.event_funcs.setdefault(etype, [])\n        # Don't add multiple times!\n        if func not in self.event_funcs[etype]:\n            self.event_funcs[etype].append(func)","14":"def _inherit_context(self, node):\n        '''_inherit_context(self, node) -> list\n        Scan ancestors of attribute and namespace context.  Used only\n        for single element node canonicalization, not for subset\n        canonicalization.'''\n\n        # Collect the initial list of xml:foo attributes.\n        xmlattrs = filter(_IN_XML_NS, _attrs(node))\n\n        # Walk up and get all xml:XXX attributes we inherit.\n        inherited, parent = [], node.parentNode\n        while parent and parent.nodeType == Node.ELEMENT_NODE:\n            for a in filter(_IN_XML_NS, _attrs(parent)):\n                n = a.localName\n                if n not in xmlattrs:\n                    xmlattrs.append(n)\n                    inherited.append(a)\n            parent = parent.parentNode\n        return inherited","15":"def _update_internal_column_state(self, column_names):\n        \"\"\" Update the internal state with some (possibly) new columns\n\n        :param column_names: an iterable which contains new column names\n        \"\"\"\n        for k in column_names:\n            if k not in self._column_name_idx:\n                self._column_name_idx[k] = len(self._column_name_list)\n                self._column_name_list.append(k)","16":"def uniqify(list_):\n  \"inefficient on long lists; short lists only. preserves order.\"\n  a=[]\n  for x in list_:\n    if x not in a: a.append(x)\n  return a","17":"def list_hierarchy(class_name, bases):\n    \"\"\"\n    Creates a list of the class hierarchy\n\n    Args:\n    -----\n        class_name: name of the current class\n        bases: list\/tuple of bases for the current class\n    \"\"\"\n\n    class_list = [Uri(class_name)]\n    for base in bases:\n        if base.__name__ not in IGNORE_CLASSES:\n            class_list.append(Uri(base.__name__))\n    return list([i for i in set(class_list)])","18":"def _get_paths(self):\n        \"\"\"Return a list of paths to search for plugins in\n\n        The list is searched in order.\"\"\"\n        ret = []\n        ret += ['%s\/library\/' % os.path.dirname(os.path.dirname(__file__))]        \n        \n        ret += self._extra_dirs\n        for basedir in _basedirs:\n            fullpath = os.path.join(basedir, self.subdir)\n            if fullpath not in ret:\n                ret.append(fullpath)\n        ret += self.config.split(os.pathsep)\n        ret += self._get_package_path()\n        return ret","19":"def print_paths(self):\n        \"\"\"Returns a string suitable for printing of the search path\"\"\"\n        # Uses a list to get the order right\n        ret = []\n        for i in self._get_paths():\n            if i not in ret:\n                ret.append(i)\n        return os.pathsep.join(ret)","20":"def get_keys(self) -> typing.List[str]:\n        \"\"\"\n        Return list of SHA512 hash keys that exist in datafile\n\n        :return: list of keys\n        \"\"\"\n        keys = []\n        for key in self.data.keys():\n            if key not in ['__header__', '__version__', '__globals__']:\n                keys.append(key)\n        return keys","21":"def stateNames(self):\r\n        \"\"\"Returns:\r\n             list: the names of all saved sessions\r\n        \"\"\"\r\n#         if self.current_session:\r\n        s = self.tmp_dir_session\r\n        l = [x for x in s.listdir() if s.join(x).isdir()]\r\n        naturalSorting(l)\r\n        # else:\r\n        #    l=[]\r\n        # bring autosave to first position:\r\n        if 'autoSave' in l:\r\n            l.remove('autoSave')\r\n            l.insert(0, 'autoSave')\r\n        return l","22":"def set_remove(parent, idx, value):\n  \"\"\"Remove an item from a list.\"\"\"\n  lst = get_child(parent, idx)\n  if value in lst:\n    lst.remove(value)","23":"def set_add(parent, idx, value):\n  \"\"\"Add an item to a list if it doesn't exist.\"\"\"\n  lst = get_child(parent, idx)\n  if value not in lst:\n    lst.append(value)","24":"def blog_months(*args):\n    \"\"\"\n    Put a list of dates for blog posts into the template context.\n    \"\"\"\n    dates = BlogPost.objects.published().values_list(\"publish_date\", flat=True)\n    date_dicts = [{\"date\": datetime(d.year, d.month, 1)} for d in dates]\n    month_dicts = []\n    for date_dict in date_dicts:\n        if date_dict not in month_dicts:\n            month_dicts.append(date_dict)\n    for i, date_dict in enumerate(month_dicts):\n        month_dicts[i][\"post_count\"] = date_dicts.count(date_dict)\n    return month_dicts","25":"def remove_regions_with_no_gates(regions):\n    \"\"\" Removes all Jove regions from a list of regions.\n\n    :param regions: A list of tuples (regionID, regionName)\n    :type regions: list\n\n    :return: A list of regions minus those in jove space\n    :rtype: list\n    \"\"\"\n\n    list_of_gateless_regions = [\n        (10000004, 'UUA-F4'),\n        (10000017, 'J7HZ-F'),\n        (10000019, 'A821-A'),\n    ]\n\n    for gateless_region in list_of_gateless_regions:\n        if gateless_region in regions:\n            regions.remove(gateless_region)\n\n    return regions","26":"def total_core(self):\n        \"\"\"\n        Determine the total number of core genes present\n        \"\"\"\n        corefile = os.path.join(self.reffilepath, self.analysistype, 'Escherichia', 'core_combined.fasta')\n        for record in SeqIO.parse(corefile, 'fasta'):\n            gene_name = record.id.split('-')[0]\n            if gene_name not in self.coregenomes:\n                self.coregenomes.append(gene_name)","27":"def _add_epsilon_states(self, stateset, gathered_epsilons):\n    '''\n    stateset is the list of initial states\n    gathered_epsilons is a dictionary of (dst: src) epsilon dictionaries\n    '''\n    for i in list(stateset):\n      if i not in gathered_epsilons:\n        gathered_epsilons[i] = {}\n        q = _otq()\n        q.append(i)\n        while q:\n          s = q.popleft()\n          for j in self._transitions.setdefault(s, {}).setdefault(NFA.EPSILON, set()):\n            gathered_epsilons[i][j] = s if j not in gathered_epsilons[i] else self.choose(s, j)\n            q.append(j)\n      stateset.update(gathered_epsilons[i].keys())","28":"def enqueue_mod(self, dn, mod):\n        \"\"\"Enqueue a LDAP modification.\n\n        Arguments:\n        dn -- the distinguished name of the object to modify\n        mod -- an ldap modfication entry to enqueue\n        \"\"\"\n        # mark for update\n        if dn not in self.__pending_mod_dn__:\n            self.__pending_mod_dn__.append(dn)\n            self.__mod_queue__[dn] = []\n\n        self.__mod_queue__[dn].append(mod)","29":"def objects_reachable_from(obj):\n    \"\"\"\n    Return graph of objects reachable from *obj* via ``gc.get_referrers``.\n\n    Returns an :class:`~refcycle.object_graph.ObjectGraph` object holding all\n    objects reachable from the given one by following the output of\n    ``gc.get_referrers``.  Note that unlike the\n    :func:`~refcycle.creators.snapshot` function, the output graph may\n    include non-gc-tracked objects.\n\n    \"\"\"\n    # Depth-first search.\n    found = ObjectGraph.vertex_set()\n    to_process = [obj]\n    while to_process:\n        obj = to_process.pop()\n        found.add(obj)\n        for referent in gc.get_referents(obj):\n            if referent not in found:\n                to_process.append(referent)\n    return ObjectGraph(found)","30":"def _bfs_sort(self, start):\n        \"\"\"\n        maintain a map of states distance using BFS\n        Args:\n            start (fst state): The initial DFA state\n        Returns:\n            list: An ordered list of DFA states\n                  using path distance\n        \"\"\"\n        pathstates = {}\n        # maintain a queue of nodes to be visited. Both current and previous\n        # node must be included.\n        queue = []\n        # push the first path into the queue\n        queue.append([0, start])\n        pathstates[start.stateid] = 0\n        while queue:\n            # get the first node from the queue\n            leaf = queue.pop(0)\n            node = leaf[1]\n            pathlen = leaf[0]\n            # enumerate all adjacent nodes, construct a new path and push it\n            # into the queue\n            for arc in node.arcs:\n                next_state = self.mma[arc.nextstate]\n                if next_state.stateid not in pathstates:\n                    queue.append([pathlen + 1, next_state])\n                    pathstates[next_state.stateid] = pathlen + 1\n        orderedstatesdict = OrderedDict(\n            sorted(\n                pathstates.items(),\n                key=lambda x: x[1],\n                reverse=False))\n        for state in self.mma.states:\n            orderedstatesdict[state.stateid] = state\n        orderedstates = [x[1] for x in list(orderedstatesdict.items())]\n        return orderedstates","31":"def registerDisplay(func):\n    \"\"\"\n    Registers a function to the display hook queue to be called on hook.\n    Look at the sys.displayhook documentation for more information.\n    \n    :param      func | <callable>\n    \"\"\"\n    setup()\n    ref = weakref.ref(func)\n    if ref not in _displayhooks:\n        _displayhooks.append(ref)","32":"def registerExcept(func):\n    \"\"\"\n    Registers a function to the except hook queue to be called on hook.\n    Look at the sys.displayhook documentation for more information.\n    \n    :param      func | <callable>\n    \"\"\"\n    setup()\n    ref = weakref.ref(func)\n    if ref not in _excepthooks:\n        _excepthooks.append(ref)","33":"def walk(start:list, graphs:iter) -> iter:\n    \"\"\"walk on given graphs, beginning on start.\n    Yield all found nodes, including start.\n\n    All graph are understood as a single one,\n    with merged keys and values.\n\n    \"\"\"\n    walked = set([start])\n    stack = [start]\n    while len(stack) > 0:\n        *stack, curr = stack\n        yield curr\n        succs = it.chain.from_iterable(graph.get(curr, ()) for graph in graphs)\n        for succ in succs:\n            if succ not in walked:\n                walked.add(curr)\n                stack.append(succ)","34":"def group_comments_by_round(comments, ranking=0):\n    \"\"\"\n    Group comments by the round to which they belong\n    \"\"\"\n    comment_rounds = {}\n    ordered_comment_round_names = []\n    for comment in comments:\n        comment_round_name = ranking and comment[11] or comment[7]\n        if comment_round_name not in comment_rounds:\n            comment_rounds[comment_round_name] = []\n            ordered_comment_round_names.append(comment_round_name)\n        comment_rounds[comment_round_name].append(comment)\n    return [(comment_round_name, comment_rounds[comment_round_name])\n            for comment_round_name in ordered_comment_round_names]","35":"def create_manifest_from_s3_files(self):\n        \"\"\"\n        To create a manifest db for the current\n        :return:\n        \"\"\"\n        for k in self.s3.list_objects(Bucket=self.sitename)['Contents']:\n            key = k[\"Key\"]\n            files = []\n            if key not in [self.manifest_file]:\n                files.append(key)\n            self._set_manifest_data(files)","36":"def neighbors(self, node_id):\n        \"\"\"Find all the nodes where there is an edge from the specified node to that node.\n        Returns a list of node ids.\"\"\"\n        node = self.get_node(node_id)\n        flattened_nodes_list = []\n        for a, b in [self.get_edge(edge_id)['vertices'] for edge_id in node['edges']]:\n            flattened_nodes_list.append(a)\n            flattened_nodes_list.append(b)\n        node_set = set(flattened_nodes_list)\n        if node_id in node_set:\n            node_set.remove(node_id)\n        return [nid for nid in node_set]","37":"def S_star(u, dfs_data):\n    \"\"\"The set of all descendants of u, with u added.\"\"\"\n    s_u = S(u, dfs_data)\n    if u not in s_u:\n        s_u.append(u)\n    return s_u","38":"def remove_dbs(self, double):\n        \"\"\"Remove double item from list\n        \"\"\"\n        one = []\n        for dup in double:\n            if dup not in one:\n                one.append(dup)\n        return one","39":"def alien_filter(packages, sizes):\n    \"\"\"This filter avoid list double packages from\n    alien repository\n    \"\"\"\n    cache, npkg, nsize = [], [], []\n    for p, s in zip(packages, sizes):\n        name = split_package(p)[0]\n        if name not in cache:\n            cache.append(name)\n            npkg.append(p)\n            nsize.append(s)\n    return npkg, nsize","40":"def clear_masters(self):\n        \"\"\"Clear master packages if already exist in dependencies\n        or if added to install two or more times\n        \"\"\"\n        packages = []\n        for mas in Utils().remove_dbs(self.packages):\n            if mas not in self.dependencies:\n                packages.append(mas)\n        self.packages = packages","41":"def clear_masters(self):\n        \"\"\"Clear master slackbuilds if already exist in dependencies\n        or if added to install two or more times\n        \"\"\"\n        self.master_packages = Utils().remove_dbs(self.master_packages)\n        for mas in self.master_packages:\n            if mas in self.dependencies:\n                self.master_packages.remove(mas)","42":"def update(self, items):\n        \"\"\"\n        Updates the dependencies in the inverse relationship format, i.e. from an iterable or dict that is structured\n        as `(item, dependent_items)`. Note that this implementation is only valid for 1:1 relationships, i.e. that each\n        node has also exactly one dependent. For other cases, :class:`~MultiDependencyResolver` should be used.\n\n        :param items: Iterable or dictionary in the format `(item, dependent_items)`.\n        :type items: collections.Iterable\n        \"\"\"\n        for parent, sub_item in _iterate_dependencies(items):\n            dep = self._deps[sub_item]\n            if parent not in dep.parent:\n                dep.parent.append(parent)","43":"def update(self, items):\n        \"\"\"\n        Updates the dependencies in the inverse relationship format, i.e. from an iterable or dict that is structured\n        as `(item, dependent_items)`. The parent element `item` may occur multiple times.\n\n        :param items: Iterable or dictionary in the format `(item, dependent_items)`.\n        :type items: collections.Iterable\n        \"\"\"\n        for parent, sub_items in _iterate_dependencies(items):\n            for si in sub_items:\n                dep = self._deps[si]\n                if parent not in dep.parent:\n                    dep.parent.append(parent)","44":"def register_from_fields(self, *args):\n        \"\"\"\n        Register config name from field widgets\n\n        Arguments:\n            *args: Fields that contains widget\n                :class:`djangocodemirror.widget.CodeMirrorWidget`.\n\n        Returns:\n            list: List of registered config names from fields.\n        \"\"\"\n        names = []\n        for field in args:\n            widget = self.resolve_widget(field)\n            self.register(widget.config_name)\n            if widget.config_name not in names:\n                names.append(widget.config_name)\n\n        return names","45":"def import_generated_autoboto(self):\n        \"\"\"\n        Imports the autoboto package generated in the build directory (not target_dir).\n\n        For example:\n            autoboto = botogen.import_generated_autoboto()\n\n        \"\"\"\n        if str(self.config.build_dir) not in sys.path:\n            sys.path.append(str(self.config.build_dir))\n        return importlib.import_module(self.config.target_package)","46":"def import_generated_autoboto_module(self, name):\n        \"\"\"\n        Imports a module from the generated autoboto package in the build directory (not target_dir).\n\n        For example, to import autoboto.services.s3.shapes, call:\n            botogen.import_generated_autoboto_module(\"services.s3.shapes\")\n\n        \"\"\"\n        if str(self.config.build_dir) not in sys.path:\n            sys.path.append(str(self.config.build_dir))\n        return importlib.import_module(f\"{self.config.target_package}.{name}\")","47":"def apply_multicolor_transit(self,band,depth):\n        \"\"\"\n        Applies constraint corresponding to measuring transit in different band\n\n        This is not implemented yet.\n        \"\"\"\n        if '{} band transit'.format(band) not in self.constraints:\n            self.constraints.append('{} band transit'.format(band))\n        for pop in self.poplist:\n            pop.apply_multicolor_transit(band,depth)","48":"def constrain_property(self,prop,**kwargs):\n        \"\"\"\n        Constrains property for each population\n\n        See :func:`vespa.stars.StarPopulation.constrain_property`;\n        all arguments passed to that function for each population.\n\n        \"\"\"\n        if prop not in self.constraints:\n            self.constraints.append(prop)\n        for pop in self.poplist:\n            try:\n                pop.constrain_property(prop,**kwargs)\n            except AttributeError:\n                logging.info('%s model does not have property stars.%s (constraint not applied)' % (pop.model,prop))","49":"def replace_constraint(self,name,**kwargs):\n        \"\"\"\n        Replaces removed constraint in each population.\n\n        See :func:`vespa.stars.StarPopulation.replace_constraint`\n\n        \"\"\"\n\n        for pop in self.poplist:\n            pop.replace_constraint(name,**kwargs)\n        if name not in self.constraints:\n            self.constraints.append(name)","50":"def event_state_counties(self):\n        \"\"\"DEPRECATED: this will be moved elsewhere or dropped in the near future, stop using it.\n        Return an event type and it's state(s) and counties (consolidated)\"\"\"\n        # FIXME: most of this logic should be moved to the alert instance and refactored\n        counties = ''\n        state = ''\n        for alert in self._alerts:\n            locations = []\n            states = []\n            for samecode in alert.samecodes:\n                county, state = self.geo.lookup_county_state(samecode)\n                locations.append((county, state))\n                if state not in states:\n                    states.append(state)\n            for state in states:\n                counties = [x for x, y in locations if y == state]\n            counties_clean = str(counties).strip(\"[']\")\n            print(\"{0}: {1} - {2}\".format(alert.event, state, counties_clean))","51":"def all_sample_keys(self) -> List[str]:\n        \"\"\"Return the unique keys of all samples in this :class:`SampleSheet`.\n\n        The keys are discovered first by the order of samples and second by\n        the order of keys upon those samples.\n\n        \"\"\"\n        all_keys: List[str] = []\n        for key in chain.from_iterable([sample.keys() for sample in self]):\n            if key not in all_keys:\n                all_keys.append(key)\n        return all_keys","52":"def _format_generic(lines, element, printed, spacer=\"\"):\n    \"\"\"Generically formats all remaining docstrings and custom XML\n    tags that don't appear in the list of already printed documentation.\n\n    :arg printed: a list of XML tags for the element that have already\n      been handled by a higher method.\n    \"\"\"\n    for doc in element.docstring:\n        if doc.doctype.lower() not in printed:\n            lines.append(spacer + doc.__str__())","53":"def order_module_dependencies(modules, parser):\n    \"\"\"Orders the specified list of modules based on their inter-dependencies.\"\"\"\n    result = []        \n    for modk in modules:\n        if modk not in result:\n            result.append(modk)\n\n    #We also need to look up the dependencies of each of these modules\n    recursed = list(result)\n    for i in range(len(result)):\n        module = result[i]\n        _process_module_order(parser, module, i, recursed)\n\n    return recursed","54":"def _add_current_codedir(self, path):\n        \"\"\"Adds the directory of the file at the specified path as a base\n        path to find other files in.\n        \"\"\"\n        dirpath = self.tramp.dirname(path)\n        if dirpath not in self.basepaths:\n            self.basepaths.append(dirpath)\n            self.rescan()","55":"def add_parameter(self, parameter):\n        \"\"\"Adds the specified parameter value to the list.\"\"\"\n        if parameter.name.lower() not in self.paramorder:\n            self.paramorder.append(parameter.name.lower())\n        self._parameters[parameter.name.lower()] = parameter","56":"def needs(self):\n        \"\"\"Returns a unique list of module names that this module depends on.\"\"\"\n        result = []\n        for dep in self.dependencies:\n            module = dep.split(\".\")[0].lower()\n            if module not in result:\n                result.append(module)\n\n        return result","57":"def unique(input_list):\n    \"\"\"\n    Return a list of unique items (similar to set functionality).\n\n    Parameters\n    ----------\n    input_list : list\n        A list containg some items that can occur more than once.\n\n    Returns\n    -------\n    list\n        A list with only unique occurances of an item.\n\n    \"\"\"\n    output = []\n    for item in input_list:\n        if item not in output:\n            output.append(item)\n    return output","58":"def insert(self, key, value, index):\n        '''Accepts a :key:, :value:, and :index: parameter and inserts\n        a new key, value member at the desired index.\n\n        Note: Inserting with a negative index will have the following behavior:\n        >>> l = [1, 2, 3, 4]\n        >>> l.insert(-1, 5)\n        >>> l\n        [1, 2, 3, 5, 4]\n        '''\n\n        if key in self._keys:\n            self._keys.remove(key)\n        self._keys.insert(index, key)\n        self._d[key] = value","59":"def types_(self, col: str) -> pd.DataFrame:\n        \"\"\"\n        Display types of values in a column\n\n        :param col: column name\n        :type col: str\n        :return: a pandas dataframe\n        :rtype: pd.DataFrame\n\n        :example: ``ds.types_(\"Col 1\")``\n        \"\"\"\n        cols = self.df.columns.values\n        all_types = {}\n        for col in cols:\n            local_types = []\n            for i, val in self.df[col].iteritems():\n                #print(i, val, type(val))\n                t = type(val).__name__\n                if t not in local_types:\n                    local_types.append(t)\n            all_types[col] = (local_types, i)\n        df = pd.DataFrame(all_types, index=[\"type\", \"num\"])\n        return df","60":"def unregister_dependent_on(self, tree):\n        \"\"\"unregistering tree that we are dependent on\"\"\"\n        if tree in self.dependent_on:\n            self.dependent_on.remove(tree)","61":"def _get_parent_remote_paths(self):\n        \"\"\"\n        Get list of remote folders based on the list of all file urls\n        :return: set([str]): set of remote folders (that contain files)\n        \"\"\"\n        parent_paths = set([item.get_remote_parent_path() for item in self.file_urls])\n        if '' in parent_paths:\n            parent_paths.remove('')\n        return parent_paths","62":"def release_port(self, port):\n        \"\"\"release port\"\"\"\n        if port in self.__closed:\n            self.__closed.remove(port)\n        self.__ports.add(port)","63":"def register(self, backbone_view_class):\n        \"\"\"\n        Registers the given backbone view class.\n        \"\"\"\n        if backbone_view_class not in self._registry:\n            self._registry.append(backbone_view_class)","64":"def deallocate_fw_dev(self, fw_id):\n        \"\"\"Release the firewall resource. \"\"\"\n        for cnt in self.res:\n            if fw_id in self.res.get(cnt).get('fw_id_lst'):\n                self.res[cnt]['used'] = self.res[cnt]['used'] - 1\n                self.res.get(cnt).get('fw_id_lst').remove(fw_id)\n                return","65":"def add_tls_credential(self, credentials):\n        \"\"\"        \n        Add a list of TLSServerCredential to this engine.\n        TLSServerCredentials can be in element form or can also\n        be the href for the element.\n        \n        :param credentials: list of pre-created TLSServerCredentials\n        :type credentials: list(str,TLSServerCredential)\n        :return: None\n        \"\"\"\n        for cred in credentials:\n            href = element_resolver(cred)\n            if href not in self.engine.server_credential:\n                self.engine.server_credential.append(href)","66":"def remove_tls_credential(self, credentials):\n        \"\"\"    \n        Remove a list of TLSServerCredentials on this engine.\n        \n        :param credentials: list of credentials to remove from the\n            engine\n        :type credentials: list(str,TLSServerCredential)\n        :return: None\n        \"\"\"\n        for cred in credentials:\n            href = element_resolver(cred)\n            if href in self.engine.server_credential:\n                self.engine.server_credential.remove(href)","67":"def remove_permission(self, elements):\n        \"\"\"    \n        Remove permission\/s to this ACL. Change is committed at end of\n        method call.\n        \n        :param list elements: list of element\/s to remove\n        :type elements: list(str,Element)\n        :raises UpdateElementFailed: Failed modifying permissions\n        :return: None\n        \"\"\"\n        elements = element_resolver(elements)\n        for element in elements:\n            if element in self.granted_element:\n                self.data['granted_element'].remove(element)\n        self.update()","68":"def add_loaded_callback(self, callback):\n        \"\"\"Add a callback to be run when the ALDB load is complete.\"\"\"\n        if callback not in self._cb_aldb_loaded:\n            self._cb_aldb_loaded.append(callback)","69":"def _makeScriptOrder(gpos):\n    \"\"\"\n    Run therough GPOS and make an alphabetically\n    ordered list of scripts. If DFLT is in the list,\n    move it to the front.\n    \"\"\"\n    scripts = []\n    for scriptRecord in gpos.ScriptList.ScriptRecord:\n        scripts.append(scriptRecord.ScriptTag)\n    if \"DFLT\" in scripts:\n        scripts.remove(\"DFLT\")\n        scripts.insert(0, \"DFLT\")\n    return sorted(scripts)","70":"def remove(self, models):\n        \"\"\" Removed the passed model(s) from the selection\"\"\"\n        models = self._check_model_types(models)\n        for model in models:\n            if model in self._selected:\n                self._selected.remove(model)","71":"def register_actions(self, shortcut_manager):\n        \"\"\"Register callback methods for triggered actions in all child controllers.\n\n        :param rafcon.gui.shortcut_manager.ShortcutManager shortcut_manager: Shortcut Manager Object holding mappings\n            between shortcuts and actions.\n        \"\"\"\n        assert isinstance(shortcut_manager, ShortcutManager)\n        self.__shortcut_manager = shortcut_manager\n\n        for controller in list(self.__child_controllers.values()):\n            if controller not in self.__action_registered_controllers:\n                try:\n                    controller.register_actions(shortcut_manager)\n                except Exception as e:\n                    logger.error(\"Error while registering action for {0}: {1}\".format(controller.__class__.__name__, e))\n                self.__action_registered_controllers.append(controller)","72":"def set_state(self, state):\n        \"\"\"\n        Activate and put this conversation into the given state.\n\n        The relation name will be interpolated in the state name, and it is\n        recommended that it be included to avoid conflicts with states from\n        other relations.  For example::\n\n            conversation.set_state('{relation_name}.state')\n\n        If called from a converation handling the relation \"foo\", this will\n        activate the \"foo.state\" state, and will add this conversation to\n        that state.\n\n        Note: This uses :mod:`charmhelpers.core.unitdata` and requires that\n        :meth:`~charmhelpers.core.unitdata.Storage.flush` be called.\n        \"\"\"\n        state = state.format(relation_name=self.relation_name)\n        value = _get_flag_value(state, {\n            'relation': self.relation_name,\n            'conversations': [],\n        })\n        if self.key not in value['conversations']:\n            value['conversations'].append(self.key)\n        set_flag(state, value)","73":"def suck_out_variations_only(reporters):\n    \"\"\"Builds a dictionary of variations to canonical reporters.\n\n    The dictionary takes the form of:\n        {\n         \"A. 2d\": [\"A.2d\"],\n         ...\n         \"P.R.\": [\"Pen. & W.\", \"P.R.R.\", \"P.\"],\n        }\n\n    In other words, it's a dictionary that maps each variation to a list of\n    reporters that it could be possibly referring to.\n    \"\"\"\n    variations_out = {}\n    for reporter_key, data_list in reporters.items():\n        # For each reporter key...\n        for data in data_list:\n            # For each book it maps to...\n            for variation_key, variation_value in data[\"variations\"].items():\n                try:\n                    variations_list = variations_out[variation_key]\n                    if variation_value not in variations_list:\n                        variations_list.append(variation_value)\n                except KeyError:\n                    # The item wasn't there; add it.\n                    variations_out[variation_key] = [variation_value]\n\n    return variations_out","74":"def selectPeerToIntroduce(self, otherPeers):\n        \"\"\"\n        Choose a peer to introduce.  Return a q2q address or None, if there are\n        no suitable peers to introduce at this time.\n        \"\"\"\n        for peer in otherPeers:\n            if peer not in self.otherPeers:\n                self.otherPeers.append(peer)\n                return peer","75":"def uniq(self) -> 'TList[T]':\n        \"\"\"\n        Usage:\n\n            >>> TList([1, 2, 3, 2, 1]).uniq()\n            [1, 2, 3]\n        \"\"\"\n        rs = TList()\n        for e in self:\n            if e not in rs:\n                rs.append(e)\n        return rs","76":"def _pre_tidy(html):\n    \"\"\" This method transforms a few things before tidy runs. When we get rid\n        of tidy, this can go away. \"\"\"\n    tree = etree.fromstring(html, etree.HTMLParser())\n    for el in tree.xpath('\/\/u'):\n        el.tag = 'em'\n        c = el.attrib.get('class', '').split()\n        if 'underline' not in c:\n            c.append('underline')\n            el.attrib['class'] = ' '.join(c)\n\n    return tohtml(tree)","77":"def bokeh(model, scale: float = 0.7, responsive: bool = True):\n    \"\"\"\n    Adds a Bokeh plot object to the notebook display.\n\n    :param model:\n        The plot object to be added to the notebook display.\n    :param scale:\n        How tall the plot should be in the notebook as a fraction of screen\n        height. A number between 0.1 and 1.0. The default value is 0.7.\n    :param responsive:\n        Whether or not the plot should responsively scale to fill the width\n        of the notebook. The default is True.\n    \"\"\"\n    r = _get_report()\n\n    if 'bokeh' not in r.library_includes:\n        r.library_includes.append('bokeh')\n\n    r.append_body(render_plots.bokeh_plot(\n        model=model,\n        scale=scale,\n        responsive=responsive\n    ))\n    r.stdout_interceptor.write_source('[ADDED] Bokeh plot\\n')","78":"def latex(source: str):\n    \"\"\"\n    Add a mathematical equation in latex math-mode syntax to the display.\n    Instead of the traditional backslash escape character, the @ character is\n    used instead to prevent backslash conflicts with Python strings. For\n    example, \\\\delta would be @delta.\n\n    :param source:\n        The string representing the latex equation to be rendered.\n    \"\"\"\n    r = _get_report()\n    if 'katex' not in r.library_includes:\n        r.library_includes.append('katex')\n\n    r.append_body(render_texts.latex(source.replace('@', '\\\\')))\n    r.stdout_interceptor.write_source('[ADDED] Latex equation\\n')","79":"def add_output_path(path: str = None) -> str:\n    \"\"\"\n    Adds the specified path to the output logging paths if it is not\n    already in the listed paths.\n\n    :param path:\n        The path to add to the logging output paths. If the path is empty\n        or no path is given, the current working directory will be used\n        instead.\n    \"\"\"\n    cleaned = paths.clean(path or os.getcwd())\n    if cleaned not in _logging_paths:\n        _logging_paths.append(cleaned)\n    return cleaned","80":"def update_recent_paths(response, path):\n    \"\"\"\n    :param response:\n    :param path:\n    :return:\n    \"\"\"\n\n    try:\n        recent_paths = environ.configs.fetch('recent_paths', [])\n\n        if path in recent_paths:\n            recent_paths.remove(path)\n\n        recent_paths.insert(0, path)\n        environ.configs.put(recent_paths=recent_paths[:10], persists=True)\n        environ.configs.save()\n    except Exception as error:  # pragma: no cover\n        response.warn(\n            code='FAILED_RECENT_UPDATE',\n            message='Unable to update recently opened projects',\n            error=str(error)\n        ).console(whitespace=1)\n\n    return True","81":"def remove_library_path(path: str) -> bool:\n    \"\"\"\n    Removes the path from the Python system path if it is found in the system\n    paths.\n\n    :param path:\n        The path to remove from the system paths\n    :return:\n        Whether or not the path was removed.\n    \"\"\"\n\n    if path in sys.path:\n        sys.path.remove(path)\n        return True\n\n    return False","82":"def finalize_env(env):\n    \"\"\"\n    Produce a platform specific env for passing into subprocess.Popen\n    family of external process calling methods, and the supplied env\n    will be updated on top of it.  Returns a new env.\n    \"\"\"\n\n    keys = _PLATFORM_ENV_KEYS.get(sys.platform, [])\n    if 'PATH' not in keys:\n        # this MUST be available due to Node.js (and others really)\n        # needing something to look for binary locations when it shells\n        # out to other binaries.\n        keys.append('PATH')\n    results = {\n        key: os.environ.get(key, '') for key in keys\n    }\n    results.update(env)\n    return results","83":"def remove_callback(self, callback) -> None:\n        \"\"\"Remove callback.\"\"\"\n        if callback in self._callbacks:\n            self._callbacks.remove(callback)","84":"def append(self, decoration):\n        \"\"\"\n        Adds a text decoration on a CodeEdit instance\n\n        :param decoration: Text decoration to add\n        :type decoration: pyqode.core.api.TextDecoration\n        \"\"\"\n        if decoration not in self._decorations:\n            self._decorations.append(decoration)\n            self._decorations = sorted(\n                self._decorations, key=lambda sel: sel.draw_order)\n            self.editor.setExtraSelections(self._decorations)\n            return True\n        return False","85":"def preferred_encodings(self):\n        \"\"\"\n        The list of user defined encodings, for display in the encodings\n        menu\/combobox.\n\n        \"\"\"\n        default_encodings = [\n            locale.getpreferredencoding().lower().replace('-', '_')]\n        if 'utf_8' not in default_encodings:\n            default_encodings.append('utf_8')\n        default_encodings = list(set(default_encodings))\n        return json.loads(self._settings.value(\n            'userDefinedEncodings', json.dumps(default_encodings)))","86":"def stub(self, obj, attr=None):\n        '''\n        Stub an object. If attr is not None, will attempt to stub that\n        attribute on the object. Only required for modules and other rare\n        cases where we can't determine the binding from the object.\n        '''\n        s = stub(obj, attr)\n        if s not in self._stubs:\n            self._stubs.append(s)\n        return s","87":"def add_milestone(self,\n                      milestone,\n                      codelistoid=\"MILESTONES\"):\n        \"\"\"\n        Add a milestone\n        :param codelistoid: specify the CodeListOID (defaults to MILESTONES)\n        :param str milestone: Milestone to add\n        \"\"\"\n        if milestone not in self.milestones.get(codelistoid, []):\n            self._milestones.setdefault(codelistoid, []).append(milestone)","88":"def unique(_list):\n    \"\"\"\n    Makes the list have unique items only and maintains the order\n\n    list(set()) won't provide that\n\n    :type _list list\n    :rtype: list\n    \"\"\"\n    ret = []\n\n    for item in _list:\n        if item not in ret:\n            ret.append(item)\n\n    return ret","89":"def _aggregation_op(cls,\n            op: Callable[[tf.Tensor, Optional[Sequence[int]]], tf.Tensor],\n            x: 'TensorFluent',\n            vars_list: List[str]) -> 'TensorFluent':\n        '''Returns a TensorFluent for the aggregation `op` applied to fluent `x`.\n\n        Args:\n            op: The aggregation operation.\n            x: The input fluent.\n            vars_list: The list of variables to be aggregated over.\n\n        Returns:\n            A TensorFluent wrapping the aggregation operator's output.\n        '''\n        axis = cls._varslist2axis(x, vars_list)\n        t = op(x.tensor, axis)\n\n        scope = []\n        for var in x.scope.as_list():\n            if var not in vars_list:\n                scope.append(var)\n\n        batch = x.batch\n\n        return TensorFluent(t, scope, batch=batch)","90":"def uniquify(l):\n    \"\"\"\n    Uniquify a list (skip duplicate items).\n    \"\"\"\n    result = []\n    for x in l:\n        if x not in result:\n            result.append(x)\n    return result","91":"def _add_to_stack(self, item, value):\n        \"\"\"\n        Add a parameter-value pair to the stack of parameters that have been set.\n        :param item:\n        :param value:\n        :return:\n        \"\"\"\n        p_value = (item, value)\n        if p_value not in self.stack:\n            self.stack.append(p_value)","92":"def remove_receiver(self, receiver):\r\n        \"\"\" Remove a receiver to the list of receivers.\r\n\r\n        :param receiver: a callable variable\r\n        \"\"\"\r\n        if receiver in self.event_receivers:\r\n            self.event_receivers.remove(receiver)","93":"def get_themes(templates_path):\n    \"\"\"Returns available themes list.\"\"\"\n    themes = os.listdir(templates_path)\n    if '__common__' in themes:\n        themes.remove('__common__')\n    return themes","94":"def serial_layers(self):\n        \"\"\"Yield a layer for all gates of this circuit.\n\n        A serial layer is a circuit with one gate. The layers have the\n        same structure as in layers().\n        \"\"\"\n        for next_node in self.topological_op_nodes():\n            new_layer = DAGCircuit()\n            for qreg in self.qregs.values():\n                new_layer.add_qreg(qreg)\n            for creg in self.cregs.values():\n                new_layer.add_creg(creg)\n            # Save the support of the operation we add to the layer\n            support_list = []\n            # Operation data\n            op = copy.copy(next_node.op)\n            qa = copy.copy(next_node.qargs)\n            ca = copy.copy(next_node.cargs)\n            co = copy.copy(next_node.condition)\n            _ = self._bits_in_condition(co)\n\n            # Add node to new_layer\n            new_layer.apply_operation_back(op, qa, ca, co)\n            # Add operation to partition\n            if next_node.name not in [\"barrier\",\n                                      \"snapshot\", \"save\", \"load\", \"noise\"]:\n                support_list.append(list(qa))\n            l_dict = {\"graph\": new_layer, \"partition\": support_list}\n            yield l_dict","95":"def remove_subscriber(self, ws):\n        \"\"\"\n        Remove a websocket subscriber.\n\n        ws -- the websocket\n        \"\"\"\n        if ws in self.subscribers:\n            self.subscribers.remove(ws)\n\n        for name in self.available_events:\n            self.remove_event_subscriber(name, ws)","96":"def add_from_depend(self, node, from_module):\n        \"\"\"add dependencies created by from-imports\n        \"\"\"\n        mod_name = node.root().name\n        obj = self.module(mod_name)\n        if from_module not in obj.node.depends:\n            obj.node.depends.append(from_module)","97":"def lost_dimensions(point_fmt_in, point_fmt_out):\n    \"\"\"  Returns a list of the names of the dimensions that will be lost\n    when converting from point_fmt_in to point_fmt_out\n    \"\"\"\n\n    unpacked_dims_in = PointFormat(point_fmt_in).dtype\n    unpacked_dims_out = PointFormat(point_fmt_out).dtype\n\n    out_dims = unpacked_dims_out.fields\n    completely_lost = []\n    for dim_name in unpacked_dims_in.names:\n        if dim_name not in out_dims:\n            completely_lost.append(dim_name)\n    return completely_lost","98":"def set_pois(self, category, maxdist, maxitems, x_col, y_col):\n        \"\"\"\n        Set the location of all the pois of this category. The pois are\n        connected to the closest node in the Pandana network which assumes\n        no impedance between the location of the variable and the location\n        of the closest network node.\n\n        Parameters\n        ----------\n        category : string\n            The name of the category for this set of pois\n        maxdist - the maximum distance that will later be used in\n            find_all_nearest_pois\n        maxitems - the maximum number of items that will later be requested\n            in find_all_nearest_pois\n        x_col : Pandas Series (float)\n            The x location (longitude) of pois in this category\n        y_col : Pandas Series (Float)\n            The y location (latitude) of pois in this category\n\n        Returns\n        -------\n        Nothing\n        \"\"\"\n        if category not in self.poi_category_names:\n            self.poi_category_names.append(category)\n\n        self.max_pois = maxitems\n\n        node_ids = self.get_node_ids(x_col, y_col)\n\n        self.poi_category_indexes[category] = node_ids.index\n\n        node_idx = self._node_indexes(node_ids)\n\n        self.net.initialize_category(maxdist, maxitems, category.encode('utf-8'), node_idx.values)","99":"def blocking(indices, block_size, initial_boundary=0):\n    \"\"\"\n    Split list of integers into blocks of block_size and return block indices.\n\n    First block element will be located at initial_boundary (default 0).\n\n    >>> blocking([0, -1, -2, -3, -4, -5, -6, -7, -8, -9], 8)\n    [0,-1]\n    >>> blocking([0], 8)\n    [0]\n    >>> blocking([0], 8, initial_boundary=32)\n    [-4]\n    \"\"\"\n    blocks = []\n\n    for idx in indices:\n        bl_idx = (idx-initial_boundary)\/\/float(block_size)\n        if bl_idx not in blocks:\n            blocks.append(bl_idx)\n    blocks.sort()\n\n    return blocks","100":"def get_course(self, course_id, params={}):\n        \"\"\"\n        Return course resource for given canvas course id.\n\n        https:\/\/canvas.instructure.com\/doc\/api\/courses.html#method.courses.show\n        \"\"\"\n        include = params.get(\"include\", [])\n        if \"term\" not in include:\n            include.append(\"term\")\n        params[\"include\"] = include\n\n        url = COURSES_API.format(course_id)\n        return CanvasCourse(data=self._get_resource(url, params=params))","101":"def get_sections_with_students_in_course(self, course_id, params={}):\n        \"\"\"\n        Return list of sections including students for the passed course ID.\n        \"\"\"\n        include = params.get(\"include\", [])\n        if \"students\" not in include:\n            include.append(\"students\")\n        params[\"include\"] = include\n\n        return self.get_sections_in_course(course_id, params)","102":"def unregisterObserver(self, observer):\n        \"\"\" Remove an observer from the meter update() chain.\n\n        Args:\n            observer (MeterObserver): Subclassed MeterObserver.\n        \"\"\"\n        if observer in self.m_observers:\n            self.m_observers.remove(observer)\n        pass","103":"def add_path(self, path):\n        \"\"\"\n        Adds a path to search through when attempting to look up a module.\n\n        :param path: the path the add to the list of searchable paths\n        \"\"\"\n        if path not in self.paths:\n            self.paths.append(path)","104":"def fix_compile(remove_flags):\n    \"\"\"\n    Monkey-patch compiler to allow for removal of default compiler flags.\n    \"\"\"\n    import distutils.ccompiler\n\n    def _fix_compile(self, sources, output_dir=None, macros=None, include_dirs=None, debug=0,\n            extra_preargs=None, extra_postargs=None, depends=None):\n        for flag in remove_flags:\n            if flag in self.compiler_so:\n                self.compiler_so.remove(flag)\n        macros, objects, extra_postargs, pp_opts, build = self._setup_compile(output_dir, macros,\n                include_dirs, sources, depends, extra_postargs)\n        cc_args = self._get_cc_args(pp_opts, debug, extra_preargs)\n        for obj in objects:\n            try:\n                src, ext = build[obj]\n            except KeyError:\n                continue\n            self._compile(obj, src, ext, cc_args, extra_postargs, pp_opts)\n        return objects\n\n    distutils.ccompiler.CCompiler.compile = _fix_compile","105":"def get_active_stats(self):\n        \"\"\"\n        Returns all of the active statistics for the gadgets currently registered.\n        \"\"\"\n        stats = []\n        for gadget in self._registry.values():\n            for s in gadget.stats:\n                if s not in stats:\n                    stats.append(s)\n        return stats","106":"def _uniquify(_list):\n    \"\"\"Remove duplicates in a list.\"\"\"\n    seen = set()\n    result = []\n    for x in _list:\n        if x not in seen:\n            result.append(x)\n            seen.add(x)\n    return result","107":"def walk_egg(egg_dir):\n    \"\"\"Walk an unpacked egg's contents, skipping the metadata directory\"\"\"\n    walker = os.walk(egg_dir)\n    base,dirs,files = walker.next()\n    if 'EGG-INFO' in dirs:\n        dirs.remove('EGG-INFO')\n    yield base,dirs,files\n    for bdf in walker:\n        yield bdf","108":"def register_transformer(self, transformer):\n        \"\"\"Register a transformer instance.\"\"\"\n        if transformer not in self._transformers:\n            self._transformers.append(transformer)\n            self.sort_transformers()","109":"def unregister_transformer(self, transformer):\n        \"\"\"Unregister a transformer instance.\"\"\"\n        if transformer in self._transformers:\n            self._transformers.remove(transformer)","110":"def register_checker(self, checker):\n        \"\"\"Register a checker instance.\"\"\"\n        if checker not in self._checkers:\n            self._checkers.append(checker)\n            self.sort_checkers()","111":"def unregister_checker(self, checker):\n        \"\"\"Unregister a checker instance.\"\"\"\n        if checker in self._checkers:\n            self._checkers.remove(checker)","112":"def uniq_stable(elems):\n    \"\"\"uniq_stable(elems) -> list\n\n    Return from an iterable, a list of all the unique elements in the input,\n    but maintaining the order in which they first appear.\n\n    A naive solution to this problem which just makes a dictionary with the\n    elements as keys fails to respect the stability condition, since\n    dictionaries are unsorted by nature.\n\n    Note: All elements in the input must be valid dictionary keys for this\n    routine to work, as it internally uses a dictionary for efficiency\n    reasons.\"\"\"\n\n    unique = []\n    unique_dict = {}\n    for nn in elems:\n        if nn not in unique_dict:\n            unique.append(nn)\n            unique_dict[nn] = None\n    return unique","113":"def check_arguments(cls, passed):\n        \"\"\"Put warnings of arguments whose can't be handle by the class\"\"\"\n        defaults = list(cls.default_arguments().keys())\n        template = (\"Pass arg {argument:!r} in {cname:!r}, can be a typo? \"\n                    \"Supported key arguments: {defaults}\")\n        fails = []\n        for arg in passed:\n            if arg not in defaults:\n                warn(template.format(argument=arg,\n                                     cname=cls.__name__,\n                                     defaults=defaults))\n                fails.append(arg)\n\n        return any(fails)","114":"def register_service(self, service):\n        \"\"\"\n            Register service into the system. Called by Services.\n        \"\"\"\n        if service not in self.services:\n            self.services.append(service)","115":"def solve(m,c):\n    \"\"\"\n    run the algorithm to find the path list\n    \"\"\"\n    G={ (m,c,1):[] }\n    frontier=[ (m,c,1) ]  # 1 as boat starts on left bank \n    while len(frontier) > 0:\n        hold=list(frontier)\n        for node in hold:\n            newnode=[]\n            frontier.remove(node)\n            newnode.extend(pick_next_boat_trip(node, m,c, frontier))\n            for neighbor in newnode:\n                if neighbor not in G:\n                    G[node].append(neighbor)\n                    G[neighbor]=[node]\n                    frontier.append(neighbor)\n    return mod_plan.find_path_BFS(G,(m,c,1),(0,0,0))","116":"def remove_update_callback(self, callback, device):\n        \"\"\" Remove a registered update callback. \"\"\"\n        if [callback, device] in self._update_callbacks:\n            self._update_callbacks.remove([callback, device])\n            _LOGGER.debug('Removed update callback %s for %s',\n                          callback, device)","117":"def _append_to(self, field, element):\n        \"\"\"Append the ``element`` to the ``field`` of the record.\n\n        This method is smart: it does nothing if ``element`` is empty and\n        creates ``field`` if it does not exit yet.\n\n        Args:\n            :param field: the name of the field of the record to append to\n            :type field: string\n            :param element: the element to append\n        \"\"\"\n        if element not in EMPTIES:\n            self.obj.setdefault(field, [])\n            self.obj.get(field).append(element)","118":"def parse_glob(path, included):\n\t\"\"\"Parse a glob.\"\"\"\n\tfiles = glob.glob(path, recursive=True)\n\n\tarray = []\n\n\tfor file in files:\n\t\tfile = os.path.abspath(file)\n\t\tif file not in included:\n\t\t\tarray.append(file)\n\n\tincluded += array\n\treturn array","119":"def _generate_notebook_by_tag_body(notebook_object, dict_by_tag):\n    \"\"\"\n    Internal function that is used for generation of the page where notebooks are organized by\n    tag values.\n\n    ----------\n    Parameters\n    ----------\n    notebook_object : notebook object\n        Object of \"notebook\" class where the body will be created.\n\n    dict_by_tag : dict\n        Dictionary where each key is a tag and the respective value will be a list containing the\n        Notebooks (title and filename) that include this tag.\n\n    \"\"\"\n\n    tag_keys = list(dict_by_tag.keys())\n    tag_keys.sort()\n    for tag in tag_keys:\n        if tag.lower() not in SIGNAL_TYPE_LIST:\n            markdown_cell = group_tag_code.TAG_TABLE_HEADER\n            markdown_cell = markdown_cell.replace(\"Tag i\", tag)\n            for notebook_file in dict_by_tag[tag]:\n                split_path = notebook_file.split(\"\\\\\")\n                notebook_type = split_path[-2]\n                notebook_name = split_path[-1].split(\"&\")[0]\n                notebook_title = split_path[-1].split(\"&\")[1]\n                markdown_cell += \"\\t<tr>\\n\\t\\t<td width='20%' class='header_image_color_\" + \\\n                                 str(NOTEBOOK_KEYS[notebook_type]) + \"'><img \" \\\n                                 \"src='..\/..\/images\/icons\/\" + notebook_type.title() +\\\n                                 \".png' width='15%'>\\n\\t\\t<\/td>\"\n                markdown_cell += \"\\n\\t\\t<td width='60%' class='center_cell open_cell_light'>\" + \\\n                                 notebook_title + \"\\n\\t\\t<\/td>\"\n                markdown_cell += \"\\n\\t\\t<td width='20%' class='center_cell'>\\n\\t\\t\\t<a href='\" \\\n                                 \"..\/\" + notebook_type.title() + \"\/\" + notebook_name + \\\n                                 \"'><div class='file_icon'><\/div><\/a>\\n\\t\\t<\/td>\\n\\t<\/tr>\"\n\n            markdown_cell += \"<\/table>\"\n\n            # ==================== Insertion of HTML table in a new Notebook cell ======================\n            notebook_object[\"cells\"].append(nb.v4.new_markdown_cell(markdown_cell))","120":"def _inv_key(list_keys, valid_keys):\n    \"\"\"\n    -----\n    Brief\n    -----\n    A sub-function of _filter_keywords function.\n\n    -----------\n    Description\n    -----------\n    Function used for identification when a list of keywords contains invalid keywords not present\n    in the valid list.\n\n    ----------\n    Parameters\n    ----------\n    list_keys : list\n        List of keywords that must be verified, i.e., all the inputs needs to be inside valid_keys\n        in order to a True boolean be returned.\n\n    valid_keys : list\n        List of valid keywords.\n\n    Returns\n    -------\n    out : boolean, list\n        Boolean indicating if all the inserted keywords are valid. If true a list with invalid\n        keywords will be returned.\n    \"\"\"\n\n    inv_keys = []\n    bool_out = True\n    for i in list_keys:\n        if i not in valid_keys:\n            bool_out = False\n            inv_keys.append(i)\n\n    return bool_out, inv_keys","121":"def dedupe_list(l):\n    \"\"\"Remove duplicates from a list preserving the order.\n\n    We might be tempted to use the list(set(l)) idiom, but it doesn't preserve\n    the order, which hinders testability and does not work for lists with\n    unhashable elements.\n    \"\"\"\n    result = []\n\n    for el in l:\n        if el not in result:\n            result.append(el)\n\n    return result","122":"def move_to_result(self, lst_idx):\n        \"\"\"Moves element from lst available at lst_idx.\"\"\"\n        self.in_result_idx.add(lst_idx)\n\n        if lst_idx in self.not_in_result_root_match_idx:\n            self.not_in_result_root_match_idx.remove(lst_idx)","123":"def subscribe(self, event, hook):\n        \"\"\"\n        Subscribe a callback to an event\n\n        Parameters\n        ----------\n        event : str\n            Available events are 'precall', 'postcall', and 'capacity'.\n            precall is called with: (connection, command, query_kwargs)\n            postcall is called with: (connection, command, query_kwargs, response)\n            capacity is called with: (connection, command, query_kwargs, response, capacity)\n        hook : callable\n\n        \"\"\"\n        if hook not in self._hooks[event]:\n            self._hooks[event].append(hook)","124":"def unsubscribe(self, event, hook):\n        \"\"\" Unsubscribe a hook from an event \"\"\"\n        if hook in self._hooks[event]:\n            self._hooks[event].remove(hook)","125":"def add_rate_limit(self, limiter):\n        \"\"\" Add a RateLimit to the connection \"\"\"\n        if limiter not in self.rate_limiters:\n            self.subscribe('capacity', limiter.on_capacity)\n            self.rate_limiters.append(limiter)","126":"def remove_rate_limit(self, limiter):\n        \"\"\" Remove a RateLimit from the connection \"\"\"\n        if limiter in self.rate_limiters:\n            self.unsubscribe('capacity', limiter.on_capacity)\n            self.rate_limiters.remove(limiter)","127":"def find_flag_alias(self, flag):\n        \"\"\"Return alias set of a flag; return None if flag is not defined in\n        \"Options\".\n        \"\"\"\n        for each in self.opt_names:\n            if flag in each:\n                result = set(each)  # a copy\n                result.remove(flag)\n                return result\n        return None","128":"def task_finished(self, watchdog):\n\t\t\"\"\" Handle\/process scheduled task stop\n\n\t\t:param watchdog: watchdog of task that was stopped\n\n\t\t:return: None\n\t\t\"\"\"\n\t\tif watchdog in self.__running_registry:  # when cleanup hits stop\n\t\t\tself.__running_registry.remove(watchdog)\n\t\t\tself.__done_registry.append(watchdog)\n\t\t\tself.cleanup_event().set()","129":"def merge_lists(l, base):\n    \"\"\"\n    Merge in undefined list entries from given list.\n    \n    @param l: List to be merged into.\n    @type l: list\n    \n    @param base: List to be merged into.\n    @type base: list\n    \"\"\"\n    \n    for i in base:\n        if i not in l:\n            l.append(i)","130":"def addFilename( self, filename ):\n        \"\"\"\n        Adds a new filename to the top of the list.  If the filename is \\\n        already loaded, it will be moved to the front of the list.\n        \n        :param          filename | <str>\n        \"\"\"\n        filename = os.path.normpath(nativestring(filename))\n        \n        if filename in self._filenames:\n            self._filenames.remove(filename)\n        \n        self._filenames.insert(0, filename)\n        self._filenames = self._filenames[:self.maximumLength()]\n        \n        self.refresh()","131":"def pop(cls, anchors):\n        \"\"\"\n        Args:\n            anchors (str | unicode | list): Optional paths to use as anchors for short()\n        \"\"\"\n        for anchor in flattened(anchors, split=SANITIZED | UNIQUE):\n            if anchor in cls.paths:\n                cls.paths.remove(anchor)","132":"def remove_role(self, role):\n        \"\"\" Remove role from user \"\"\"\n        if role in self.__roles:\n            self.__roles.remove(role)","133":"def _prune_maps_to_sequences(self):\n        ''' When we merge the SIFTS maps, we can extend the sequence maps such that they have elements in their domain that we removed\n            from the sequence e.g. 1A2P, residue 'B   3 ' is removed because Rosetta barfs on it. Here, we prune the maps so that their\n            domains do not have elements that were removed from sequences.'''\n\n        for c, seq in self.atom_sequences.iteritems():\n            res_ids = [r[0] for r in seq]\n            for_removal = []\n            for k, _, _ in self.atom_to_seqres_sequence_maps[c]:\n                if k not in res_ids:\n                    for_removal.append(k)\n            for res_id in for_removal:\n                self.atom_to_seqres_sequence_maps[c].remove(res_id)","134":"def add(self, spec):\n        \"\"\"\n        Add limitations of given spec to self's.\n\n        Args:\n            spec (PackageSpec): another spec.\n        \"\"\"\n        for limit in spec.limit_to:\n            if limit not in self.limit_to:\n                self.limit_to.append(limit)","135":"def get_trending_daily_not_starred(self):\n        \"\"\"Gets trending repositories NOT starred by user\n        :return: List of daily-trending repositories which are not starred\n        \"\"\"\n        trending_daily = self.get_trending_daily()  # repos trending daily\n        starred_repos = self.get_starred_repos()  # repos starred by user\n        repos_list = []\n\n        for repo in trending_daily:\n            if repo not in starred_repos:\n                repos_list.append(repo)\n\n        return repos_list","136":"def logged_exception(self, e):\n        \"\"\"Record the exception, but don't log it; it's already been logged\n\n        :param e:  Exception to log.\n\n        \"\"\"\n        if str(e) not in self._errors:\n            self._errors.append(str(e))\n\n        self.set_error_state()\n        self.buildstate.state.exception_type = str(e.__class__.__name__)\n        self.buildstate.state.exception = str(e)","137":"def warn(self, message):\n        \"\"\"Log an error messsage.\n\n        :param message:  Log message.\n\n        \"\"\"\n        if message not in self._warnings:\n            self._warnings.append(message)\n\n        self.logger.warn(message)","138":"def delete_metric(name):\n    \"\"\"Remove the named metric\"\"\"\n\n    with LOCK:\n        old_metric = REGISTRY.pop(name, None)\n\n        # look for the metric name in the tags and remove it\n        for _, tags in py3comp.iteritems(TAGS):\n            if name in tags:\n                tags.remove(name)\n\n    return old_metric","139":"def add_input_file(self, filename):\n    \"\"\"\n    Add filename as a necessary input file for this DAG node.\n\n    @param filename: input filename to add\n    \"\"\"\n    if filename not in self.__input_files:\n      self.__input_files.append(filename)","140":"def add_output_file(self, filename):\n    \"\"\"\n    Add filename as a output file for this DAG node.\n\n    @param filename: output filename to add\n    \"\"\"\n    if filename not in self.__output_files:\n      self.__output_files.append(filename)","141":"def add_checkpoint_file(self, filename):\n    \"\"\"\n    Add filename as a checkpoint file for this DAG job.\n    \"\"\"\n    if filename not in self.__checkpoint_files:\n        self.__checkpoint_files.append(filename)","142":"def add_file_arg(self, filename):\n    \"\"\"\n    Add a file argument to the executable. Arguments are appended after any\n    options and their order is guaranteed. Also adds the file name to the\n    list of required input data for this job.\n    @param filename: file to add as argument.\n    \"\"\"\n    self.__arguments.append(filename)\n    if filename not in self.__input_files:\n      self.__input_files.append(filename)","143":"def add_file_opt(self, opt, filename):\n    \"\"\"\n    Add a command line option to the executable. The order that the arguments\n    will be appended to the command line is not guaranteed, but they will\n    always be added before any command line arguments. The name of the option\n    is prefixed with double hyphen and the program is expected to parse it\n    with getopt_long().\n    @param opt: command line option to add.\n    @param value: value to pass to the option (None for no argument).\n    \"\"\"\n    self.__options[opt] = filename\n    if filename not in self.__input_files:\n      self.__input_files.append(filename)","144":"def add_var_condor_cmd(self, command):\n    \"\"\"\n    Add a condor command to the submit file that allows variable (macro)\n    arguments to be passes to the executable.\n    \"\"\"\n    if command not in self.__var_cmds:\n        self.__var_cmds.append(command)\n        macro = self.__bad_macro_chars.sub( r'', command )\n        self.add_condor_cmd(command, '$(macro' + macro + ')')","145":"def unique(self):\n\t\t\"\"\"\n\t\tReturn a Cache which has every element of self, but without\n\t\tduplication.  Preserve order.  Does not hash, so a bit slow.\n\t\t\"\"\"\n\t\tnew = self.__class__([])\n\t\tfor elem in self:\n\t\t\tif elem not in new:\n\t\t\t\tnew.append(elem)\n\t\treturn new","146":"def _insert(self, trigram):\n        \"\"\"\n        Insert a trigram in the DB\n        \"\"\"\n        words = list(map(self._sanitize, trigram))\n\n        key = self._WSEP.join(words[:2]).lower()\n        next_word = words[2]\n\n        self._db.setdefault(key, [])\n        # we could use a set here, but sets are not serializables in JSON. This\n        # is the same reason we use dicts instead of defaultdicts.\n        if next_word not in self._db[key]:\n            self._db[key].append(next_word)","147":"def add_value(self, value, index_point):\r\n        \"\"\"The function is addeing new value to provied index. If index does not exist\"\"\"\r\n        if index_point not in self.index:\r\n            self.values.append(value)\r\n            self.index.append(index_point)","148":"def add_tags(self, archive_name, tags):\n        '''\n        Add tags to an archive\n\n        Parameters\n        ----------\n        archive_name:s tr\n            Name of archive\n\n        tags: list or tuple of strings\n            tags to add to the archive\n\n        '''\n        updated_tag_list = list(self._get_tags(archive_name))\n        for tag in tags:\n            if tag not in updated_tag_list:\n                updated_tag_list.append(tag)\n\n        self._set_tags(archive_name, updated_tag_list)","149":"def delete_tags(self, archive_name, tags):\n        '''\n        Delete tags from an archive\n\n        Parameters\n        ----------\n        archive_name:s tr\n            Name of archive\n\n        tags: list or tuple of strings\n            tags to delete from the archive\n\n        '''\n        updated_tag_list = list(self._get_tags(archive_name))\n        for tag in tags:\n            if tag in updated_tag_list:\n                updated_tag_list.remove(tag)\n\n        self._set_tags(archive_name, updated_tag_list)","150":"def unique_list(lst):\n    \"\"\"Make a list unique, retaining order of initial appearance.\"\"\"\n    uniq = []\n    for item in lst:\n        if item not in uniq:\n            uniq.append(item)\n    return uniq","151":"def remove(self, observableElement):\n        \"\"\"\n        remove an obsrvable element\n\n        :param str observableElement: the name of the observable element\n        \"\"\"\n        if observableElement in self._observables:\n            self._observables.remove(observableElement)","152":"def setActiveModule(Module):\r\n  r\"\"\"Helps with collecting the members of the imported modules.\r\n  \"\"\"\r\n  module_name = Module.__name__\r\n\r\n  if module_name not in ModuleMembers:\r\n    ModuleMembers[module_name] = []\r\n    ModulesQ.append(module_name)\r\n    Group(Module, {}) # brand the module with __ec_member__\r\n\r\n  state.ActiveModuleMemberQ = ModuleMembers[module_name]","153":"def listMemberHelps(TargetGroup):\n  r\"\"\"Gets help on a group's children.\n  \"\"\"\n  Members = []\n\n  for Member in TargetGroup.Members.values(): # get unique children (by discarding aliases)\n    if Member not in Members:\n      Members.append(Member)\n\n  Ret = []\n\n  for Member in Members:\n    Config = Member.Config\n    Ret.append(('%s%s' % (Config['name'], ', %s' % Config['alias'] if 'alias' in Config else ''), Config.get('desc', '')))\n\n  return Ret","154":"def google_cloud_datastore_delete_expired_sessions(dormant_for=86400, limit=500):\n    \"\"\"\n    Deletes expired sessions\n    A session is expired if it expires date is set and has passed or\n    if it has not been accessed for a given period of time.\n\n    :param dormant_for: seconds since last access to delete sessions, defaults to 24 hours.\n    :type dormant_for: int\n    :param limit: amount to delete in one call of the method, the maximum and default for this is the NDB fetch limit of 500\n    :type limit: int\n    \"\"\"\n    from vishnu.backend.client.google_cloud_datastore import TABLE_NAME\n    from google.cloud import datastore\n    from datetime import datetime\n    from datetime import timedelta\n\n    now = datetime.utcnow()\n    last_accessed = now - timedelta(seconds=dormant_for)\n\n    client = datastore.Client()\n    accessed_query = client.query(kind=TABLE_NAME)\n    accessed_query.add_filter(\"last_accessed\", \"<=\", last_accessed)\n    accessed_results = accessed_query.fetch(limit=limit)\n\n    expires_query = client.query(kind=TABLE_NAME)\n    expires_query.add_filter(\"expires\", \"<=\", now)\n    expires_results = expires_query.fetch(limit=limit)\n\n    keys = list()\n    for result in accessed_results:\n        keys.append(result.key)\n    for result in expires_results:\n        if result.key not in keys:\n            keys.append(result.key)\n\n    client.delete_multi(keys)\n\n    return len(keys) < limit","155":"def remove_socket(self, socket):\n        \"\"\"\n        Remove a socket from the multiplexer.\n\n        :param socket: The socket. If it was removed already or if it wasn't\n            added, the call does nothing.\n        \"\"\"\n        if socket in self._sockets:\n            socket.on_closed.disconnect(self.remove_socket)\n            self._sockets.remove(socket)","156":"def close_monomers(self, group, cutoff=4.0):\n        \"\"\"Returns a list of Monomers from within a cut off distance of the Monomer\n\n        Parameters\n        ----------\n        group: BaseAmpal or Subclass\n            Group to be search for Monomers that are close to this Monomer.\n        cutoff: float\n            Distance cut off.\n\n        Returns\n        -------\n        nearby_residues: [Monomers]\n            List of Monomers within cut off distance.\n        \"\"\"\n        nearby_residues = []\n        for self_atom in self.atoms.values():\n            nearby_atoms = group.is_within(cutoff, self_atom)\n            for res_atom in nearby_atoms:\n                if res_atom.parent not in nearby_residues:\n                    nearby_residues.append(res_atom.parent)\n        return nearby_residues","157":"async def spawn(self):\n        \"\"\"Spawn the command wrapped in this object as a subprocess.\"\"\"\n        self._server._pending_set.add(self)\n        await self._server._sem.acquire()\n        self._subprocess = await asyncio.create_subprocess_shell(\n            self._cmd,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE)\n        self._began_at = datetime.now()\n        if self in self._server._pending_set:\n            self._server._pending_set.remove(self)\n        self._server._running_set.add(self)\n        self._began_running_evt.set()","158":"def _generate_iam_role_policy(self):\n        \"\"\"\n        Generate the policy for the IAM Role.\n\n        Terraform name: aws_iam_role.lambda_role\n        \"\"\"\n        endpoints = self.config.get('endpoints')\n        queue_arns = []\n        for ep in endpoints:\n            for qname in endpoints[ep]['queues']:\n                qarn = 'arn:aws:sqs:%s:%s:%s' % (self.aws_region,\n                                                 self.aws_account_id, qname)\n                if qarn not in queue_arns:\n                    queue_arns.append(qarn)\n        pol = {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\n                    \"Effect\": \"Allow\",\n                    \"Action\": \"logs:CreateLogGroup\",\n                    \"Resource\": \"arn:aws:logs:%s:%s:*\" % (\n                        self.aws_region, self.aws_account_id\n                    )\n                },\n                {\n                    \"Effect\": \"Allow\",\n                    \"Action\": [\n                        \"logs:CreateLogStream\",\n                        \"logs:PutLogEvents\"\n                    ],\n                    \"Resource\": [\n                        \"arn:aws:logs:%s:%s:log-group:%s:*\" % (\n                            self.aws_region, self.aws_account_id,\n                            '\/aws\/lambda\/%s' % self.resource_name\n                        )\n                    ]\n                },\n                {\n                    'Effect': 'Allow',\n                    'Action': [\n                        'sqs:ListQueues'\n                    ],\n                    'Resource': '*'\n                },\n                {\n                    \"Effect\": \"Allow\",\n                    \"Action\": [\n                        \"sqs:GetQueueUrl\",\n                        \"sqs:SendMessage\"\n                    ],\n                    \"Resource\": sorted(queue_arns)\n                }\n            ]\n        }\n        self.tf_conf['resource']['aws_iam_role_policy']['role_policy'] = {\n            'name': self.resource_name,\n            'role': '${aws_iam_role.lambda_role.id}',\n            'policy': json.dumps(pol)\n        }","159":"async def destroy_attachment(self, a: Attachment):\n        \"\"\" destroy a match attachment\n\n        |methcoro|\n\n        Args:\n            a: the attachment you want to destroy\n\n        Raises:\n            APIException\n\n        \"\"\"\n        await self.connection('DELETE', 'tournaments\/{}\/matches\/{}\/attachments\/{}'.format(self._tournament_id, self._id, a._id))\n        if a in self.attachments:\n            self.attachments.remove(a)","160":"async def remove_participant(self, p: Participant):\n        \"\"\" remove a participant from the tournament\n\n        |methcoro|\n\n        Args:\n            p: the participant to remove\n\n        Raises:\n            APIException\n\n        \"\"\"\n        await self.connection('DELETE', 'tournaments\/{}\/participants\/{}'.format(self._id, p._id))\n        if p in self.participants:\n            self.participants.remove(p)","161":"async def destroy_tournament(self, t: Tournament):\n        \"\"\" completely removes a tournament from Challonge\n\n        |methcoro|\n\n        Note:\n            |from_api| Deletes a tournament along with all its associated records. There is no undo, so use with care!\n\n        Raises:\n            APIException\n\n        \"\"\"\n        await self.connection('DELETE', 'tournaments\/{}'.format(t.id))\n        if t in self.tournaments:\n            self.tournaments.remove(t)","162":"def remove_die(self, die):\n        '''Remove ``Die`` (first matching) from Roll.\n        :param die: Die instance\n        '''\n        if die in self._dice:\n            self._dice.remove(die)","163":"def add_user(self, nick, prefixes=None):\n        \"\"\"Add a user to our internal list of nicks.\"\"\"\n        if nick not in self._user_nicks:\n            self._user_nicks.append(nick)\n\n        self.prefixes[nick] = prefixes","164":"def exclude_invert(self):\n        # type:() -> None\n        \"\"\"\n        Inverts the values in self.exclude\n\n        .. code-block:: python\n\n            >>> import pydarksky\n            >>> darksky = pydarksky.DarkSky('0' * 32)\n\n            >>> darksky.EXCLUDES\n            ('currently', 'minutely', 'hourly', 'daily', 'alerts', 'flags')\n\n            >>> darksky.exclude = [\"alerts\", \"flags\"]\n\n            >>> darksky.exclude\n            ['alerts', 'flags']\n\n            >>> darksky.exclude_invert()\n\n            >>> darksky.exclude\n            ['currently', 'minutely', 'hourly', 'daily']\n\n        \"\"\"\n        tmp = self.exclude\n        self._exclude = []\n        for i in self.EXCLUDES:\n            if i not in tmp:\n                self._exclude.append(i)","165":"def _positionalArgumentKeyValueList(self,\n                                        originalConstructorExpectedArgList,\n                                        syntheticMemberList,\n                                        argTuple):\n        \"\"\"Transforms args tuple to a dictionary mapping argument names to values using original constructor\npositional args specification, then it adds synthesized members at the end if they are not already present.\n    :type syntheticMemberList: list(SyntheticMember)\n    :type argTuple: tuple\n\"\"\"\n\n        # First, the list of expected arguments is set to original constructor's arg spec. \n        expectedArgList = copy.copy(originalConstructorExpectedArgList)\n\n        # ... then we append members that are not already present.\n        for syntheticMember in syntheticMemberList:\n            memberName = syntheticMember.memberName()\n            if memberName not in expectedArgList:\n                expectedArgList.append(memberName)\n\n        # Makes a list of tuples (argumentName, argumentValue) with each element of each list (expectedArgList, argTuple)\n        # until the shortest list's end is reached.\n        positionalArgumentKeyValueList = list(zip(expectedArgList, argTuple))\n\n        # Add remanining arguments (those that are not expected by the original constructor).\n        for argumentValue in argTuple[len(positionalArgumentKeyValueList):]:\n            positionalArgumentKeyValueList.append((None, argumentValue))\n\n        return positionalArgumentKeyValueList","166":"def wrapper__ignore(self, type_):\n        \"\"\"\n        Selectively ignore certain types when wrapping attributes.\n\n        :param class type: The class\/type definition to ignore.\n\n        :rtype list(type): The current list of ignored types\n        \"\"\"\n        if type_ not in self.__exclusion_list:\n            self.__exclusion_list.append(type_)\n        return self.__exclusion_list","167":"def wrapper__unignore(self, type_):\n        \"\"\"\n        Stop selectively ignoring certain types when wrapping attributes.\n\n        :param class type: The class\/type definition to stop ignoring.\n\n        :rtype list(type): The current list of ignored types\n        \"\"\"\n        if type_ in self.__exclusion_list:\n            self.__exclusion_list.remove( type_ )\n        return self.__exclusion_list","168":"def drop_stopwords(list):\n    \"\"\"\u53bb\u505c\u8bcd\n\n    Keyword arguments:\n    list            -- \u5217\u8868\u7c7b\u578b\n    Return:\n        \u4e0d\u542b\u505c\u8bcd\u7684list\n    \"\"\"\n    stopwords_list = []\n    with open(get_default_stop_words_file(), encoding='utf-8') as stopwords:\n        for line in stopwords:\n            stopwords_list.append(line.replace('\\n', ''))\n\n    list_clean = []\n    for i in list:\n        if i not in stopwords_list:\n            list_clean.append(i)\n    return list_clean","169":"def remove(self, item):\n        \"\"\"Remove an item from the set. Inverts the add operation.\n\n        >>> from ngram import NGram\n        >>> n = NGram(['spam', 'eggs'])\n        >>> n.remove('spam')\n        >>> list(n)\n        ['eggs']\n        \"\"\"\n        if item in self:\n            super(NGram, self).remove(item)\n            del self.length[item]\n            for ngram in self.splititem(item):\n                del self._grams[ngram][item]","170":"def removeStepListener(listener):\n    \"\"\"removeStepListener(traci.StepListener) -> bool\n\n    Remove the step listener from traci's step listener container.\n    Returns True if the listener was removed successfully, False if it wasn't registered.\n    \"\"\"\n    if listener in _stepListeners:\n        _stepListeners.remove(listener)\n        return True\n    warnings.warn(\n        \"removeStepListener(listener): listener %s not registered as step listener\" % str(listener))\n    return False","171":"def checkAndCreateClasses(self, classes):\n        \"\"\" Function checkAndCreateClasses\n        Check and add puppet class\n\n        @param classes: The classes ids list\n        @return RETURN: boolean\n        \"\"\"\n        actual_classes = self['puppetclasses'].keys()\n        for i in classes:\n            if i not in actual_classes:\n                self['puppetclasses'].append(i)\n        self.reload()\n        return set(classes).issubset(set((self['puppetclasses'].keys())))","172":"def register_alarm(self, alarm):\n        \"\"\"Register (create) an alarm.\n\n        :param AlarmType|list[AlarmType] alarm: Alarm.\n\n        \"\"\"\n        for alarm in listify(alarm):\n            if alarm not in self._alarms:\n                self._set('alarm', alarm, multi=True)\n                self._alarms.append(alarm)\n\n        return self._section","173":"def set_plugins_params(self, plugins=None, search_dirs=None, autoload=None, required=False):\n        \"\"\"Sets plugin-related parameters.\n\n        :param list|str|unicode|OptionsGroup|list[OptionsGroup] plugins: uWSGI plugins to load\n\n        :param list|str|unicode search_dirs: Directories to search for uWSGI plugins.\n\n        :param bool autoload: Try to automatically load plugins when unknown options are found.\n\n        :param bool required: Load uWSGI plugins and exit on error.\n\n        \"\"\"\n        plugins = plugins or []\n\n        command = 'need-plugin' if required else 'plugin'\n\n        for plugin in listify(plugins):\n\n            if plugin not in self._plugins:\n                self._set(command, plugin, multi=True)\n                self._plugins.append(plugin)\n\n        self._set('plugins-dir', search_dirs, multi=True, priority=0)\n        self._set('autoload', autoload, cast=bool)\n\n        return self","174":"def _parse_fields(self, result, field_name):\n        \"\"\" If Schema access, parse fields and build respective lists\n        \"\"\"\n        field_list = []\n        for key, value in result.get('schema', {}).get(field_name, {}).items():\n            if key not in field_list:\n                field_list.append(key)\n        return field_list","175":"def logs_map_and_reduce(logs, _map, _reduce):\n    \"\"\"\n    :type logs str[]\n    :type _map (list) -> str\n    :type _reduce (list) -> obj\n    \"\"\"\n    keys = []\n    mapped_count = Counter()\n    mapped = defaultdict(list)\n\n    # first map all entries\n    for log in logs:\n        key = _map(log)\n        mapped[key].append(log)\n        mapped_count[key] += 1\n\n        if key not in keys:\n            keys.append(key)\n\n    # the most common mapped item\n    top_count = mapped_count.most_common(1).pop()[1]\n\n    # now reduce mapped items\n    reduced = []\n\n    # keep the order under control\n    for key in keys:\n        entries = mapped[key]\n        # print(key, entries)\n\n        # add \"value\" field to each reduced item (1.0 will be assigned to the most \"common\" item)\n        item = _reduce(entries)\n        item['value'] = 1. * len(entries) \/ top_count\n\n        reduced.append(item)\n\n    # print(mapped)\n    return reduced","176":"def get_peer_ips(self):\n        '''\n        Generates list of peer IPs from tracker response. Note: not all of\n        these IPs might be good, which is why we only init peer objects for\n        the subset that respond to handshake\n        '''\n        presponse = [ord(i) for i in self.tracker_response['peers']]\n        while presponse:\n            peer_ip = (('.'.join(str(x) for x in presponse[0:4]),\n                       256 * presponse[4] + presponse[5]))\n            if peer_ip not in self.peer_ips:\n                self.peer_ips.append(peer_ip)\n            presponse = presponse[6:]","177":"def _handle_stop_workflow(self, request):\n        \"\"\" The handler for the stop_workflow request.\n\n        The stop_workflow request adds all running dags to the list of dags\n        that should be stopped and prevents new dags from being started. The dags will\n        then stop queueing new tasks, which will terminate the dags and in turn the\n        workflow.\n\n        Args:\n            request (Request): Reference to a request object containing the\n                               incoming request.\n\n        Returns:\n            Response: A response object containing the following fields:\n                          - success: True if the dags were added successfully to the list\n                                     of dags that should be stopped.\n        \"\"\"\n        self._stop_workflow = True\n        for name, dag in self._dags_running.items():\n            if name not in self._stop_dags:\n                self._stop_dags.append(name)\n        return Response(success=True, uid=request.uid)","178":"def stop(self, consumer):\n        \"\"\" This function is called when the worker received a request to terminate.\n\n        Upon the termination of the worker, the workflows for all running jobs are\n        stopped gracefully.\n\n        Args:\n            consumer (Consumer): Reference to the consumer object that handles messages\n                                 from the broker.\n        \"\"\"\n        stopped_workflows = []\n        for request in [r for r in consumer.controller.state.active_requests]:\n            job = AsyncResult(request.id)\n\n            workflow_id = job.result['workflow_id']\n            if workflow_id not in stopped_workflows:\n                client = Client(\n                    SignalConnection(**consumer.app.user_options['config'].signal,\n                                     auto_connect=True),\n                    request_key=workflow_id)\n                client.send(Request(action='stop_workflow'))\n\n                stopped_workflows.append(workflow_id)","179":"def get_unique_groups(input_list):\n    \"\"\"Function to get a unique list of groups.\"\"\"\n    out_list = []\n    for item in input_list:\n        if item not in out_list:\n            out_list.append(item)\n    return out_list","180":"def keep(self, keep_names):\n        \"\"\"Keeps variables (keep_names) while dropping other parameters\"\"\"\n        \n        current_names = self._data.columns\n        drop_names = []\n        for name in current_names:\n            if name not in keep_names:\n                drop_names.append(name)\n        self.drop(drop_names)","181":"def label_list_parser(self, url):\n        \"\"\"\n        Extracts comma separate tag=value pairs from a string\n        Assumes all characters other than \/ and , are valid\n        \"\"\"\n        labels = re.findall('([^\/,]+=[^\/,]+)', url)\n        slabels = set(labels)\n        if '' in slabels:\n            slabels.remove('')\n        return slabels","182":"def to_table_data(self):\n        \"\"\"\n        :raises ValueError:\n        :raises pytablereader.error.ValidationError:\n        \"\"\"\n\n        self._validate_source_data()\n\n        header_list = []\n        for json_record in self._buffer:\n            for key in json_record:\n                if key not in header_list:\n                    header_list.append(key)\n\n        self._loader.inc_table_count()\n\n        yield TableData(\n            self._make_table_name(),\n            header_list,\n            self._buffer,\n            dp_extractor=self._loader.dp_extractor,\n            type_hints=self._extract_type_hints(header_list),\n        )","183":"def get_pkglist():\n    \"\"\"\n    Return list of all installed packages\n\n    Note: It returns one project name per pkg no matter how many versions\n    of a particular package is installed\n\n    @returns: list of project name strings for every installed pkg\n\n    \"\"\"\n\n    dists = Distributions()\n    projects = []\n    for (dist, _active) in dists.get_distributions(\"all\"):\n        if dist.project_name not in projects:\n            projects.append(dist.project_name)\n    return projects","184":"def add_service(self, service_name):\n        \"\"\"\n        Add the given service to the manifest.\n        \"\"\"\n        if service_name not in self.manifest['services']:\n            self.manifest['services'].append(service_name)","185":"def restore_default(self, key):\n        \"\"\"\n        Restore (and return) default value for the specified key.\n\n        This method will only work for a ConfigObj that was created\n        with a configspec and has been validated.\n\n        If there is no default value for this key, ``KeyError`` is raised.\n        \"\"\"\n        default = self.default_values[key]\n        dict.__setitem__(self, key, default)\n        if key not in self.defaults:\n            self.defaults.append(key)\n        return default","186":"def get_raw_data(self, times=5):\n        \"\"\"\n        do some readings and aggregate them using the defined statistics function\n\n        :param times: how many measures to aggregate\n        :type times: int\n        :return: the aggregate of the measured values\n        :rtype float\n        \"\"\"\n\n        self._validate_measure_count(times)\n\n        data_list = []\n        while len(data_list) < times:\n            data = self._read()\n            if data not in [False, -1]:\n                data_list.append(data)\n\n        return data_list","187":"def queueTypeUpgrade(self, oldtype):\n        \"\"\"\n        Queue a type upgrade for C{oldtype}.\n        \"\"\"\n        if oldtype not in self._oldTypesRemaining:\n            self._oldTypesRemaining.append(oldtype)","188":"def main():\n    \"\"\"\n    Main function.\n\n    :return:\n        None.\n    \"\"\"\n    try:\n        # Get the `src` directory's absolute path\n        src_path = os.path.dirname(\n            # `aoiklivereload` directory's absolute path\n            os.path.dirname(\n                # `demo` directory's absolute path\n                os.path.dirname(\n                    # This file's absolute path\n                    os.path.abspath(__file__)\n                )\n            )\n        )\n\n        # If the `src` directory path is not in `sys.path`\n        if src_path not in sys.path:\n            # Add to `sys.path`.\n            #\n            # This aims to save user setting PYTHONPATH when running this demo.\n            #\n            sys.path.append(src_path)\n\n        # Import reloader class\n        from aoiklivereload import LiveReloader\n\n        # Create reloader\n        reloader = LiveReloader(\n            # Reload mode.\n            #\n            # In windows, have to use `spawn_exit` reload mode and force the\n            # current process to exit immediately, otherwise will get the\n            # error:\n            # ```\n            # OSError: [WinError 10048] Only one usage of each socket address\n            # (protocol\/network address\/port) is normally permitted\n            # ```\n            #\n            # Notice in `spawn_exit` reload mode, the user will not be able\n            # to kill the new process using Ctrl-c.\n            #\n            reload_mode=('spawn_exit' if sys.platform == 'win32' else 'exec'),\n            force_exit=True,\n        )\n\n        # Start watcher thread\n        reloader.start_watcher_thread()\n\n        # Server host\n        server_host = '0.0.0.0'\n\n        # Server port\n        server_port = 8000\n\n        # Get message\n        msg = '# ----- Run server -----\\nHost: {}\\nPort: {}'.format(\n            server_host, server_port\n        )\n\n        # Print message\n        print(msg)\n\n        # Create request handler\n        class HelloHandler(tornado.web.RequestHandler):\n            \"\"\"\n            Request handler class.\n            \"\"\"\n\n            def get(self):\n                \"\"\"\n                Request handler.\n\n                :return:\n                    None.\n                \"\"\"\n                # Write response body\n                self.write('hello')\n\n        # List of tuples that maps URL pattern to handler\n        handler_tuples = [\n            ('\/', HelloHandler),\n        ]\n\n        # Create Tornado app\n        tornado_app = tornado.web.Application(\n            handler_tuples,\n            # Disable Tornado's reloader\n            debug=False,\n        )\n\n        # Start listening\n        tornado_app.listen(server_port, address=server_host)\n\n        # Get event loop\n        io_loop = tornado.ioloop.IOLoop.current()\n\n        # Run event loop\n        io_loop.start()\n\n    # If have `KeyboardInterrupt`\n    except KeyboardInterrupt:\n        # Not treat as error\n        pass","189":"def main():\n    \"\"\"\n    Main function.\n\n    :return:\n        None.\n    \"\"\"\n    try:\n        # Get the `src` directory's absolute path\n        src_path = os.path.dirname(\n            # `aoiklivereload` directory's absolute path\n            os.path.dirname(\n                # `demo` directory's absolute path\n                os.path.dirname(\n                    # This file's absolute path\n                    os.path.abspath(__file__)\n                )\n            )\n        )\n\n        # If the `src` directory path is not in `sys.path`\n        if src_path not in sys.path:\n            # Add to `sys.path`.\n            #\n            # This aims to save user setting PYTHONPATH when running this demo.\n            #\n            sys.path.append(src_path)\n\n        # Import reloader class\n        from aoiklivereload import LiveReloader\n\n        # Create reloader\n        reloader = LiveReloader()\n\n        # Start watcher thread\n        reloader.start_watcher_thread()\n\n        # Server host\n        server_host = '0.0.0.0'\n\n        # Server port\n        server_port = 8000\n\n        # Get message\n        msg = '# ----- Run server -----\\nHost: {}\\nPort: {}'.format(\n            server_host, server_port\n        )\n\n        # Print message\n        print(msg)\n\n        # Create request handler\n        @bottle.get('\/')\n        def hello_handler():  # pylint: disable=unused-variable\n            \"\"\"\n            Request handler.\n\n            :return:\n                Response body.\n            \"\"\"\n            # Return response body\n            return 'hello'\n\n        # Run server\n        bottle.run(host=server_host, port=server_port)\n\n    # If have `KeyboardInterrupt`\n    except KeyboardInterrupt:\n        # Not treat as error\n        pass","190":"def main():\n    \"\"\"\n    Main function.\n\n    :return:\n        None.\n    \"\"\"\n    try:\n        # Get the `src` directory's absolute path\n        src_path = os.path.dirname(\n            # `aoiklivereload` directory's absolute path\n            os.path.dirname(\n                # `demo` directory's absolute path\n                os.path.dirname(\n                    # This file's absolute path\n                    os.path.abspath(__file__)\n                )\n            )\n        )\n\n        # If the `src` directory path is not in `sys.path`\n        if src_path not in sys.path:\n            # Add to `sys.path`.\n            #\n            # This aims to save user setting PYTHONPATH when running this demo.\n            #\n            sys.path.append(src_path)\n\n        # Import reloader class\n        from aoiklivereload import LiveReloader\n\n        # Create reloader\n        reloader = LiveReloader()\n\n        # Start watcher thread\n        reloader.start_watcher_thread()\n\n        # Server host\n        server_host = '0.0.0.0'\n\n        # Server port\n        server_port = 8000\n\n        # Get message\n        msg = '# ----- Run server -----\\nHost: {}\\nPort: {}'.format(\n            server_host, server_port\n        )\n\n        # Print message\n        print(msg)\n\n        # Create Flask app\n        flask_app = flask.Flask(__name__)\n\n        # Create request handler\n        @flask_app.route('\/')\n        def hello_handler():  # pylint: disable=unused-variable\n            \"\"\"\n            Request handler.\n\n            :return:\n                Response body.\n            \"\"\"\n            # Return response body\n            return 'hello'\n\n        # Run server\n        flask_app.run(\n            host=server_host,\n            port=server_port,\n            # Disable Flask's reloader\n            debug=False,\n        )\n\n    # If have `KeyboardInterrupt`\n    except KeyboardInterrupt:\n        # Not treat as error\n        pass","191":"def get_worksheet_keys(data_dict, result_info_key):\n    \"\"\"Gets sorted keys from the dict, ignoring result_info_key and 'meta' key\n    Args:\n        data_dict: dict to pull keys from\n\n    Returns:\n        list of keys in the dict other than the result_info_key\n    \"\"\"\n    keys = set(data_dict.keys())\n    keys.remove(result_info_key)\n    if 'meta' in keys:\n        keys.remove('meta')\n    return sorted(keys)","192":"def main():\n    \"\"\"\n    Main function.\n\n    :return:\n        None.\n    \"\"\"\n    try:\n        # Get the `src` directory's absolute path\n        src_path = os.path.dirname(\n            # `aoiklivereload` directory's absolute path\n            os.path.dirname(\n                # `demo` directory's absolute path\n                os.path.dirname(\n                    # This file's absolute path\n                    os.path.abspath(__file__)\n                )\n            )\n        )\n\n        # If the `src` directory path is not in `sys.path`\n        if src_path not in sys.path:\n            # Add to `sys.path`.\n            #\n            # This aims to save user setting PYTHONPATH when running this demo.\n            #\n            sys.path.append(src_path)\n\n        # Import reloader class\n        from aoiklivereload import LiveReloader\n\n        # Create reloader\n        reloader = LiveReloader()\n\n        # Start watcher thread\n        reloader.start_watcher_thread()\n\n        # Server host\n        server_host = '0.0.0.0'\n\n        # Server port\n        server_port = 8000\n\n        # Get message\n        msg = '# ----- Run server -----\\nHost: {}\\nPort: {}'.format(\n            server_host, server_port\n        )\n\n        # Print message\n        print(msg)\n\n        # Create Sanic app\n        sanic_app = Sanic()\n\n        # Create request handler\n        @sanic_app.route('\/')\n        async def hello_handler(request):  # pylint: disable=unused-variable\n            \"\"\"\n            Request handler.\n\n            :return:\n                Response body.\n            \"\"\"\n            # Return response body\n            return text('hello')\n\n        # Run server.\n        #\n        # Notice `KeyboardInterrupt` will be caught inside `sanic_app.run`.\n        #\n        sanic_app.run(\n            host=server_host,\n            port=server_port,\n        )\n\n    # If have `KeyboardInterrupt`\n    except KeyboardInterrupt:\n        # Not treat as error\n        pass","193":"def list_remove_repeat(x):\n    \"\"\"Remove the repeated items in a list, and return the processed list.\n    You may need it to create merged layer like Concat, Elementwise and etc.\n\n    Parameters\n    ----------\n    x : list\n        Input\n\n    Returns\n    -------\n    list\n        A list that after removing it's repeated items\n\n    Examples\n    -------\n    >>> l = [2, 3, 4, 2, 3]\n    >>> l = list_remove_repeat(l)\n    [2, 3, 4]\n\n    \"\"\"\n    y = []\n    for i in x:\n        if i not in y:\n            y.append(i)\n\n    return y","194":"def _labelToCategoryNumber(self, label):\n    \"\"\"\n    Since the KNN Classifier stores categories as numbers, we must store each\n    label as a number. This method converts from a label to a unique number.\n    Each label is assigned a unique bit so multiple labels may be assigned to\n    a single record.\n    \"\"\"\n    if label not in self.saved_categories:\n      self.saved_categories.append(label)\n    return pow(2, self.saved_categories.index(label))","195":"def update_extend(dst, src):\n    \"\"\"Update the `dst` with the `src`, extending values where lists.\n\n    Primiarily useful for integrating results from `get_library_config`.\n\n    \"\"\"\n    for k, v in src.items():\n        existing = dst.setdefault(k, [])\n        for x in v:\n            if x not in existing:\n                existing.append(x)","196":"def ancestors(self):\n        \"\"\"Returns list of ancestor task specs based on inputs\"\"\"\n        results = []\n\n        def recursive_find_ancestors(task, stack):\n            for input in task.inputs:\n                if input not in stack:\n                    stack.append(input)\n                    recursive_find_ancestors(input, stack)\n        recursive_find_ancestors(self, results)\n\n        return results","197":"def remove_cti_file(self, file_path: str):\n        \"\"\"\n        Removes the specified CTI file from the CTI file list.\n\n        :param file_path: Set a file path to the target CTI file.\n\n        :return: None.\n        \"\"\"\n        if file_path in self._cti_files:\n            self._cti_files.remove(file_path)\n            self._logger.info(\n                'Removed {0} from the CTI file list.'.format(file_path)\n            )","198":"def exempt(self, resource):\n        \"\"\"Exempt a view function from being checked permission\n\n        :param resource: The view function exempt from checking.\n        \"\"\"\n        if resource not in self._exempt:\n            self._exempt.append(resource)","199":"def register_hook(self, func):\r\n        \"\"\"\r\n        Registers a hook. Since this probably is a bit difficult, I'll explain it in detail.\r\n        A hook basically is an object of a function you pass. This will append that object to a list and whenever\r\n        an event from the Lavalink server is dispatched, the function will be called internally. For declaring the\r\n        function that should become a hook, pass ``event` as its sole parameter.\r\n        Can be a function but also a coroutine.\r\n\r\n        Example for a method declaration inside a class:\r\n        ---------------\r\n            self.bot.lavalink.register_hook(my_hook)\r\n\r\n            async def my_hook(self, event):\r\n                channel = self.bot.get_channel(event.player.fetch('channel'))\r\n                if not channel:\r\n                    return\r\n\r\n                if isinstance(event, lavalink.Events.TrackStartEvent):\r\n                    await channel.send(embed=discord.Embed(title='Now playing:',\r\n                                                           description=event.track.title,\r\n                                                           color=discord.Color.blurple()))\r\n        ---------------\r\n        :param func:\r\n            The function that should be registered as a hook.\r\n        \"\"\"\r\n\r\n        if func not in self.hooks:\r\n            self.hooks.append(func)","200":"def unregister_hook(self, func):\r\n        \"\"\" Unregisters a hook. For further explanation, please have a look at ``register_hook``. \"\"\"\r\n        if func in self.hooks:\r\n            self.hooks.remove(func)","201":"def _generate_contents(self, tar):\n        \"\"\"\n        Adds configuration files to tarfile instance.\n\n        :param tar: tarfile instance\n        :returns: None\n        \"\"\"\n        uci = self.render(files=False)\n        # create a list with all the packages (and remove empty entries)\n        packages = packages_pattern.split(uci)\n        if '' in packages:\n            packages.remove('')\n        # create an UCI file for each configuration package used\n        for package in packages:\n            lines = package.split('\\n')\n            package_name = lines[0]\n            text_contents = '\\n'.join(lines[2:])\n            self._add_file(tar=tar,\n                           name='{0}{1}'.format(config_path, package_name),\n                           contents=text_contents)","202":"def _add_unique_file(self, item):\n        \"\"\"\n        adds a file in self.config['files'] only if not present already\n        \"\"\"\n        if item not in self.config['files']:\n            self.config['files'].append(item)","203":"def _generate_contents(self, tar):\n        \"\"\"\n        Adds configuration files to tarfile instance.\n\n        :param tar: tarfile instance\n        :returns: None\n        \"\"\"\n        uci = self.render(files=False)\n        # create a list with all the packages (and remove empty entries)\n        packages = re.split('package ', uci)\n        if '' in packages:\n            packages.remove('')\n        # create a file for each configuration package used\n        for package in packages:\n            lines = package.split('\\n')\n            package_name = lines[0]\n            text_contents = '\\n'.join(lines[2:])\n            text_contents = 'package {0}\\n\\n{1}'.format(package_name, text_contents)\n            self._add_file(tar=tar,\n                           name='uci\/{0}.conf'.format(package_name),\n                           contents=text_contents)\n        # prepare template context for install and uninstall scripts\n        template_context = self._get_install_context()\n        # add install.sh to included files\n        self._add_install(template_context)\n        # add uninstall.sh to included files\n        self._add_uninstall(template_context)\n        # add vpn up and down scripts\n        self._add_openvpn_scripts()\n        # add tc_script\n        self._add_tc_script()","204":"def merge_list(list1, list2):\n    \"\"\"\n    Merges the contents of two lists into a new list.\n\n    :param list1: the first list\n    :type list1: list\n    :param list2: the second list\n    :type list2: list\n    :returns: list\n    \"\"\"\n\n    merged = list(list1)\n\n    for value in list2:\n        if value not in merged:\n            merged.append(value)\n\n    return merged","205":"def on_tool_finish(self, tool):\n        \"\"\"\n        Called when an individual tool completes execution.\n\n        :param tool: the name of the tool that completed\n        :type tool: str\n        \"\"\"\n\n        with self._lock:\n            if tool in self.current_tools:\n                self.current_tools.remove(tool)\n                self.completed_tools.append(tool)","206":"def _close_stream(self):\n        \"\"\"Same as `close_stream` but with the `lock` acquired.\n        \"\"\"\n        self.stream.close()\n        if self.stream.transport in self._ml_handlers:\n            self._ml_handlers.remove(self.stream.transport)\n            self.main_loop.remove_handler(self.stream.transport)\n        self.stream = None\n        self.uplink = None","207":"def remove_handler(self, handler):\n        \"\"\"Remove a handler object.\n\n        :Parameters:\n            - `handler`: the object to remove\n        \"\"\"\n        with self.lock:\n            if handler in self.handlers:\n                self.handlers.remove(handler)\n                self._update_handlers()","208":"def remove_redundancies(levels):\n    \"\"\"\n    There are repeats in the output from get_levels(). We\n    want only the earliest occurrence (after it's reversed)\n    \"\"\"\n    seen = []\n    final = []\n    for line in levels:\n        new_line = []\n        for item in line:\n            if item not in seen:\n                seen.append(item)\n                new_line.append(item)\n        final.append(new_line)\n    return final","209":"def get_default_fields(self):\n        \"\"\"\n        get all fields of model, execpt id\n        \"\"\"\n        field_names = self._meta.get_all_field_names()\n        if 'id' in field_names:\n            field_names.remove('id')\n\n        return field_names","210":"def image_urls(self):\n        \"\"\"\n        Combine finder_image_urls and extender_image_urls,\n        remove duplicate but keep order\n        \"\"\"\n\n        all_image_urls = self.finder_image_urls[:]\n        for image_url in self.extender_image_urls:\n            if image_url not in all_image_urls:\n                all_image_urls.append(image_url)\n\n        return all_image_urls","211":"def retract(self, sentence):\n        \"Remove the sentence's clauses from the KB.\"\n        for c in conjuncts(to_cnf(sentence)):\n            if c in self.clauses:\n                self.clauses.remove(c)","212":"def list_move_to_front(l,value='other'):\n    \"\"\"if the value is in the list, move it to the front and return it.\"\"\"\n    l=list(l)\n    if value in l:\n        l.remove(value)\n        l.insert(0,value)\n    return l","213":"def list_move_to_back(l,value='other'):\n    \"\"\"if the value is in the list, move it to the back and return it.\"\"\"\n    l=list(l)\n    if value in l:\n        l.remove(value)\n        l.append(value)\n    return l","214":"def list_order_by(l,firstItems):\n    \"\"\"given a list and a list of items to be first, return the list in the\n    same order except that it begins with each of the first items.\"\"\"\n    l=list(l)\n    for item in firstItems[::-1]: #backwards\n        if item in l:\n            l.remove(item)\n            l.insert(0,item)\n    return l","215":"def unique_by_index(sequence):\n    \"\"\" unique elements in `sequence` in the order in which they occur\n\n    Parameters\n    ----------\n    sequence : iterable\n\n    Returns\n    -------\n    uniques : list\n        unique elements of sequence, ordered by the order in which the element\n        occurs in `sequence`\n    \"\"\"\n    uniques = []\n    for element in sequence:\n        if element not in uniques:\n            uniques.append(element)\n    return uniques","216":"def _receive(self):\n        \"\"\"Receive any incoming socket data.\n\n            If an error is thrown, handle it and return an empty string.\n\n        :return: data_in\n        :rtype: bytes\n        \"\"\"\n        data_in = EMPTY_BUFFER\n        try:\n            data_in = self._read_from_socket()\n        except socket.timeout:\n            pass\n        except (IOError, OSError) as why:\n            if why.args[0] not in (EWOULDBLOCK, EAGAIN):\n                self._exceptions.append(AMQPConnectionError(why))\n                self._running.clear()\n        return data_in","217":"def get_uniques(l):\n    \"\"\" Returns a list with no repeated elements.\n    \"\"\"\n    result = []\n\n    for i in l:\n        if i not in result:\n            result.append(i)\n\n    return result","218":"def preview_unmount(self):\n        \"\"\"Returns a list of all commands that would be executed if the :func:`unmount` method would be called.\n\n        Note: any system changes between calling this method and calling :func:`unmount` aren't listed by this command.\n        \"\"\"\n\n        commands = []\n        for mountpoint in self.find_bindmounts():\n            commands.append('umount {0}'.format(mountpoint))\n        for mountpoint in self.find_mounts():\n            commands.append('umount {0}'.format(mountpoint))\n            commands.append('rm -Rf {0}'.format(mountpoint))\n        for vgname, pvname in self.find_volume_groups():\n            commands.append('lvchange -a n {0}'.format(vgname))\n            commands.append('losetup -d {0}'.format(pvname))\n        for device in self.find_loopbacks():\n            commands.append('losetup -d {0}'.format(device))\n        for mountpoint in self.find_base_images():\n            commands.append('fusermount -u {0}'.format(mountpoint))\n            commands.append('rm -Rf {0}'.format(mountpoint))\n        for folder in self.find_clean_dirs():\n            cmd = 'rm -Rf {0}'.format(folder)\n            if cmd not in commands:\n                commands.append(cmd)\n        return commands","219":"def hydrophobic_atoms(self, all_atoms):\n        \"\"\"Select all carbon atoms which have only carbons and\/or hydrogens as direct neighbors.\"\"\"\n        atom_set = []\n        data = namedtuple('hydrophobic', 'atom orig_atom orig_idx')\n        atm = [a for a in all_atoms if a.atomicnum == 6 and set([natom.GetAtomicNum() for natom\n                                                                 in pybel.ob.OBAtomAtomIter(a.OBAtom)]).issubset(\n            {1, 6})]\n        for atom in atm:\n            orig_idx = self.Mapper.mapid(atom.idx, mtype=self.mtype, bsid=self.bsid)\n            orig_atom = self.Mapper.id_to_atom(orig_idx)\n            if atom.idx not in self.altconf:\n                atom_set.append(data(atom=atom, orig_atom=orig_atom, orig_idx=orig_idx))\n        return atom_set","220":"def get_element_child_info(doc, attr):\n    \"\"\"Get information from child elements of this elementas a list since order is important.\n\n    Don't include signature tags.\n\n    :param doc: XML element\n    :param attr: Attribute to get from the elements, for example \"tag\" or \"text\".\n    \"\"\"\n    props = []\n    for child in doc:\n        if child.tag not in [\"author_signature\", \"parent_author_signature\"]:\n            props.append(getattr(child, attr))\n    return props","221":"def gen_multivalued_slot(self, target_name_base: str, target_type: IRIREF) -> IRIREF:\n        \"\"\" Generate a shape that represents an RDF list of target_type\n\n        @param target_name_base:\n        @param target_type:\n        @return:\n        \"\"\"\n        list_shape_id = IRIREF(target_name_base + \"__List\")\n        if list_shape_id not in self.list_shapes:\n            list_shape = Shape(id=list_shape_id, closed=True)\n            list_shape.expression = EachOf()\n            expressions = [TripleConstraint(predicate=RDF.first, valueExpr=target_type, min=0, max=1)]\n            targets = ShapeOr()\n            targets.shapeExprs = [(NodeConstraint(values=[RDF.nil])), list_shape_id]\n            expressions.append(TripleConstraint(predicate=RDF.rest, valueExpr=targets))\n            list_shape.expression.expressions = expressions\n            self.shapes.append(list_shape)\n            self.list_shapes.append(list_shape_id)\n        return list_shape_id","222":"def remove_message(self, message):\n        '''\n        Remove a message from the batch\n        '''\n        if message in self.__messages:\n            self.__messages.remove(message)","223":"def deactivate(self, mod):\n        \"\"\"\n        Set external module *mod* to inactive.\n        \"\"\"\n        if mod in self.active:\n            self.active.remove(mod)","224":"def normalize(self):\n        \"\"\"\n        Normalizes environment variables and the Python path.\n\n        This method first updates the environment variables (``os.environ``).\n        Then, it normalizes the Python path (``sys.path``) by resolving all\n        references to the user directory and environment variables.\n        \"\"\"\n        # Normalize configuration\n        self.__state.normalize()\n\n        # Update sys.path, avoiding duplicates\n        whole_path = list(self.__state.paths)\n        whole_path.extend(sys.path)\n\n        # Ensure the working directory as first search path\n        sys.path = [\".\"]\n        for path in whole_path:\n            if path not in sys.path:\n                sys.path.append(path)","225":"def remove(self, item):\n        \"\"\" Remove an item from the set, returning if it was present \"\"\"\n        with self.lock:\n            if item in self.set:\n                self.set.remove(item)\n                return True\n            return False","226":"def check_for_reqd_cols(data, reqd_cols):\n    \"\"\"\n    Check data (PmagPy list of dicts) for required columns\n    \"\"\"\n    missing = []\n    for col in reqd_cols:\n        if col not in data[0]:\n            missing.append(col)\n    return missing","227":"def get_specs(data):\n    \"\"\"\n    Takes a magic format file and returns a list of unique specimen names\n    \"\"\"\n    # sort the specimen names\n    speclist = []\n    for rec in data:\n        try:\n            spec = rec[\"er_specimen_name\"]\n        except KeyError as e:\n            spec = rec[\"specimen\"]\n        if spec not in speclist:\n            speclist.append(spec)\n    speclist.sort()\n    return speclist","228":"def get_specs(self,data):\n        \"\"\"\n         takes a magic format file and returns a list of unique specimen names\n        \"\"\"\n    # sort the specimen names\n    #\n#        print \"calling get_specs()\"\n        speclist=[]\n        for rec in data:\n          spec=rec[\"er_specimen_name\"]\n          if spec not in speclist:speclist.append(spec)\n        speclist.sort()\n        #print speclist\n        return speclist","229":"def render_source(output_dir, package_spec):\n    \"\"\"\n    Render and output to a directory given a package specification.\n    \"\"\"\n    path, name = package_spec.filepath\n    destination_filename = '%s\/%s.proto' % (output_dir, name)\n    pb_template = JENV.get_template(MESSAGES_TEMPLATE_NAME)\n    includes = [include[:-5] if include.endswith('.yaml') else include for include in package_spec.includes]\n    if 'types' in includes:\n        includes.remove('types')\n    with open(destination_filename, 'w') as f:\n        f.write(pb_template.render(\n            name=name,\n            package=package_spec.identifier,\n            messages=package_spec.definitions,\n            includes=includes,\n            description=package_spec.description,\n        ))","230":"def add(self, items):\n        \"\"\"Add entry to global presubs list\"\"\"\n        with _presub_lock:\n            self.load_from_cloud()\n            for entry in utils.ensure_listable(items):\n                # add new entries, but only if they're unique\n                if entry not in self.current:\n                    self.current.append(entry)\n            self.save_to_cloud()","231":"def unsubscribe(self, client):\n        \"\"\"Unsubscribe a client from the channel.\"\"\"\n        if client in self.clients:\n            self.clients.remove(client)\n            log(\"Unsubscribed client {} from channel {}\".format(client, self.name))","232":"def apply_theme(self, themename, themeoptions):\n        \"\"\"Apply a new theme to the document.\n\n        This will store the existing theme configuration and apply a new one.\n\n        \"\"\"\n\n        # push the existing values onto the Stack\n        self._theme_stack.append(\n            (self.theme, self.theme_options)\n        )\n\n        theme_factory = HTMLThemeFactory(self.app)\n        theme_factory.load_additional_themes(self.get_builtin_theme_dirs() + self.config.slide_theme_path)\n\n        self.theme = theme_factory.create(themename)\n        self.theme_options = themeoptions.copy()\n        self.templates.init(self, self.theme)\n        self.templates.environment.filters['json'] = json.dumps\n\n        if self.theme not in self._additional_themes:\n            self._additional_themes.append(self.theme)","233":"def add_mv_grid_district(self, mv_grid_district):\n        \"\"\"Adds a MV grid_district to _mv_grid_districts if not already existing\"\"\"\n        # TODO: use setter method here (make attribute '_mv_grid_districts' private)\n        if mv_grid_district not in self.mv_grid_districts():\n            self._mv_grid_districts.append(mv_grid_district)","234":"def add_lv_load_area_group(self, lv_load_area_group):\n        \"\"\"Adds a LV load_area to _lv_load_areas if not already existing.\n        \"\"\"\n        if lv_load_area_group not in self.lv_load_area_groups():\n            self._lv_load_area_groups.append(lv_load_area_group)","235":"def remove_callback(self, callback):\n        \"\"\"Remove callback previously registered.\"\"\"\n        if callback in self._async_callbacks:\n            self._async_callbacks.remove(callback)","236":"def create(client, name):\n    \"\"\"Create an empty dataset in the current repo.\"\"\"\n    from renku.models.datasets import Author\n\n    with client.with_dataset(name=name) as dataset:\n        click.echo('Creating a dataset ... ', nl=False)\n        author = Author.from_git(client.repo)\n        if author not in dataset.authors:\n            dataset.authors.append(author)\n\n    click.secho('OK', fg='green')","237":"def hook_inform(self, inform_name, callback):\n        \"\"\"Hookup a function to be called when an inform is received.\n\n        Useful for interface-changed and sensor-status informs.\n\n        Parameters\n        ----------\n        inform_name : str\n            The name of the inform.\n        callback : function\n            The function to be called.\n\n        \"\"\"\n        # Do not hook the same callback multiple times\n        if callback not in self._inform_hooks[inform_name]:\n            self._inform_hooks[inform_name].append(callback)","238":"def set_allowed(self, initial_state, *allowed_states):\n        # type: (str, *str) -> None\n        \"\"\"Add an allowed transition from initial_state to allowed_states\"\"\"\n        allowed_states = list(allowed_states)\n        self._allowed.setdefault(initial_state, set()).update(allowed_states)\n        for state in allowed_states + [initial_state]:\n            if state not in self.possible_states:\n                self.possible_states.append(state)","239":"def register_model(self, model):\n        \"\"\"\n        Register ``model`` to this group\n\n        :param model: model name\n        :return: None\n        \"\"\"\n\n        assert isinstance(model, str)\n        if model not in self.all_models:\n            self.all_models.append(model)","240":"def remove_enclosure(self, left_char, right_char):\n        \"\"\"\n        Remove enclosure pair from set of enclosures.\n\n        :param str left_char: left character of enclosure pair - e.g. \"(\"\n        :param str right_char: right character of enclosure pair - e.g. \")\"\n        \"\"\"\n        assert len(left_char) == 1, \\\n            \"Parameter left_char must be character not string\"\n        assert len(right_char) == 1, \\\n            \"Parameter right_char must be character not string\"\n        rm_enclosure = (left_char, right_char)\n        if rm_enclosure in self._enclosure:\n            self._enclosure.remove(rm_enclosure)\n\n        self._after_tld_chars = self._get_after_tld_chars()","241":"def get_note_names(self):\n        \"\"\"Return a list of unique note names in the Bar.\"\"\"\n        res = []\n        for cont in self.bar:\n            for x in cont[2].get_note_names():\n                if x not in res:\n                    res.append(x)\n        return res","242":"def attach(self, listener):\n        \"\"\"Attach an object that should be notified of events.\n\n        The object should have a notify(msg_type, param_dict) function.\n        \"\"\"\n        if listener not in self.listeners:\n            self.listeners.append(listener)","243":"def detach(self, listener):\n        \"\"\"Detach a listening object so that it won't receive any events\n        anymore.\"\"\"\n        if listener in self.listeners:\n            self.listeners.remove(listener)","244":"def remove_duplicate_notes(self):\n        \"\"\"Remove duplicate and enharmonic notes from the container.\"\"\"\n        res = []\n        for x in self.notes:\n            if x not in res:\n                res.append(x)\n        self.notes = res\n        return res","245":"def get_note_names(self):\n        \"\"\"Return a list with all the note names in the current container.\n\n        Every name will only be mentioned once.\n        \"\"\"\n        res = []\n        for n in self.notes:\n            if n.name not in res:\n                res.append(n.name)\n        return res","246":"def get_method_list():\r\n    '''\r\n    Include manual methods by default\r\n    '''\r\n    methods = [str(_('Cash')),str(_('Check')),str(_('Bank\/Debit Card')),str(_('Other'))]\r\n    methods += ExpenseItem.objects.order_by().values_list('paymentMethod',flat=True).distinct()\r\n    methods += RevenueItem.objects.order_by().values_list('paymentMethod',flat=True).distinct()\r\n    methods_list = list(set(methods))\r\n\r\n    if None in methods_list:\r\n        methods_list.remove(None)\r\n\r\n    return methods_list","247":"def get_child_classes(cls, slot, page, instance=None):\n        \"\"\"Restrict child classes of Card to one of each: Header, Body and Footer\"\"\"\n        child_classes = super(BootstrapCardPlugin, cls).get_child_classes(slot, page, instance)\n        # allow only one child of type Header, Body, Footer\n        for child in instance.get_children():\n            if child.plugin_type in child_classes:\n                child_classes.remove(child.plugin_type)\n        return child_classes","248":"def merge(self, other):\n        \"\"\"Merge data from another Compound into this Compound.\"\"\"\n        log.debug('Merging: %s and %s' % (self.serialize(), other.serialize()))\n        for k in self.keys():\n            for new_item in other[k]:\n                if new_item not in self[k]:\n                    self[k].append(new_item)\n        log.debug('Result: %s' % self.serialize())\n        return self","249":"def unconnect(self, *funcs):\n        \"\"\"Unconnect specified callback functions.\"\"\"\n        for func in funcs:\n            for callbacks in self._callbacks.values():\n                if func in callbacks:\n                    callbacks.remove(func)","250":"def symmetric_difference_update(self, other):\n        \"\"\"\n        Throws out all intervals except those only in self or other,\n        not both.\n        \"\"\"\n        other = set(other)\n        ivs = list(self)\n        for iv in ivs:\n            if iv in other:\n                self.remove(iv)\n                other.remove(iv)\n        self.update(other)","251":"def add_config_path(self, path):\n        \"\"\"Add a path for Vyper to search for the config file in.\n        Can be called multiple times to define multiple search paths.\n        \"\"\"\n        abspath = util.abs_pathify(path)\n        if abspath not in self._config_paths:\n            log.info(\"Adding {0} to paths to search\".format(abspath))\n            self._config_paths.append(abspath)","252":"def _to_sky_params(self, wcs, mode='all'):\n        \"\"\"\n        Convert the pixel aperture parameters to those for a sky\n        aperture.\n\n        Parameters\n        ----------\n        wcs : `~astropy.wcs.WCS`\n            The world coordinate system (WCS) transformation to use.\n\n        mode : {'all', 'wcs'}, optional\n            Whether to do the transformation including distortions\n            (``'all'``; default) or only including only the core WCS\n            transformation (``'wcs'``).\n\n        Returns\n        -------\n        sky_params : dict\n            A dictionary of parameters for an equivalent sky aperture.\n        \"\"\"\n\n        sky_params = {}\n        x, y = np.transpose(self.positions)\n        sky_params['positions'] = pixel_to_skycoord(x, y, wcs, mode=mode)\n\n        # The aperture object must have a single value for each shape\n        # parameter so we must use a single pixel scale for all positions.\n        # Here, we define the scale at the WCS CRVAL position.\n        crval = SkyCoord([wcs.wcs.crval], frame=wcs_to_celestial_frame(wcs),\n                         unit=wcs.wcs.cunit)\n        scale, angle = pixel_scale_angle_at_skycoord(crval, wcs)\n\n        params = self._params[:]\n        theta_key = 'theta'\n        if theta_key in self._params:\n            sky_params[theta_key] = (self.theta * u.rad) - angle.to(u.rad)\n            params.remove(theta_key)\n\n        param_vals = [getattr(self, param) for param in params]\n        for param, param_val in zip(params, param_vals):\n            sky_params[param] = (param_val * u.pix * scale).to(u.arcsec)\n\n        return sky_params","253":"def cancel(self):\n        \"\"\"If the call is currently being executed or sent for remote\n           execution, then it cannot be cancelled and the method will return\n           False, otherwise the call will be cancelled and the method will\n           return True.\"\"\"\n        if self in scoop._control.execQueue.movable:\n            self.exceptionValue = CancelledError()\n            scoop._control.futureDict[self.id]._delete()\n            scoop._control.execQueue.remove(self)\n            return True\n        return False","254":"def _get_choices(self, gandi):\n        \"\"\" Internal method to get choices list \"\"\"\n        packages = super(CertificatePackageMax, self)._get_choices(gandi)\n        ret = list(set([pack.split('_')[2] for pack in packages]))\n        if 'w' in ret:\n            ret.remove('w')\n        return ret","255":"def extend_unique(list1, list2):\n    \"\"\"Extend list1 with list2 as list.extend(), but doesn't append duplicates\n    to list1\"\"\"\n    for item in list2:\n        if item not in list1:\n            list1.append(item)","256":"def add_hostname_cn_ip(self, addresses):\n        \"\"\"Add an address to the SAN list for the hostname request\n\n        :param addr: [] List of address to be added\n        \"\"\"\n        for addr in addresses:\n            if addr not in self.hostname_entry['addresses']:\n                self.hostname_entry['addresses'].append(addr)","257":"def addCodedValue(self, name, code):\n        \"\"\"\n        adds a coded value to the domain\n\n        Inputs:\n           name - name of the domain\n           code - value\n        \"\"\"\n        i = {\"name\" : name, \"code\" : code}\n        if i not in self._codedValues:\n            self._codedValues.append(i)","258":"def register(self, event, fn):\n        \"\"\"\n        Registers the given function as a handler to be applied\n        in response to the the given event.\n        \"\"\"\n\n        # TODO: Can we check the method signature?\n        self._handler_dict.setdefault(event, [])\n        if fn not in self._handler_dict[event]:\n            self._handler_dict[event].append(fn)","259":"def _assign_zones(self):\n        \"\"\"Assign all RainCloudyFaucetZone managed by faucet.\"\"\"\n        for zone_id in range(1, 5):\n            zone = \\\n                RainCloudyFaucetZone(\n                    parent=self._parent,\n                    controller=self._controller,\n                    faucet=self,\n                    zone_id=zone_id)\n\n            if zone not in self.zones:\n                self.zones.append(zone)","260":"def find_untranscribed_wavs(wav_path: Path, transcription_path: Path, label_type: str) -> List[str]:\n    \"\"\"Find the prefixes for all the wav files that do not have an associated transcription\n    Args:\n        wav_path: Path to search for wav files in\n        transcription_path: Path to search for transcriptions in\n        label_type: The type of labels for transcriptions. Eg \"phonemes\" \"phonemes_and_tones\"\n    Returns:\n        A list of all untranscribed prefixes\n    \"\"\"\n    audio_files = wav_path.glob(\"**\/*.wav\")\n    transcription_files = transcription_path.glob(\"**\/*.{}\".format(label_type))\n\n    transcription_file_prefixes = [t_file.stem for t_file in transcription_files]\n\n    untranscribed_prefixes = [] # type: List[str]\n    for a_file in audio_files:\n        if a_file.stem not in transcription_file_prefixes:\n            untranscribed_prefixes.append(a_file.stem)\n    return untranscribed_prefixes","261":"def getSymbols(self):\n        \"\"\"Returns every symbol\"\"\"\n        symbollist = []\n        for rule in self.productions:\n            for symbol in rule.leftside + rule.rightside:\n                if symbol not in symbollist:\n                    symbollist.append(symbol)\n        symbollist += self.terminal_symbols\n        return symbollist","262":"def addPolicyURI(self, policy_uri):\n        \"\"\"Add an acceptable authentication policy URI to this request\n\n        This method is intended to be used by the relying party to add\n        acceptable authentication types to the request.\n\n        @param policy_uri: The identifier for the preferred type of\n            authentication.\n        @see: http:\/\/openid.net\/specs\/openid-provider-authentication-policy-extension-1_0-01.html#auth_policies\n        \"\"\"\n        if policy_uri not in self.preferred_auth_policies:\n            self.preferred_auth_policies.append(policy_uri)","263":"def addPolicyURI(self, policy_uri):\n        \"\"\"Add a authentication policy to this response\n\n        This method is intended to be used by the provider to add a\n        policy that the provider conformed to when authenticating the user.\n\n        @param policy_uri: The identifier for the preferred type of\n            authentication.\n        @see: http:\/\/openid.net\/specs\/openid-provider-authentication-policy-extension-1_0-01.html#auth_policies\n        \"\"\"\n        if policy_uri not in self.auth_policies:\n            self.auth_policies.append(policy_uri)","264":"def _unique(list_of_dicts):\n    '''\n    Returns an unique list of dictionaries given a list that may contain duplicates.\n    '''\n    unique_list = []\n    for ele in list_of_dicts:\n        if ele not in unique_list:\n            unique_list.append(ele)\n    return unique_list","265":"def salt_call():\n    '''\n    Directly call a salt command in the modules, does not require a running\n    salt minion to run.\n    '''\n    import salt.cli.call\n    if '' in sys.path:\n        sys.path.remove('')\n    client = salt.cli.call.SaltCall()\n    _install_signal_handlers(client)\n    client.run()","266":"def salt_run():\n    '''\n    Execute a salt convenience routine.\n    '''\n    import salt.cli.run\n    if '' in sys.path:\n        sys.path.remove('')\n    client = salt.cli.run.SaltRun()\n    _install_signal_handlers(client)\n    client.run()","267":"def salt_ssh():\n    '''\n    Execute the salt-ssh system\n    '''\n    import salt.cli.ssh\n    if '' in sys.path:\n        sys.path.remove('')\n    try:\n        client = salt.cli.ssh.SaltSSH()\n        _install_signal_handlers(client)\n        client.run()\n    except SaltClientError as err:\n        trace = traceback.format_exc()\n        try:\n            hardcrash = client.options.hard_crash\n        except (AttributeError, KeyError):\n            hardcrash = False\n        _handle_interrupt(\n            SystemExit(err),\n            err,\n            hardcrash, trace=trace)","268":"def salt_cloud():\n    '''\n    The main function for salt-cloud\n    '''\n    # Define 'salt' global so we may use it after ImportError. Otherwise,\n    # UnboundLocalError will be raised.\n    global salt  # pylint: disable=W0602\n\n    try:\n        # Late-imports for CLI performance\n        import salt.cloud\n        import salt.cloud.cli\n    except ImportError as e:\n        # No salt cloud on Windows\n        log.error('Error importing salt cloud: %s', e)\n        print('salt-cloud is not available in this system')\n        sys.exit(salt.defaults.exitcodes.EX_UNAVAILABLE)\n    if '' in sys.path:\n        sys.path.remove('')\n\n    client = salt.cloud.cli.SaltCloud()\n    _install_signal_handlers(client)\n    client.run()","269":"def salt_main():\n    '''\n    Publish commands to the salt system from the command line on the\n    master.\n    '''\n    import salt.cli.salt\n    if '' in sys.path:\n        sys.path.remove('')\n    client = salt.cli.salt.SaltCMD()\n    _install_signal_handlers(client)\n    client.run()","270":"def salt_support():\n    '''\n    Run Salt Support that collects system data, logs etc for debug and support purposes.\n    :return:\n    '''\n\n    import salt.cli.support.collector\n    if '' in sys.path:\n        sys.path.remove('')\n    client = salt.cli.support.collector.SaltSupport()\n    _install_signal_handlers(client)\n    client.run()","271":"def release_plugin(self, name):\n        \"\"\"\n        Deactivate and remove the plugin with name ``name``.\n        \"\"\"\n        plugin = self._active_plugins[name]\n        if id(plugin) in self._provided_by_preset:\n            self._provided_by_preset.remove(id(plugin))\n\n        del self._active_plugins[name]\n        delattr(self, name)","272":"def _pick_exit(self, block_address, stmt_idx, target_ips):\n        \"\"\"\n        Include an exit in the final slice.\n\n        :param block_address:   Address of the basic block.\n        :param stmt_idx:        ID of the exit statement.\n        :param target_ips:      The target address of this exit statement.\n        \"\"\"\n\n        # TODO: Support context-sensitivity\n\n        tpl = (stmt_idx, target_ips)\n        if tpl not in self.chosen_exits[block_address]:\n            self.chosen_exits[block_address].append(tpl)","273":"def process_log_event(event, context):\n    \"\"\"Format log events and relay via sns\/email\"\"\"\n    init()\n    serialized = event['awslogs'].pop('data')\n    data = json.loads(zlib.decompress(\n        base64.b64decode(serialized), 16 + zlib.MAX_WBITS))\n\n    # Fetch additional logs for context (20s window)\n    timestamps = [e['timestamp'] for e in data['logEvents']]\n    start = min(timestamps) - 1000 * 15\n    end = max(timestamps) + 1000 * 5\n\n    events = logs.get_log_events(\n        logGroupName=data['logGroup'],\n        logStreamName=data['logStream'],\n        startTime=start,\n        endTime=end,\n        startFromHead=True)['events']\n\n    message = [\n        \"An error was detected\",\n        \"\",\n        \"Log Group: %s\" % data['logGroup'],\n        \"Log Stream: %s\" % data['logStream'],\n        \"Log Owner: %s\" % data['owner'],\n        \"\",\n        \"Log Contents\",\n        \"\"]\n\n    # We may get things delivered from log sub that are not in log events\n    for evt in data['logEvents']:\n        if evt not in events:\n            events.append(evt)\n\n    for evt in events:\n        message.append(message_event(evt))\n        message.append(\"\")\n\n    params = dict(\n        TopicArn=config['topic'],\n        Subject=config['subject'],\n        Message='\\n'.join(message))\n\n    sns.publish(**params)","274":"def get_columns_list(self):\n        \"\"\"\n        modified: removing the '_cls' column added by Mongoengine to support\n        mongodb document inheritance\n        cf. http:\/\/docs.mongoengine.org\/apireference.html#documents:\n        \"A Document subclass may be itself subclassed,\n        to create a specialised version of the document that will be\n        stored in the same collection.\n        To facilitate this behaviour a _cls field is added to documents\n        (hidden though the MongoEngine interface).\n        To disable this behaviour and remove the dependence on the presence of _cls set\n        allow_inheritance to False in the meta dictionary.\"\n        \"\"\"\n        columns = list(self.obj._fields.keys())\n        if \"_cls\" in columns:\n            columns.remove(\"_cls\")\n        return columns","275":"def color_range(color, N=20):\n    \"\"\"\n    Generates a scale of colours from a base colour\n\n    Parameters:\n    -----------\n            color : string\n                    Color representation in hex\n            N   : int\n                    number of colours to generate\n\n    Example:\n            color_range('#ff9933',20)\n    \"\"\"\n    color = normalize(color)\n    org = color\n    color = hex_to_hsv(color)\n    HSV_tuples = [(color[0], x, color[2]) for x in np.arange(0, 1, 2.0 \/ N)]\n    HSV_tuples.extend([(color[0], color[1], x)\n                       for x in np.arange(0, 1, 2.0 \/ N)])\n    hex_out = []\n    for c in HSV_tuples:\n        c = colorsys.hsv_to_rgb(*c)\n        c = [int(_ * 255) for _ in c]\n        # hex_out.append(\"#\"+\"\".join([chr(x).encode('hex') for x in c]))\n        hex_out.append(\"#\" + \"\".join(['{0:02x}'.format(x) for x in c]))\n    if org not in hex_out:\n        hex_out.append(org)\n    hex_out.sort()\n    return hex_out","276":"def _GetChunkForReading(self, chunk):\n    \"\"\"Returns the relevant chunk from the datastore and reads ahead.\"\"\"\n    try:\n      return self.chunk_cache.Get(chunk)\n    except KeyError:\n      pass\n\n    # We don't have this chunk already cached. The most common read\n    # access pattern is contiguous reading so since we have to go to\n    # the data store already, we read ahead to reduce round trips.\n\n    missing_chunks = []\n    for chunk_number in range(chunk, chunk + 10):\n      if chunk_number not in self.chunk_cache:\n        missing_chunks.append(chunk_number)\n\n    self._ReadChunks(missing_chunks)\n    # This should work now - otherwise we just give up.\n    try:\n      return self.chunk_cache.Get(chunk)\n    except KeyError:\n      raise aff4.ChunkNotFoundError(\"Cannot open chunk %s\" % chunk)","277":"def GetIPAddresses(self):\n    \"\"\"IP addresses from all interfaces.\"\"\"\n    result = []\n    filtered_ips = [\"127.0.0.1\", \"::1\", \"fe80::1\"]\n\n    for interface in self.interfaces:\n      for address in interface.addresses:\n        if address.human_readable_address not in filtered_ips:\n          result.append(Text(address.human_readable_address))\n    return sorted(result)","278":"def Add(self, other):\n    \"\"\"Returns a copy of this set with a new element added.\"\"\"\n    new_descriptors = []\n    for desc in self.descriptors + other.descriptors:\n      if desc not in new_descriptors:\n        new_descriptors.append(desc)\n\n    return TypeDescriptorSet(*new_descriptors)","279":"def Append(self, desc):\n    \"\"\"Append the descriptor to this set.\"\"\"\n    if desc not in self.descriptors:\n      self.descriptors.append(desc)\n      self.descriptor_map[desc.name] = desc\n      self.descriptor_names.append(desc.name)","280":"def _get_all_miller_e(self):\n        \"\"\"\n        from self:\n            get miller_list(unique_miller), e_surf_list and symmetry\n            operations(symmops) according to lattice\n        apply symmops to get all the miller index, then get normal,\n        get all the facets functions for wulff shape calculation:\n            |normal| = 1, e_surf is plane's distance to (0, 0, 0),\n            normal[0]x + normal[1]y + normal[2]z = e_surf\n\n        return:\n            [WulffFacet]\n        \"\"\"\n        all_hkl = []\n        color_ind = self.color_ind\n        planes = []\n        recp = self.structure.lattice.reciprocal_lattice_crystallographic\n        recp_symmops = get_recp_symmetry_operation(self.structure, self.symprec)\n\n        for i, (hkl, energy) in enumerate(zip(self.hkl_list,\n                                              self.e_surf_list)):\n            for op in recp_symmops:\n                miller = tuple([int(x) for x in op.operate(hkl)])\n                if miller not in all_hkl:\n                    all_hkl.append(miller)\n                    normal = recp.get_cartesian_coords(miller)\n                    normal \/= sp.linalg.norm(normal)\n                    normal_pt = [x * energy for x in normal]\n                    dual_pt = [x \/ energy for x in normal]\n                    color_plane = color_ind[divmod(i, len(color_ind))[1]]\n                    planes.append(WulffFacet(normal, energy, normal_pt,\n                                             dual_pt, color_plane, i, hkl))\n\n        # sort by e_surf\n        planes.sort(key=lambda x: x.e_surf)\n        return planes","281":"def walk_egg(egg_dir):\n    \"\"\"Walk an unpacked egg's contents, skipping the metadata directory\"\"\"\n    walker = sorted_walk(egg_dir)\n    base, dirs, files = next(walker)\n    if 'EGG-INFO' in dirs:\n        dirs.remove('EGG-INFO')\n    yield base, dirs, files\n    for bdf in walker:\n        yield bdf","282":"def find_children(self):\n        \"\"\"Take a tree and set the children according to the parents.\n\n        Takes a tree structure which lists the parents of each vertex\n        and computes the children for each vertex and places them in.\"\"\"\n        for i in range(len(self.vertices)):\n            self.vertices[i].children = []\n        for i in range(len(self.vertices)):\n            for parent in self.vertices[i].parents:\n                if i not in self.vertices[parent].children:\n                    self.vertices[parent].children.append(i)","283":"def find_parents(self):\n        \"\"\"Take a tree and set the parents according to the children\n\n        Takes a tree structure which lists the children of each vertex\n        and computes the parents for each vertex and places them in.\"\"\"\n        for i in range(len(self.vertices)):\n            self.vertices[i].parents = []\n        for i in range(len(self.vertices)):\n            for child in self.vertices[i].children:\n                if i not in self.vertices[child].parents:\n                    self.vertices[child].parents.append(i)","284":"def purge_duplicates(list_in):\n    \"\"\"Remove duplicates from list while preserving order.\n\n    Parameters\n    ----------\n    list_in: Iterable\n\n    Returns\n    -------\n    list\n        List of first occurences in order\n    \"\"\"\n    _list = []\n    for item in list_in:\n        if item not in _list:\n            _list.append(item)\n    return _list","285":"def _ensure_channel_connected(self, destination_id):\n        \"\"\" Ensure we opened a channel to destination_id. \"\"\"\n        if destination_id not in self._open_channels:\n            self._open_channels.append(destination_id)\n\n            self.send_message(\n                destination_id, NS_CONNECTION,\n                {MESSAGE_TYPE: TYPE_CONNECT,\n                 'origin': {},\n                 'userAgent': 'PyChromecast',\n                 'senderInfo': {\n                     'sdkType': 2,\n                     'version': '15.605.1.3',\n                     'browserVersion': \"44.0.2403.30\",\n                     'platform': 4,\n                     'systemVersion': 'Macintosh; Intel Mac OS X10_10_3',\n                     'connectionType': 1}},\n                no_add_request_id=True)","286":"def disconnect_channel(self, destination_id):\n        \"\"\" Disconnect a channel with destination_id. \"\"\"\n        if destination_id in self._open_channels:\n            try:\n                self.send_message(\n                    destination_id, NS_CONNECTION,\n                    {MESSAGE_TYPE: TYPE_CLOSE, 'origin': {}},\n                    no_add_request_id=True, force=True)\n            except NotConnected:\n                pass\n            except Exception:  # pylint: disable=broad-except\n                self.logger.exception(\"[%s:%s] Exception\",\n                                      self.fn or self.host, self.port)\n\n            self._open_channels.remove(destination_id)\n\n            self.handle_channel_disconnected()","287":"def remove_node(self, node):\n        \"\"\"\n        Remove the node from the graph, removes also all connections.\n\n        :param androguard.decompiler.dad.node.Node node: the node to remove\n        \"\"\"\n        preds = self.reverse_edges.get(node, [])\n        for pred in preds:\n            self.edges[pred].remove(node)\n\n        succs = self.edges.get(node, [])\n        for suc in succs:\n            self.reverse_edges[suc].remove(node)\n\n        exc_preds = self.reverse_catch_edges.pop(node, [])\n        for pred in exc_preds:\n            self.catch_edges[pred].remove(node)\n\n        exc_succs = self.catch_edges.pop(node, [])\n        for suc in exc_succs:\n            self.reverse_catch_edges[suc].remove(node)\n\n        self.nodes.remove(node)\n        if node in self.rpo:\n            self.rpo.remove(node)\n        del node","288":"def get_requested_third_party_permissions(self):\n        \"\"\"\n        Returns list of requested permissions not declared within AOSP project.\n\n        :rtype: list of strings\n        \"\"\"\n        third_party_permissions = []\n        all_permissions = self.get_permissions()\n        for perm in all_permissions:\n            if perm not in list(self.permission_module.keys()):\n                third_party_permissions.append(perm)\n        return third_party_permissions","289":"def get_certificates(self):\n        \"\"\"\n        Return a list of unique :class:`asn1crypto.x509.Certificate` which are found\n        in v1, v2 and v3 signing\n        Note that we simply extract all certificates regardless of the signer.\n        Therefore this is just a list of all certificates found in all signers.\n        \"\"\"\n        fps = []\n        certs = []\n        for x in self.get_certificates_v1() + self.get_certificates_v2() + self.get_certificates_v3():\n            if x.sha256 not in fps:\n                fps.append(x.sha256)\n                certs.append(x)\n        return certs","290":"def add_candidate_peer_endpoints(self, peer_endpoints):\n        \"\"\"Adds candidate endpoints to the list of endpoints to\n        attempt to peer with.\n\n        Args:\n            peer_endpoints ([str]): A list of public uri's which the\n                validator can attempt to peer with.\n        \"\"\"\n        with self._lock:\n            for endpoint in peer_endpoints:\n                if endpoint not in self._candidate_peer_endpoints:\n                    self._candidate_peer_endpoints.append(endpoint)","291":"def merge_dimensions(dimensions_list):\n    \"\"\"\n    Merges lists of fully or partially overlapping dimensions by\n    merging their values.\n\n    >>> from holoviews import Dimension\n    >>> dim_list = [[Dimension('A', values=[1, 2, 3]), Dimension('B')],\n    ...             [Dimension('A', values=[2, 3, 4])]]\n    >>> dimensions = merge_dimensions(dim_list)\n    >>> dimensions\n    [Dimension('A'), Dimension('B')]\n    >>> dimensions[0].values\n    [1, 2, 3, 4]\n    \"\"\"\n    dvalues = defaultdict(list)\n    dimensions = []\n    for dims in dimensions_list:\n        for d in dims:\n            dvalues[d.name].append(d.values)\n            if d not in dimensions:\n                dimensions.append(d)\n    dvalues = {k: list(unique_iterator(itertools.chain(*vals)))\n               for k, vals in dvalues.items()}\n    return [d(values=dvalues.get(d.name, [])) for d in dimensions]","292":"def unlink(self, func):\n        '''\n        Remove a callback function previously added with link()\n\n        Example:\n\n            base.unlink( callback )\n\n        '''\n        if func in self._syn_links:\n            self._syn_links.remove(func)","293":"def _set_unicode(self, value):\n        \"\"\"\n        Assign the primary unicode to the glyph.\n        This will be an integer or None.\n\n        Subclasses may override this method.\n        \"\"\"\n        values = list(self.unicodes)\n        if value in values:\n            values.remove(value)\n        values.insert(0, value)\n        self.unicodes = values","294":"def set_report_recipients(self, report, recipients):\n        \"\"\"Set recipients to the reports w\/o overwriting the old ones\n\n        :param reports: list of ARReports\n        :param recipients: list of name,email strings\n        \"\"\"\n        to_set = report.getRecipients()\n        for recipient in recipients:\n            if recipient not in to_set:\n                to_set.append(recipient)\n        report.setRecipients(to_set)","295":"def used(self, fieldname):\n        \"\"\"fieldname is used, remove from list of unused fields\"\"\"\n        if fieldname in self.unused:\n            self.unused.remove(fieldname)","296":"def getPartitions(self):\n        \"\"\"This functions returns the partitions from the analysis request's\n        analyses.\n\n        :returns: a list with the full partition objects\n        \"\"\"\n        partitions = []\n        for analysis in self.getAnalyses(full_objects=True):\n            if analysis.getSamplePartition() not in partitions:\n                partitions.append(analysis.getSamplePartition())\n        return partitions","297":"def tag_del(self, item, tag):\n        \"\"\"\n        Remove tag from the tags of item.\n        \n        :param item: item identifier\n        :type item: str\n        :param tag: tag name\n        :type tag: str\n        \"\"\"\n        tags = list(self.item(item, \"tags\"))\n        if tag in tags:\n            tags.remove(tag)\n            self.item(item, tags=tuple(tags))","298":"def remove_repeat_coordinates(x, y, z):\n    r\"\"\"Remove all x, y, and z where (x,y) is repeated and keep the first occurrence only.\n\n    Will not destroy original values.\n\n    Parameters\n    ----------\n    x: array_like\n        x coordinate\n    y: array_like\n        y coordinate\n    z: array_like\n        observation value\n\n    Returns\n    -------\n    x, y, z\n        List of coordinate observation pairs without\n        repeated coordinates.\n\n    \"\"\"\n    coords = []\n    variable = []\n\n    for (x_, y_, t_) in zip(x, y, z):\n        if (x_, y_) not in coords:\n            coords.append((x_, y_))\n            variable.append(t_)\n\n    coords = np.array(coords)\n\n    x_ = coords[:, 0]\n    y_ = coords[:, 1]\n\n    z_ = np.array(variable)\n\n    return x_, y_, z_","299":"def resample_nn_1d(a, centers):\n    \"\"\"Return one-dimensional nearest-neighbor indexes based on user-specified centers.\n\n    Parameters\n    ----------\n    a : array-like\n        1-dimensional array of numeric values from which to\n        extract indexes of nearest-neighbors\n    centers : array-like\n        1-dimensional array of numeric values representing a subset of values to approximate\n\n    Returns\n    -------\n        An array of indexes representing values closest to given array values\n\n    \"\"\"\n    ix = []\n    for center in centers:\n        index = (np.abs(a - center)).argmin()\n        if index not in ix:\n            ix.append(index)\n    return ix","300":"def _get_nameservers(domain):\n        \"\"\"\n        Looks for domain nameservers and returns the IPs of the nameservers as a list.\n        The list is empty, if no nameservers were found. Needed associated domain zone\n        name for lookup.\n        \"\"\"\n        nameservers = []\n        rdtypes_ns = ['SOA', 'NS']\n        rdtypes_ip = ['A', 'AAAA']\n        for rdtype_ns in rdtypes_ns:\n            for rdata_ns in Provider._dns_lookup(domain, rdtype_ns):\n                for rdtype_ip in rdtypes_ip:\n                    for rdata_ip in Provider._dns_lookup(rdata_ns.to_text().split(' ')[0],\n                                                         rdtype_ip):\n                        if rdata_ip.to_text() not in nameservers:\n                            nameservers.append(rdata_ip.to_text())\n        LOGGER.debug('DNS Lookup => %s IN NS %s', domain, ' '.join(nameservers))\n        return nameservers","301":"def _GetSignatureMatchParserNames(self, file_object):\n    \"\"\"Determines if a file-like object matches one of the known signatures.\n\n    Args:\n      file_object (file): file-like object whose contents will be checked\n          for known signatures.\n\n    Returns:\n      list[str]: parser names for which the contents of the file-like object\n          matches their known signatures.\n    \"\"\"\n    parser_names = []\n    scan_state = pysigscan.scan_state()\n    self._file_scanner.scan_file_object(scan_state, file_object)\n\n    for scan_result in iter(scan_state.scan_results):\n      format_specification = (\n          self._formats_with_signatures.GetSpecificationBySignature(\n              scan_result.identifier))\n\n      if format_specification.identifier not in parser_names:\n        parser_names.append(format_specification.identifier)\n\n    return parser_names","302":"def _extract(param_names: List[str],\n             params: Dict[str, mx.nd.NDArray],\n             ext_params: Dict[str, np.ndarray]) -> List[str]:\n    \"\"\"\n    Extract specific parameters from a given base.\n\n    :param param_names: Names of parameters to be extracted.\n    :param params: Mapping from parameter names to the actual NDArrays parameters.\n    :param ext_params: Extracted parameter dictionary.\n    :return: Remaining names of parameters to be extracted.\n    \"\"\"\n    remaining_param_names = list(param_names)\n    for name in param_names:\n        if name in params:\n            logger.info(\"\\tFound '%s': shape=%s\", name, str(params[name].shape))\n            ext_params[name] = params[name].asnumpy()\n            remaining_param_names.remove(name)\n    return remaining_param_names","303":"def backlink(node):\n    \"\"\"Given a CFG with outgoing links, create incoming links.\"\"\"\n    seen = set()\n    to_see = [node]\n    while to_see:\n      node = to_see.pop()\n      seen.add(node)\n      for succ in node.next:\n        succ.prev.add(node)\n        if succ not in seen:\n          to_see.append(succ)","304":"def submit(self, ctx: commands.Context):\n        \"\"\"\n        A context-manager that submits the current task to jishaku's task list\n        and removes it afterwards.\n\n        Arguments\n        ---------\n        ctx: commands.Context\n            A Context object used to derive information about this command task.\n        \"\"\"\n\n        self.task_count += 1\n        cmdtask = CommandTask(self.task_count, ctx, asyncio.Task.current_task())\n        self.tasks.append(cmdtask)\n\n        try:\n            yield cmdtask\n        finally:\n            if cmdtask in self.tasks:\n                self.tasks.remove(cmdtask)","305":"def RemoveConnectedPeer(self, peer):\n        \"\"\"\n        Remove a connected peer from the known peers list.\n\n        Args:\n            peer (NeoNode): instance.\n        \"\"\"\n        if peer in self.Peers:\n            self.Peers.remove(peer)","306":"def RemoveFromQueue(self, addr):\n        \"\"\"\n        Remove an address from the connection queue\n        Args:\n            addr:\n\n        Returns:\n\n        \"\"\"\n        if addr in self.connection_queue:\n            self.connection_queue.remove(addr)","307":"def append_fallback(model, fields):\n    \"\"\"\n    If translated field is encountered, add also all its fallback fields.\n    Returns tuple: (set_of_new_fields_to_use, set_of_translated_field_names)\n    \"\"\"\n    fields = set(fields)\n    trans = set()\n    from modeltranslation.translator import translator\n    opts = translator.get_options_for_model(model)\n    for key, _ in opts.fields.items():\n        if key in fields:\n            langs = resolution_order(get_language(), getattr(model, key).fallback_languages)\n            fields = fields.union(build_localized_fieldname(key, lang) for lang in langs)\n            fields.remove(key)\n            trans.add(key)\n    return fields, trans","308":"def unique(seq):\n    \"\"\"Return the unique values in a sequence.\n\n    Parameters\n    ----------\n    seq : sequence\n        Sequence with (possibly duplicate) elements.\n\n    Returns\n    -------\n    unique : list\n        Unique elements of ``seq``.\n        Order is guaranteed to be the same as in seq.\n\n    Examples\n    --------\n    Determine unique elements in list\n\n    >>> unique([1, 2, 3, 3])\n    [1, 2, 3]\n\n    >>> unique((1, 'str', 'str'))\n    [1, 'str']\n\n    The utility also works with unhashable types:\n\n    >>> unique((1, [1], [1]))\n    [1, [1]]\n    \"\"\"\n    # First check if all elements are hashable, if so O(n) can be done\n    try:\n        return list(OrderedDict.fromkeys(seq))\n    except TypeError:\n        # Unhashable, resort to O(n^2)\n        unique_values = []\n        for i in seq:\n            if i not in unique_values:\n                unique_values.append(i)\n        return unique_values","309":"def validate_sqs_policy(self, accounts):\n        \"\"\"Given a list of accounts, ensures that the SQS policy allows all the accounts to write to the queue\n\n        Args:\n            accounts (`list` of :obj:`Account`): List of accounts\n\n        Returns:\n            `None`\n        \"\"\"\n        sqs_queue_name = self.dbconfig.get('sqs_queue_name', self.ns)\n        sqs_queue_region = self.dbconfig.get('sqs_queue_region', self.ns)\n        sqs_account = AWSAccount.get(self.dbconfig.get('sqs_queue_account', self.ns))\n        session = get_aws_session(sqs_account)\n\n        sqs = session.client('sqs', region_name=sqs_queue_region)\n        sqs_queue_url = sqs.get_queue_url(QueueName=sqs_queue_name, QueueOwnerAWSAccountId=sqs_account.account_number)\n        sqs_attribs = sqs.get_queue_attributes(QueueUrl=sqs_queue_url['QueueUrl'], AttributeNames=['Policy'])\n\n        policy = json.loads(sqs_attribs['Attributes']['Policy'])\n\n        for account in accounts:\n            arn = 'arn:aws:sns:*:{}:{}'.format(account.account_number, sqs_queue_name)\n            if arn not in policy['Statement'][0]['Condition']['ForAnyValue:ArnEquals']['aws:SourceArn']:\n                self.log.warning('SQS policy is missing condition for ARN {}'.format(arn))\n                policy['Statement'][0]['Condition']['ForAnyValue:ArnEquals']['aws:SourceArn'].append(arn)\n\n        sqs.set_queue_attributes(QueueUrl=sqs_queue_url['QueueUrl'], Attributes={'Policy': json.dumps(policy)})","310":"def mime(self):\n        \"\"\"A list of mime types (:class:`mutagen.text`)\"\"\"\n\n        mimes = []\n        for Kind in type(self).__mro__:\n            for mime in getattr(Kind, '_mimes', []):\n                if mime not in mimes:\n                    mimes.append(mime)\n        return mimes","311":"def get_interface_addresses():\n    \"\"\"\n    Get addresses of available network interfaces.\n    See netifaces on pypi for details.\n\n    Returns a list of dicts\n    \"\"\"\n\n    addresses = []\n    ifaces = netifaces.interfaces()\n    for iface in ifaces:\n        addrs = netifaces.ifaddresses(iface)\n        families = addrs.keys()\n\n        # put IPv4 to the end so it lists as the main iface address\n        if netifaces.AF_INET in families:\n            families.remove(netifaces.AF_INET)\n            families.append(netifaces.AF_INET)\n\n        for family in families:\n            for addr in addrs[family]:\n                address = {\n                    'name': iface,\n                    'family': family,\n                    'ip': addr['addr'],\n                }\n                addresses.append(address)\n\n    return addresses","312":"def members_to_ask(self, name_id):\n        \"\"\"Find the member of the Virtual Organization that I haven't already\n        spoken too\n        \"\"\"\n\n        vo_members = self._affiliation_members()\n        for member in self.member:\n            if member not in vo_members:\n                vo_members.append(member)\n\n        # Remove the ones I have cached data from about this subject\n        vo_members = [m for m in vo_members if not self.sp.users.cache.active(\n            name_id, m)]\n        logger.info(\"VO members (not cached): %s\", vo_members)\n        return vo_members","313":"def setdefault (self, key, *args):\n        \"\"\"Remember key order if key not found.\"\"\"\n        if key not in self:\n            self._keys.append(key)\n        return super(ListDict, self).setdefault(key, *args)","314":"def pop (self, key):\n        \"\"\"Remove key from dict and return value.\"\"\"\n        if key in self._keys:\n            self._keys.remove(key)\n        super(ListDict, self).pop(key)","315":"def add_info (self, s):\n        \"\"\"\n        Add an info string.\n        \"\"\"\n        if s not in self.info:\n            self.info.append(s)","316":"def generateRandomSymbol(numColumns, sparseCols):\n  \"\"\"\n  Generates a random SDR with sparseCols number of active columns\n  \n  @param numColumns (int) number of columns in the temporal memory\n  @param sparseCols (int) number of sparse columns for desired SDR  \n  @return symbol (list) SDR\n  \"\"\"\n  symbol = list()\n  remainingCols = sparseCols\n  while remainingCols > 0:\n    col = random.randrange(numColumns)\n    if col not in symbol:\n      symbol.append(col)\n      remainingCols -= 1\n  return symbol","317":"def keys(self):\n        \"\"\"Returns a list containing all the keys available as subviews\n        of the current views. This enumerates all the keys in *all*\n        dictionaries matching the current view, in contrast to\n        ``view.get(dict).keys()``, which gets all the keys for the\n        *first* dict matching the view. If the object for this view in\n        any source is not a dict, then a ConfigTypeError is raised. The\n        keys are ordered according to how they appear in each source.\n        \"\"\"\n        keys = []\n\n        for dic, _ in self.resolve():\n            try:\n                cur_keys = dic.keys()\n            except AttributeError:\n                raise ConfigTypeError(\n                    u'{0} must be a dict, not {1}'.format(\n                        self.name, type(dic).__name__\n                    )\n                )\n\n            for key in cur_keys:\n                if key not in keys:\n                    keys.append(key)\n\n        return keys","318":"def add_transaction(self, transaction):\n        \"\"\"\n        Add a transaction.\n\n        Arguments:\n            transaction -- A transaction as an iterable object (eg. ['A', 'B']).\n        \"\"\"\n        for item in transaction:\n            if item not in self.__transaction_index_map:\n                self.__items.append(item)\n                self.__transaction_index_map[item] = set()\n            self.__transaction_index_map[item].add(self.__num_transaction)\n        self.__num_transaction += 1","319":"def addToTimeVary(self,*params):\n        '''\n        Adds any number of parameters to time_vary for this instance.\n\n        Parameters\n        ----------\n        params : string\n            Any number of strings naming attributes to be added to time_vary\n\n        Returns\n        -------\n        None\n        '''\n        for param in params:\n            if param not in self.time_vary:\n                self.time_vary.append(param)","320":"def addToTimeInv(self,*params):\n        '''\n        Adds any number of parameters to time_inv for this instance.\n\n        Parameters\n        ----------\n        params : string\n            Any number of strings naming attributes to be added to time_inv\n\n        Returns\n        -------\n        None\n        '''\n        for param in params:\n            if param not in self.time_inv:\n                self.time_inv.append(param)","321":"def delFromTimeVary(self,*params):\n        '''\n        Removes any number of parameters from time_vary for this instance.\n\n        Parameters\n        ----------\n        params : string\n            Any number of strings naming attributes to be removed from time_vary\n\n        Returns\n        -------\n        None\n        '''\n        for param in params:\n            if param in self.time_vary:\n                self.time_vary.remove(param)","322":"def delFromTimeInv(self,*params):\n        '''\n        Removes any number of parameters from time_inv for this instance.\n\n        Parameters\n        ----------\n        params : string\n            Any number of strings naming attributes to be removed from time_inv\n\n        Returns\n        -------\n        None\n        '''\n        for param in params:\n            if param in self.time_inv:\n                self.time_inv.remove(param)","323":"def updateDynamics(self):\n        '''\n        Calculates a new \"aggregate dynamic rule\" using the history of variables\n        named in track_vars, and distributes this rule to AgentTypes in agents.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        dynamics : instance\n            The new \"aggregate dynamic rule\" that agents believe in and act on.\n            Should have attributes named in dyn_vars.\n        '''\n        # Make a dictionary of inputs for the dynamics calculator\n        history_vars_string = ''\n        arg_names = list(getArgNames(self.calcDynamics))\n        if 'self' in arg_names:\n            arg_names.remove('self')\n        for name in arg_names:\n            history_vars_string += ' \\'' + name + '\\' : self.' + name + '_hist,'\n        update_dict = eval('{' + history_vars_string + '}')\n\n        # Calculate a new dynamic rule and distribute it to the agents in agent_list\n        dynamics = self.calcDynamics(**update_dict) # User-defined dynamics calculator\n        for var_name in self.dyn_vars:\n            this_obj = getattr(dynamics,var_name)\n            for this_type in self.agents:\n                setattr(this_type,var_name,this_obj)\n        return dynamics","324":"def populate_startup_nodes(self):\n        \"\"\"\n        Do something with all startup nodes and filters out any duplicates\n        \"\"\"\n        for item in self.startup_nodes:\n            self.set_node_name(item)\n\n        for n in self.nodes.values():\n            if n not in self.startup_nodes:\n                self.startup_nodes.append(n)\n\n        # freeze it so we can set() it\n        uniq = {frozenset(node.items()) for node in self.startup_nodes}\n        # then thaw it back out into a list of dicts\n        self.startup_nodes = [dict(node) for node in uniq]","325":"def _deduplicate(lst):\n    \"\"\"Auxiliary function to deduplicate lst.\"\"\"\n    out = []\n    for i in lst:\n        if i not in out:\n            out.append(i)\n    return out","326":"def keys(self):\n        \"\"\"Only expose some of the attributes when using as a dictionary\"\"\"\n        keys = Struct.keys(self)\n        for key in (\n                '_cloud_provider',\n                '_naming_policy',\n                '_setup_provider',\n                'known_hosts_file',\n                'repository',\n        ):\n            if key in keys:\n                keys.remove(key)\n        return keys","327":"def top_level(self):\n        \"\"\"\n        Output the top level cells from the GDSII data.\n\n        Top level cells are those that are not referenced by any other\n        cells.\n\n        Returns\n        -------\n        out : list\n            List of top level cells.\n        \"\"\"\n        top = list(self.cell_dict.values())\n        for cell in self.cell_dict.values():\n            for dependency in cell.get_dependencies():\n                if dependency in top:\n                    top.remove(dependency)\n        return top","328":"def _remove_duplicates(self, items):\n        \"\"\"\n        Remove duplicates, while keeping the order.\n        (Sometimes we have duplicates, because the there several matches of the\n        same grammar, each yielding similar completions.)\n        \"\"\"\n        result = []\n        for i in items:\n            if i not in result:\n                result.append(i)\n        return result","329":"def setup_html_filter(portal):\n    \"\"\"Setup HTML filtering for resultsinterpretations\n    \"\"\"\n    logger.info(\"*** Setup HTML Filter ***\")\n    # bypass the broken API from portal_transforms\n    adapter = IFilterSchema(portal)\n    style_whitelist = adapter.style_whitelist\n    for style in ALLOWED_STYLES:\n        logger.info(\"Allow style '{}'\".format(style))\n        if style not in style_whitelist:\n            style_whitelist.append(style)\n    adapter.style_whitelist = style_whitelist","330":"def add_and_display_buffer(self, buffer, redraw=True):\n        \"\"\"\n        add provided buffer to buffer list and display it\n\n        :param buffer:\n        :return:\n        \"\"\"\n        # FIXME: some buffers have arguments, do a proper comparison -- override __eq__\n        if buffer not in self.buffers:\n            logger.debug(\"adding new buffer {!r}\".format(buffer))\n            self.buffers.append(buffer)\n        self.display_buffer(buffer, redraw=redraw)","331":"def eulerian_tour_undirected(graph):\n    \"\"\"Eulerian tour on an undirected graph\n\n       :param graph: directed graph in listlist format, cannot be listdict\n       :assumes: graph is eulerian\n       :returns: eulerian cycle as a vertex list\n       :complexity: `O(|V|+|E|)`\n    \"\"\"\n    P = []\n    Q = [0]\n    R = []\n    succ = [0] * len(graph)\n    seen = [set() for _ in graph]\n    while Q:\n        node = Q.pop()\n        P.append(node)\n        while succ[node] < len(graph[node]):\n            neighbor = graph[node][succ[node]]\n            succ[node] += 1\n            if neighbor not in seen[node]:\n                seen[neighbor].add(node)\n                R.append(neighbor)\n                node = neighbor\n        while R:\n            Q.append(R.pop())\n    return P","332":"def listen(self, listen):\n        \"\"\"\n        Attach an :class:`txtorcon.interface.IStreamListener` to this stream.\n\n        See also :meth:`txtorcon.TorState.add_stream_listener` to\n        listen to all streams.\n\n        :param listen: something that knows\n            :class:`txtorcon.interface.IStreamListener`\n        \"\"\"\n\n        listener = IStreamListener(listen)\n        if listener not in self.listeners:\n            self.listeners.append(listener)","333":"def __del_running_bp_from_all_threads(self, bp):\n        \"Auxiliary method.\"\n        for (tid, bpset) in compat.iteritems(self.__runningBP):\n            if bp in bpset:\n                bpset.remove(bp)\n                self.system.get_thread(tid).clear_tf()","334":"def __cleanup_thread(self, event):\n        \"\"\"\n        Auxiliary method for L{_notify_exit_thread}\n        and L{_notify_exit_process}.\n        \"\"\"\n        tid = event.get_tid()\n\n        # Cleanup running breakpoints\n        try:\n            for bp in self.__runningBP[tid]:\n                self.__cleanup_breakpoint(event, bp)\n            del self.__runningBP[tid]\n        except KeyError:\n            pass\n\n        # Cleanup hardware breakpoints\n        try:\n            for bp in self.__hardwareBP[tid]:\n                self.__cleanup_breakpoint(event, bp)\n            del self.__hardwareBP[tid]\n        except KeyError:\n            pass\n\n        # Cleanup set of threads being traced\n        if tid in self.__tracing:\n            self.__tracing.remove(tid)","335":"def register_fetcher(self, ctx_fetcher):\n        \"\"\"\n        Register another context-specialized fetcher\n        :param Callable ctx_fetcher: A callable that will return the id or raise ExecutedOutsideContext if it was\n         executed outside its context\n        \"\"\"\n        if ctx_fetcher not in self.ctx_fetchers:\n            self.ctx_fetchers.append(ctx_fetcher)","336":"def parse_nds_env(env='NDSSERVER'):\n    \"\"\"Parse the NDSSERVER environment variable into a list of hosts\n\n    Parameters\n    ----------\n    env : `str`, optional\n        environment variable name to use for server order,\n        default ``'NDSSERVER'``. The contents of this variable should\n        be a comma-separated list of `host:port` strings, e.g.\n        ``'nds1.server.com:80,nds2.server.com:80'``\n\n    Returns\n    -------\n    hostiter : `list` of `tuple`\n        a list of (unique) ``(str, int)`` tuples for each host:port\n        pair\n    \"\"\"\n    hosts = []\n    for host in os.getenv(env).split(','):\n        try:\n            host, port = host.rsplit(':', 1)\n        except ValueError:\n            port = None\n        else:\n            port = int(port)\n        if (host, port) not in hosts:\n            hosts.append((host, port))\n    return hosts","337":"def register(self, type_, handler):\n        \"\"\"\u6ce8\u518c\u4e8b\u4ef6\u5904\u7406\u51fd\u6570\u76d1\u542c\"\"\"\n        # \u5c1d\u8bd5\u83b7\u53d6\u8be5\u4e8b\u4ef6\u7c7b\u578b\u5bf9\u5e94\u7684\u5904\u7406\u51fd\u6570\u5217\u8868\uff0c\u82e5\u65e0defaultDict\u4f1a\u81ea\u52a8\u521b\u5efa\u65b0\u7684list\n        handlerList = self.__handlers[type_]\n        \n        # \u82e5\u8981\u6ce8\u518c\u7684\u5904\u7406\u5668\u4e0d\u5728\u8be5\u4e8b\u4ef6\u7684\u5904\u7406\u5668\u5217\u8868\u4e2d\uff0c\u5219\u6ce8\u518c\u8be5\u4e8b\u4ef6\n        if handler not in handlerList:\n            handlerList.append(handler)","338":"def registerGeneralHandler(self, handler):\n        \"\"\"\u6ce8\u518c\u901a\u7528\u4e8b\u4ef6\u5904\u7406\u51fd\u6570\u76d1\u542c\"\"\"\n        if handler not in self.__generalHandlers:\n            self.__generalHandlers.append(handler)","339":"def unregisterGeneralHandler(self, handler):\n        \"\"\"\u6ce8\u9500\u901a\u7528\u4e8b\u4ef6\u5904\u7406\u51fd\u6570\u76d1\u542c\"\"\"\n        if handler in self.__generalHandlers:\n            self.__generalHandlers.remove(handler)","340":"def get_env_dirs(self):\n        \"\"\"Return list of directories in env_root.\"\"\"\n        repo_dirs = next(os.walk(self.env_root))[1]\n        if '.git' in repo_dirs:\n            repo_dirs.remove('.git')  # not relevant for any repo operations\n        return repo_dirs","341":"def distinct(keys):\n    \"\"\"\n    Return the distinct keys in order.\n    \"\"\"\n    known = set()\n    outlist = []\n    for key in keys:\n        if key not in known:\n            outlist.append(key)\n        known.add(key)\n    return outlist","342":"def add_site_states(self, site, states):\n        \"\"\"Create new states on an agent site if the state doesn't exist.\"\"\"\n        for state in states:\n            if state not in self.site_states[site]:\n                self.site_states[site].append(state)","343":"def add_activity_type(self, activity_type):\n        \"\"\"Adds an activity type to an Agent.\n\n        Parameters\n        ----------\n        activity_type : str\n            The type of activity to add such as 'activity', 'kinase',\n            'gtpbound'\n        \"\"\"\n        if activity_type not in self.activity_types:\n            self.activity_types.append(activity_type)","344":"def set_section_order(self, section_name_list):\n        \"\"\"Set the order of the sections, which are by default unorderd.\n\n        Any unlisted sections that exist will be placed at the end of the\n        document in no particular order.\n        \"\"\"\n        self.section_headings = section_name_list[:]\n        for section_name in self.sections.keys():\n            if section_name not in section_name_list:\n                self.section_headings.append(section_name)\n        return","345":"def _create_projects_file(project_name, data_source, items):\n    \"\"\" Create a projects file from the items origin data \"\"\"\n\n    repositories = []\n    for item in items:\n        if item['origin'] not in repositories:\n            repositories.append(item['origin'])\n    projects = {\n        project_name: {\n            data_source: repositories\n        }\n    }\n\n    projects_file, projects_file_path = tempfile.mkstemp(prefix='track_items_')\n\n    with open(projects_file_path, \"w\") as pfile:\n        json.dump(projects, pfile, indent=True)\n\n    return projects_file_path","346":"def unique (inlist):\n    \"\"\"\nReturns all unique items in the passed list.  If the a list-of-lists\nis passed, unique LISTS are found (i.e., items in the first dimension are\ncompared).\n\nUsage:   unique (inlist)\nReturns: the unique elements (or rows) in inlist\n\"\"\"\n    uniques = []\n    for item in inlist:\n        if item not in uniques:\n            uniques.append(item)\n    return uniques","347":"def remove_in_progress_check(self, check):\n        \"\"\"Remove check from check in progress\n\n        :param check: Check to remove\n        :type check: alignak.objects.check.Check\n        :return: None\n        \"\"\"\n        # The check is consumed, update the in_checking properties\n        if check in self.checks_in_progress:\n            self.checks_in_progress.remove(check)\n        self.update_in_checking()","348":"def update(self, prefixes):\n        \"\"\"Add a value to the list.\n\n        Arguments:\n            prefixes(list): A list to add the value\n        \"\"\"\n        if self.ip_prefix not in prefixes:\n            prefixes.append(self.ip_prefix)\n            self.log.info(\"announcing %s for %s\", self.ip_prefix, self.name)\n            return True\n\n        return False","349":"def get_tree(config=None):\n    ''' Get the tree for a given config\n\n    Parameters:\n        config (str):\n            The name of the tree config to load\n\n    Returns:\n        a Python Tree instance\n    '''\n    path = os.path.dirname(os.path.abspath(__file__))\n    pypath = os.path.realpath(os.path.join(path, '..', 'python'))\n    if pypath not in sys.path:\n        sys.path.append(pypath)\n    os.chdir(pypath)\n    from tree.tree import Tree\n    tree = Tree(config=config)\n    return tree","350":"def retarget(self, remove=[], add=[]):\n        \"\"\"Return a TargetSettings object for the same module but with some of the architectures\n        removed and others added.\n        \"\"\"\n\n        archs = self.arch_list().split('\/')\n\n        for r in remove:\n            if r in archs:\n                archs.remove(r)\n\n        archs.extend(add)\n\n        archstr = \"\/\".join(archs)\n        return self.family.find(archstr, self.module_name())","351":"def src_suffixes(self, env):\n        \"\"\"\n        Returns the list of source suffixes for all src_builders of this\n        Builder.\n\n        This is essentially a recursive descent of the src_builder \"tree.\"\n        (This value isn't cached because there may be changes in a\n        src_builder many levels deep that we can't see.)\n        \"\"\"\n        sdict = {}\n        suffixes = self.subst_src_suffixes(env)\n        for s in suffixes:\n            sdict[s] = 1\n        for builder in self.get_src_builders(env):\n            for s in builder.src_suffixes(env):\n                if s not in sdict:\n                    sdict[s] = 1\n                    suffixes.append(s)\n        return suffixes","352":"def _RegisterKeyFlagForModule(self, module_name, flag):\n    \"\"\"Specifies that a flag is a key flag for a module.\n\n    Args:\n      module_name: A string, the name of a Python module.\n      flag: A Flag object, a flag that is key to the module.\n    \"\"\"\n    key_flags_by_module = self.KeyFlagsByModuleDict()\n    # The list of key flags for the module named module_name.\n    key_flags = key_flags_by_module.setdefault(module_name, [])\n    # Add flag, but avoid duplicates.\n    if flag not in key_flags:\n      key_flags.append(flag)","353":"def add_to(self, parent, additions):\n        \"Modify parent to include all elements in additions\"\n        for x in additions:\n            if x not in parent:\n                parent.append(x)\n                self.changed()","354":"def extend_unique(seq, more):\n    \"\"\"Return a new sequence containing the items in `seq` plus any items in\n    `more` that aren't already in `seq`, preserving the order of both.\n    \"\"\"\n    seen = set(seq)\n    new = []\n    for item in more:\n        if item not in seen:\n            seen.add(item)\n            new.append(item)\n\n    return seq + type(seq)(new)","355":"def reject(lst, *values):\n    \"\"\"Removes the given values from the list\"\"\"\n    lst = List.from_maybe(lst)\n    values = frozenset(List.from_maybe_starargs(values))\n\n    ret = []\n    for item in lst:\n        if item not in values:\n            ret.append(item)\n    return List(ret, use_comma=lst.use_comma)","356":"async def get_metadata(self, target):\n        \"\"\"\n        Return user metadata information.\n        This is a blocking asynchronous method: it has to be called from a coroutine, as follows:\n\n            metadata = await self.get_metadata('#foo')\n        \"\"\"\n        if target not in self._pending['metadata']:\n            await self.rawmsg('METADATA', target, 'LIST')\n\n            self._metadata_queue.append(target)\n            self._metadata_info[target] = {}\n            self._pending['metadata'][target] = self.eventloop.create_future()\n\n        return self._pending['metadata'][target]","357":"def subscribe(self, coro):\n        \"\"\"\n        Subscribe to status updates from the Opentherm Gateway.\n        Can only be used after connect()\n        @coro is a coroutine which will be called with a single\n        argument (status) when a status change occurs.\n        Return True on success, False if not connected or already\n        subscribed.\n        \"\"\"\n        if coro not in self._notify:\n            self._notify.append(coro)\n            return True\n        return False","358":"def unsubscribe(self, coro):\n        \"\"\"\n        Unsubscribe from status updates from the Opentherm Gateway.\n        Can only be used after connect()\n        @coro is a coroutine which has been subscribed with subscribe()\n        earlier.\n        Return True on success, false if not connected or subscribed.\n        \"\"\"\n        if coro in self._notify:\n            self._notify.remove(coro)\n            return True\n        return False","359":"def directoryAdd(self, dir_key, key):\n    '''Adds directory entry `key` to directory at `dir_key`.\n\n    If the directory `dir_key` does not exist, it is created.\n    '''\n    key = str(key)\n\n    dir_items = self.get(dir_key) or []\n    if key not in dir_items:\n      dir_items.append(key)\n      self.put(dir_key, dir_items)","360":"def get_unique_fields(self):\n        \"\"\"List field names that are unique_together with `sort_order`.\"\"\"\n        for unique_together in self._meta.unique_together:\n            if 'sort_order' in unique_together:\n                unique_fields = list(unique_together)\n                unique_fields.remove('sort_order')\n                return ['%s_id' % f for f in unique_fields]\n        return []","361":"def remove_callback(self, name, fn, *args, **kwargs):\n        \"\"\"Remove a specific callback that was added.\n        \"\"\"\n        try:\n            tup = (fn, args, kwargs)\n            if tup in self.cb[name]:\n                self.cb[name].remove(tup)\n        except KeyError:\n            raise CallbackError(\"No callback category of '%s'\" % (\n                name))","362":"def _add_observer(self, signal, observer):\n        \"\"\"Associate an observer to a valid signal.\n\n        Parameters\n        ----------\n        signal : str\n            a valid signal.\n        observer : @func\n            an obervation function.\n\n        \"\"\"\n\n        if observer not in self._observers[signal]:\n            self._observers[signal].append(observer)","363":"def _remove_observer(self, signal, observer):\n        \"\"\"Remove an observer to a valid signal.\n\n        Parameters\n        ----------\n        signal : str\n            a valid signal.\n        observer : @func\n            an obervation function to be removed.\n\n        \"\"\"\n\n        if observer in self._observers[signal]:\n            self._observers[signal].remove(observer)","364":"def _build_matches(matches, uuids, no_filtered, fastmode=False):\n    \"\"\"Build a list with matching subsets\"\"\"\n\n    result = []\n\n    for m in matches:\n        mk = m[0].uuid if not fastmode else m[0]\n        subset = [uuids[mk]]\n\n        for id_ in m[1:]:\n            uk = id_.uuid if not fastmode else id_\n            u = uuids[uk]\n\n            if u not in subset:\n                subset.append(u)\n\n        result.append(subset)\n\n    result += no_filtered\n    result.sort(key=len, reverse=True)\n\n    sresult = []\n    for r in result:\n        r.sort(key=lambda id_: id_.uuid)\n        sresult.append(r)\n\n    return sresult","365":"def forget(self):\n        \"\"\"\n        Reset _observed events. Remove self from observers.\n        :return: Nothing\n        \"\"\"\n        self._observed_events = {}\n        if self in self._observers:\n            self._observers.remove(self)","366":"def append_note(self, note, root, scale=0):\n        \"\"\" Append a note to quality\n\n        :param str note: note to append on quality\n        :param str root: root note of chord\n        :param int scale: key scale\n        \"\"\"\n        root_val = note_to_val(root)\n        note_val = note_to_val(note) - root_val + scale * 12\n        if note_val not in self.components:\n            self.components.append(note_val)\n            self.components.sort()","367":"def get_non_compulsory_fields(layer_purpose, layer_subcategory=None):\n    \"\"\"Get non compulsory field based on layer_purpose and layer_subcategory.\n\n    Used for get field in InaSAFE Fields step in wizard.\n\n    :param layer_purpose: The layer purpose.\n    :type layer_purpose: str\n\n    :param layer_subcategory: Exposure or hazard value.\n    :type layer_subcategory: str\n\n    :returns: Compulsory fields\n    :rtype: list\n    \"\"\"\n    all_fields = get_fields(\n        layer_purpose, layer_subcategory, replace_null=False)\n    compulsory_field = get_compulsory_fields(\n        layer_purpose, layer_subcategory)\n    if compulsory_field in all_fields:\n        all_fields.remove(compulsory_field)\n    return all_fields","368":"def group_values(self, group_name):\n        \"\"\"Return all distinct group values for given group.\"\"\"\n        group_index = self.groups.index(group_name)\n        values = []\n        for key in self.data_keys:\n            if key[group_index] not in values:\n                values.append(key[group_index])\n        return values","369":"def _update_install_json(self, install_json):\n        \"\"\"Write install.json file.\n\n        Args:\n            install_json (dict): The contents of the install.json file.\n\n        Returns:\n            dict, bool: The contents of the install.json file and boolean value that is True if\n                an update was made.\n        \"\"\"\n        updated = False\n        # Update features\n        install_json.setdefault('features', [])\n        for feature in self.features:\n            if feature not in install_json.get('features'):\n                install_json['features'].append(feature)\n                updated = True\n                # update package data\n                self.package_data['updates'].append(\n                    {'action': 'Updated Feature:', 'output': feature}\n                )\n\n        return install_json, updated","370":"def add_callback(self, callback):\n        \"\"\"\n        Add a callback function to the listener.\n\n        The callback function will be called for each indication this listener\n        receives from any WBEM server.\n\n        If the callback function is already known to the listener, it will not\n        be added.\n\n        Parameters:\n\n          callback (:func:`~pywbem.callback_interface`):\n            Callable that is being called for each CIM indication that is\n            received while the listener threads are running.\n        \"\"\"\n        if callback not in self._callbacks:\n            self._callbacks.append(callback)","371":"def add_file(self, file_to_add):\n        \"\"\" add_file: Add to node's associated files\n            Args: file_to_add (File): file model to add to node\n            Returns: None\n        \"\"\"\n        from .files import File\n        assert isinstance(file_to_add, File), \"Files being added must be instances of a subclass of File class\"\n        file_to_add.node = self\n        if file_to_add not in self.files:\n            self.files.append(file_to_add)","372":"def _discovery(self):\n        \"\"\"\n        Find other servers asking nodes to given server\n        \"\"\"\n        data = self.cluster_nodes()\n        self.cluster_name = data[\"cluster_name\"]\n        for _, nodedata in list(data[\"nodes\"].items()):\n            server = nodedata['http_address'].replace(\"]\", \"\").replace(\"inet[\", \"http:\/\")\n            if server not in self.servers:\n                self.servers.append(server)\n        self._init_connection()\n        return self.servers","373":"def handle(self, *args, **options):  # NoQA\n        \"\"\"\n        Execute the command.\n\n        \"\"\"\n\n        # Load the settings\n        self.require_settings(args, options)\n\n        # Load your AWS credentials from ~\/.aws\/credentials\n        self.load_credentials()\n\n        try:\n            # Tail the available logs\n            all_logs = self.zappa.fetch_logs(self.lambda_name)\n            self.print_logs(all_logs)\n\n            # Keep polling, and print any new logs.\n            while True:\n                all_logs_again = self.zappa.fetch_logs(self.lambda_name)\n                new_logs = []\n                for log in all_logs_again:\n                    if log not in all_logs:\n                        new_logs.append(log)\n\n                self.print_logs(new_logs)\n                all_logs = all_logs + new_logs\n        except KeyboardInterrupt:\n            # Die gracefully\n            try:\n                sys.exit(0)\n            except SystemExit:\n                os._exit(0)\n\n        return","374":"def conflicting(self, match, predicate=None, index=None):\n        \"\"\"\n        Retrieves a list of ``Match`` objects that conflicts with given match.\n        :param match:\n        :type match:\n        :param predicate:\n        :type predicate:\n        :param index:\n        :type index:\n        :return:\n        :rtype:\n        \"\"\"\n        ret = _BaseMatches._base()\n\n        for i in range(*match.span):\n            for at_match in self.at_index(i):\n                if at_match not in ret:\n                    ret.append(at_match)\n\n        ret.remove(match)\n\n        return filter_index(ret, predicate, index)","375":"def at_span(self, span, predicate=None, index=None):\n        \"\"\"\n        Retrieves a list of matches from given (start, end) tuple.\n        \"\"\"\n        starting = self._index_dict[span[0]]\n        ending = self._index_dict[span[1] - 1]\n\n        merged = list(starting)\n        for marker in ending:\n            if marker not in merged:\n                merged.append(marker)\n\n        return filter_index(merged, predicate, index)","376":"def extend_safe(target, source):\n    \"\"\"\n    Extends source list to target list only if elements doesn't exists in target list.\n    :param target:\n    :type target: list\n    :param source:\n    :type source: list\n    \"\"\"\n    for elt in source:\n        if elt not in target:\n            target.append(elt)","377":"def remove_empty_ranks(self, tax_list):\n        '''\n        Removes empty rank prefixes\n        \n        Parameters\n        ----------\n        tax_list    : list\n            A list of taxonomic ranks.\n        Returns\n        -------\n        A list of taxonomic ranks with empty prefixes removed.\n        '''\n        new_tax_list = []\n        for rank in tax_list:\n            if rank not in self.meaningless_taxonomic_names:\n                new_tax_list.append(rank)\n        return new_tax_list","378":"def connect(self, func):\n        \"\"\"Arrange to call this function whenever something changes here.\n\n        The arguments will be this object, the key changed, and the value set.\n\n        \"\"\"\n        l = _alleged_receivers[id(self)]\n        if func not in l:\n            l.append(func)","379":"def all_downstreams(self, node):\n        \"\"\"Returns a list of all nodes ultimately downstream\n        of the given node in the dependency graph, in\n        topological order.\n\n        Args:\n             node (str): The node whose downstream nodes you want to find.\n\n        Returns:\n            list: A list of nodes that are downstream from the node.\n        \"\"\"\n        nodes = [node]\n        nodes_seen = set()\n        i = 0\n        while i < len(nodes):\n            downstreams = self.downstream(nodes[i])\n            for downstream_node in downstreams:\n                if downstream_node not in nodes_seen:\n                    nodes_seen.add(downstream_node)\n                    nodes.append(downstream_node)\n            i += 1\n        return [\n            node_ for node_ in self.topological_sort() if node_ in nodes_seen\n        ]","380":"def unique(seq):\n    \"\"\"Return the unique elements of a collection even if those elements are\n       unhashable and unsortable, like dicts and sets\"\"\"\n    cleaned = []\n    for each in seq:\n        if each not in cleaned:\n            cleaned.append(each)\n    return cleaned","381":"def register_key_flag_for_module(self, module_name, flag):\n    \"\"\"Specifies that a flag is a key flag for a module.\n\n    Args:\n      module_name: str, the name of a Python module.\n      flag: Flag, the Flag instance that is key to the module.\n    \"\"\"\n    key_flags_by_module = self.key_flags_by_module_dict()\n    # The list of key flags for the module named module_name.\n    key_flags = key_flags_by_module.setdefault(module_name, [])\n    # Add flag, but avoid duplicates.\n    if flag not in key_flags:\n      key_flags.append(flag)","382":"def remove_callback(self, callback):\n        \"\"\"Remove callback from the list of callbacks if it exists\"\"\"\n        if callback in self.callbacks:\n            self.callbacks.remove(callback)","383":"def _spawn(self, func: Callable, *args, **kwargs) -> gevent.Greenlet:\n        \"\"\" Spawn a sub-task and ensures an error on it crashes self\/main greenlet \"\"\"\n\n        def on_success(greenlet):\n            if greenlet in self.greenlets:\n                self.greenlets.remove(greenlet)\n\n        greenlet = gevent.spawn(func, *args, **kwargs)\n        greenlet.link_exception(self.on_error)\n        greenlet.link_value(on_success)\n        self.greenlets.append(greenlet)\n        return greenlet","384":"def update(self, task_name, result):\n        ''' Update the results file with new information.\n\n        Args:\n            task_name (str): Name of the currently running task. A previously unseen\n                ``task_name`` will create a new entry in both :attr:`tasks`\n                and :attr:`results`.\n            result: This will be appended to the list in :attr:`results` which\n                corresponds to the ``task_name`` in ``task_name``:attr:`tasks`.\n\n        '''\n        with open(self.filepath, 'rb') as f:\n            existing_results = pickle.load(f)\n        if task_name not in self.tasks:\n            self._add_task(task_name)\n            existing_results['tasks'].append(task_name)\n            existing_results['results'].append([])\n        task_name_idx = existing_results['tasks'].index(task_name)\n        results = existing_results['results'][task_name_idx]\n        results.append(result)\n        with open(self.filepath, 'wb') as f:\n            pickle.dump(existing_results, f)","385":"def delete_alias(self, alias_name):\n        \"\"\"Delete the alias.\"\"\"\n        for aliases in self.key_to_aliases.values():\n            if alias_name in aliases:\n                aliases.remove(alias_name)","386":"def escape_chars(text, chars):\n    \"\"\"Helper function to escape uncomfortable characters.\"\"\"\n    text = str(text)\n    chars = list(set(chars))\n    if '\\\\' in chars:\n        chars.remove('\\\\')\n        chars.insert(0, '\\\\')\n    for ch in chars:\n        text = text.replace(ch, '\\\\' + ch)\n    return text","387":"def _remove_bcbiovm_path():\n    \"\"\"Avoid referencing minimal bcbio_nextgen in bcbio_vm installation.\n    \"\"\"\n    cur_path = os.path.dirname(os.path.realpath(sys.executable))\n    paths = os.environ[\"PATH\"].split(\":\")\n    if cur_path in paths:\n        paths.remove(cur_path)\n        os.environ[\"PATH\"] = \":\".join(paths)","388":"def classifyplot_from_plotfiles(plot_files, out_csv, outtype=\"png\", title=None, size=None):\n    \"\"\"Create a plot from individual summary csv files with classification metrics.\n    \"\"\"\n    dfs = [pd.read_csv(x) for x in plot_files]\n    samples = []\n    for df in dfs:\n        for sample in df[\"sample\"].unique():\n            if sample not in samples:\n                samples.append(sample)\n    df = pd.concat(dfs)\n    df.to_csv(out_csv, index=False)\n    return classifyplot_from_valfile(out_csv, outtype, title, size, samples)","389":"def get_inheritors(cls):\n    \"\"\"Get a set of all classes that inherit from the given class.\"\"\"\n    subclasses = set()\n    work = [cls]\n    while work:\n        parent = work.pop()\n        for child in parent.__subclasses__():\n            if child not in subclasses:\n                subclasses.add(child)\n                work.append(child)\n    return subclasses","390":"def addparentstofks(rels, fks):\n    \"\"\"\n    Get a list of relations, between parents and sons and a dict of\n    clases named in dia, and modifies the fks to add the parent as fk to get\n    order on the output of classes and replaces the base class of the son, to\n    put the class parent name.\n    \"\"\"\n    for j in rels:\n        son = index(fks, j[1])\n        parent = index(fks, j[0])\n        fks[son][2] = fks[son][2].replace(\"models.Model\", parent)\n        if parent not in fks[son][0]:\n            fks[son][0].append(parent)","391":"def AddTableColumn(self, table, column):\n    \"\"\"Add column to table if it is not already there.\"\"\"\n    if column not in self._table_columns[table]:\n      self._table_columns[table].append(column)","392":"def AddTableColumns(self, table, columns):\n    \"\"\"Add columns to table if they are not already there.\n\n    Args:\n      table: table name as a string\n      columns: an iterable of column names\"\"\"\n    table_columns = self._table_columns.setdefault(table, [])\n    for attr in columns:\n      if attr not in table_columns:\n        table_columns.append(attr)","393":"def list_difference(left, right):\n    \"\"\"\n    Take the not-in-place difference of two lists (left - right), similar to sets but preserving order.\n    \"\"\"\n    blocked = set(right)\n    difference = []\n    for item in left:\n        if item not in blocked:\n            blocked.add(item)\n            difference.append(item)\n    return difference","394":"def difference(a: Iterable[Any], b: Container[Any]) -> List[Any]:\n    \"\"\" Return a list of items from `a` that are not in `b`.\n    \"\"\"\n    d = []\n    for item in a:\n        if item not in b:\n            d.append(item)\n    return d","395":"def release_tcp_port(self, port, project):\n        \"\"\"\n        Release a specific TCP port number\n\n        :param port: TCP port number\n        :param project: Project instance\n        \"\"\"\n\n        if port in self._used_tcp_ports:\n            self._used_tcp_ports.remove(port)\n            project.remove_tcp_port(port)\n            log.debug(\"TCP port {} has been released\".format(port))","396":"def release_udp_port(self, port, project):\n        \"\"\"\n        Release a specific UDP port number\n\n        :param port: UDP port number\n        :param project: Project instance\n        \"\"\"\n\n        if port in self._used_udp_ports:\n            self._used_udp_ports.remove(port)\n            project.remove_udp_port(port)\n            log.debug(\"UDP port {} has been released\".format(port))","397":"def list(self):\n        \"\"\"\n        List all VMs\n        \"\"\"\n\n        res = []\n\n        for compute in self._controller.computes.values():\n            if compute.id not in [\"local\", \"vm\"]:\n                res.append({\"vmname\": compute.name})\n        return res","398":"def release_dynamips_id(self, project_id, dynamips_id):\n        \"\"\"\n        A Dynamips id can be reused by another VM\n\n        :param project_id: UUID of the project\n        :param dynamips_id: Asked id\n        \"\"\"\n        self._dynamips_ids.setdefault(project_id, set())\n        if dynamips_id in self._dynamips_ids[project_id]:\n            self._dynamips_ids[project_id].remove(dynamips_id)","399":"def remove_tcp_port(self, port):\n        \"\"\"\n        Removes an associated TCP port number from this project.\n\n        :param port: TCP port number\n        \"\"\"\n\n        if port in self._used_tcp_ports:\n            self._used_tcp_ports.remove(port)","400":"def remove_udp_port(self, port):\n        \"\"\"\n        Removes an associated UDP port number from this project.\n\n        :param port: UDP port number\n        \"\"\"\n\n        if port in self._used_udp_ports:\n            self._used_udp_ports.remove(port)","401":"def remove_node(self, node):\n        \"\"\"\n        Removes a node from the project.\n        In theory this should be called by the node manager.\n\n        :param node: Node instance\n        \"\"\"\n\n        if node in self._nodes:\n            yield from node.delete()\n            self._nodes.remove(node)","402":"def remove_allocated_node_name(self, name):\n        \"\"\"\n        Removes an allocated node name\n\n        :param name: allocated node name\n        \"\"\"\n\n        if name in self._allocated_node_names:\n            self._allocated_node_names.remove(name)","403":"def add_role_to_user(self, user, role):\n        \"\"\"Adds a role to a user.\n\n        :param user: The user to manipulate\n        :param role: The role to add to the user\n        \"\"\"\n        user, role = self._prepare_role_modify_args(user, role)\n        if role not in user.roles:\n            user.roles.append(role)\n            self.put(user)\n            return True\n        return False","404":"def remove_role_from_user(self, user, role):\n        \"\"\"Removes a role from a user.\n\n        :param user: The user to manipulate\n        :param role: The role to remove from the user\n        \"\"\"\n        rv = False\n        user, role = self._prepare_role_modify_args(user, role)\n        if role in user.roles:\n            rv = True\n            user.roles.remove(role)\n            self.put(user)\n        return rv","405":"def get_unique_fields(fld_lists):\n    \"\"\"Get unique namedtuple fields, despite potential duplicates in lists of fields.\"\"\"\n    flds = []\n    fld_set = set([f for flst in fld_lists for f in flst])\n    fld_seen = set()\n    # Add unique fields to list of fields in order that they appear\n    for fld_list in fld_lists:\n        for fld in fld_list:\n            # Add fields if the field has not yet been seen\n            if fld not in fld_seen:\n                flds.append(fld)\n                fld_seen.add(fld)\n    assert len(flds) == len(fld_set)\n    return flds","406":"def _filter_repeating_items(download_list):\n        \"\"\" Because of data_filter some requests in download list might be the same. In order not to download them again\n        this method will reduce the list of requests. It will also return a mapping list which can be used to\n        reconstruct the previous list of download requests.\n\n        :param download_list: List of download requests\n        :type download_list: list(sentinelhub.DownloadRequest)\n        :return: reduced download list with unique requests and mapping list\n        :rtype: (list(sentinelhub.DownloadRequest), list(int))\n        \"\"\"\n        unique_requests_map = {}\n        mapping_list = []\n        unique_download_list = []\n        for download_request in download_list:\n            if download_request not in unique_requests_map:\n                unique_requests_map[download_request] = len(unique_download_list)\n                unique_download_list.append(download_request)\n            mapping_list.append(unique_requests_map[download_request])\n        return unique_download_list, mapping_list","407":"def deduplicate(list_object):\n    \"\"\"Rebuild `list_object` removing duplicated and keeping order\"\"\"\n    new = []\n    for item in list_object:\n        if item not in new:\n            new.append(item)\n    return new","408":"def addFilter(self, filterclass):\n        \"\"\"Add a filter class to the parser.\"\"\"\n        if filterclass not in self.filters:\n            self.filters.append(filterclass)","409":"def canonical_new_peer_list( self, peers_to_add ):\n        \"\"\"\n        Make a list of canonical new peers, using the\n        self.new_peers and the given peers to add\n\n        Return a shuffled list of canonicalized host:port\n        strings.\n        \"\"\"\n        new_peers = list(set(self.new_peers + peers_to_add))\n        random.shuffle( new_peers )\n        \n        # canonicalize\n        tmp = []\n        for peer in new_peers:\n            tmp.append( self.canonical_peer(peer) )\n\n        new_peers = tmp\n\n        # don't talk to myself\n        if self.my_hostport in new_peers:\n            new_peers.remove(self.my_hostport)\n\n        return new_peers","410":"def namedb_state_mutation_sanity_check( opcode, op_data ):\n    \"\"\"\n    Make sure all mutate fields for this operation are present.\n    Return True if so\n    Raise exception if not\n    \"\"\"\n\n    # sanity check:  each mutate field in the operation must be defined in op_data, even if it's null.\n    missing = []\n    mutate_fields = op_get_mutate_fields( opcode )\n    for field in mutate_fields:\n        if field not in op_data.keys():\n            missing.append( field )\n\n    assert len(missing) == 0, (\"BUG: operation '%s' is missing the following fields: %s\" % (opcode, \",\".join(missing)))\n    return True","411":"def namedb_is_history_snapshot( history_snapshot ):\n    \"\"\"\n    Given a dict, verify that it is a history snapshot.\n    It must have all consensus fields.\n    Return True if so.\n    Raise an exception of it doesn't.\n    \"\"\"\n    \n    # sanity check:  each mutate field in the operation must be defined in op_data, even if it's null.\n    missing = []\n\n    assert 'op' in history_snapshot.keys(), \"BUG: no op given\"\n\n    opcode = op_get_opcode_name( history_snapshot['op'] )\n    assert opcode is not None, \"BUG: unrecognized op '%s'\" % history_snapshot['op']\n\n    consensus_fields = op_get_consensus_fields( opcode )\n    for field in consensus_fields:\n        if field not in history_snapshot.keys():\n            missing.append( field )\n\n    assert len(missing) == 0, (\"BUG: operation '%s' is missing the following fields: %s\" % (opcode, \",\".join(missing)))\n    return True","412":"def remove_child(self, router):\n        '''remove a :class:`Router` from the :attr:`routes` list.'''\n        if router in self.routes:\n            self.routes.remove(router)\n            router._parent = None","413":"def discard(self, value):\n        \"\"\"\n        Remove the first occurrence of *value*.  If *value* is not a member,\n        does nothing.\n        \"\"\"\n        _set = self._set\n        if value in _set:\n            _set.remove(value)\n            self._list.discard(value)","414":"def unhide_tool(self, context_name, tool_name):\n        \"\"\"Unhide a tool so that it may be exposed in a suite.\n\n        Note that unhiding a tool doesn't guarantee it can be seen - a tool of\n        the same name from a different context may be overriding it.\n\n        Args:\n            context_name (str): Context containing the tool.\n            tool_name (str): Name of tool to unhide.\n        \"\"\"\n        data = self._context(context_name)\n        hidden_tools = data[\"hidden_tools\"]\n        if tool_name in hidden_tools:\n            hidden_tools.remove(tool_name)\n            self._flush_tools()","415":"def nonlocal_packages_path(self):\n        \"\"\"Returns package search paths with local path removed.\"\"\"\n        paths = self.packages_path[:]\n        if self.local_packages_path in paths:\n            paths.remove(self.local_packages_path)\n        return paths","416":"def clean_entity(self, ent):\n        \"\"\"\n        Strip out extra words that often get picked up by spaCy's NER.\n\n        To do: preserve info about what got stripped out to help with ES\/Geonames\n            resolution later.\n\n        Parameters\n        ---------\n        ent: a spaCy named entity Span\n\n        Returns\n        -------\n        new_ent: a spaCy Span, with extra words stripped out.\n\n        \"\"\"\n        dump_list = ['province', 'the', 'area', 'airport', 'district', 'square',\n                    'town', 'village', 'prison', \"river\", \"valley\", \"provincial\", \"prison\",\n                    \"region\", \"municipality\", \"state\", \"territory\", \"of\", \"in\",\n                    \"county\", \"central\"]\n                    # maybe have 'city'? Works differently in different countries\n                    # also, \"District of Columbia\". Might need to use cap\/no cap\n        keep_positions = []\n        for word in ent:\n            if word.text.lower() not in dump_list:\n                keep_positions.append(word.i)\n\n        keep_positions = np.asarray(keep_positions)\n        try:\n            new_ent = ent.doc[keep_positions.min():keep_positions.max() + 1]\n            # can't set directly\n            #new_ent.label_.__set__(ent.label_)\n        except ValueError:\n            new_ent = ent\n        return new_ent","417":"def add_operation(self, operation):\n        \"\"\"Add an :class:`Operation <stellar_base.operation.Operation>` to\n        this transaction.\n\n        This method will only add an operation if it is not already in the\n        transaction's list of operations, i.e. every operation in the\n        transaction should be unique.\n\n        :param Operation operation: The operation to add to this transaction.\n\n        \"\"\"\n        if operation not in self.operations:\n            self.operations.append(operation)","418":"def append_op(self, operation):\n        \"\"\"Append an :class:`Operation <stellar_base.operation.Operation>` to\n        the list of operations.\n\n        Add the operation specified if it doesn't already exist in the list of\n        operations of this :class:`Builder` instance.\n\n        :param operation: The operation to append to the list of operations.\n        :type operation: :class:`Operation`\n        :return: This builder instance.\n\n        \"\"\"\n        if operation not in self.ops:\n            self.ops.append(operation)\n        return self","419":"async def _kill_it_with_fire(self, container_id):\n        \"\"\"\n        Kill a container, with fire.\n        \"\"\"\n        if container_id in self._watching:\n            self._watching.remove(container_id)\n            self._container_had_error.add(container_id)\n            try:\n                await self._docker_interface.kill_container(container_id)\n            except:\n                pass","420":"def children_and_parameters(m:nn.Module):\n    \"Return the children of `m` and its direct parameters not registered in modules.\"\n    children = list(m.children())\n    children_p = sum([[id(p) for p in c.parameters()] for c in m.children()],[])\n    for p in m.parameters():\n        if id(p) not in children_p: children.append(ParameterModule(p))\n    return children","421":"def create(cls, tokens:Tokens, max_vocab:int, min_freq:int) -> 'Vocab':\n        \"Create a vocabulary from a set of `tokens`.\"\n        freq = Counter(p for o in tokens for p in o)\n        itos = [o for o,c in freq.most_common(max_vocab) if c >= min_freq]\n        for o in reversed(defaults.text_spec_tok):\n            if o in itos: itos.remove(o)\n            itos.insert(0, o)\n        return cls(itos)","422":"def remove_duplicates_from_list(array):\n    \"Preserves the order of elements in the list\"\n    output = []\n    unique = set()\n    for a in array:\n        if a not in unique:\n            unique.add(a)\n            output.append(a)\n    return output","423":"def get_graph_metadata(self, graph):\n        \"\"\"\n        Get the model metadata from a given onnx graph.\n        \"\"\"\n        _params = set()\n        for tensor_vals in graph.initializer:\n            _params.add(tensor_vals.name)\n\n        input_data = []\n        for graph_input in graph.input:\n            if graph_input.name not in _params:\n                shape = [val.dim_value for val in graph_input.type.tensor_type.shape.dim]\n                input_data.append((graph_input.name, tuple(shape)))\n\n        output_data = []\n        for graph_out in graph.output:\n            shape = [val.dim_value for val in graph_out.type.tensor_type.shape.dim]\n            output_data.append((graph_out.name, tuple(shape)))\n        metadata = {'input_tensor_data' : input_data,\n                    'output_tensor_data' : output_data\n                   }\n        return metadata","424":"def _bfs(root_node, process_node):\n    \"\"\"\n    Implementation of Breadth-first search (BFS) on caffe network DAG\n    :param root_node: root node of caffe network DAG\n    :param process_node: function to run on each node\n    \"\"\"\n\n    from collections import deque\n\n    seen_nodes = set()\n    next_nodes = deque()\n\n    seen_nodes.add(root_node)\n    next_nodes.append(root_node)\n\n    while next_nodes:\n        current_node = next_nodes.popleft()\n\n        # process current node\n        process_node(current_node)\n\n        for child_node in current_node.children:\n            if child_node not in seen_nodes:\n                seen_nodes.add(child_node)\n                next_nodes.append(child_node)","425":"def provides(self):\n        \"\"\"\n        A set of distribution names and versions provided by this distribution.\n        :return: A set of \"name (version)\" strings.\n        \"\"\"\n        plist = self.metadata.provides\n        s = '%s (%s)' % (self.name, self.version)\n        if s not in plist:\n            plist.append(s)\n        return plist","426":"def add_edge(self, x, y, label=None):\n        \"\"\"Add an edge from distribution *x* to distribution *y* with the given\n        *label*.\n\n        :type x: :class:`distutils2.database.InstalledDistribution` or\n                 :class:`distutils2.database.EggInfoDistribution`\n        :type y: :class:`distutils2.database.InstalledDistribution` or\n                 :class:`distutils2.database.EggInfoDistribution`\n        :type label: ``str`` or ``None``\n        \"\"\"\n        self.adjacency_list[x].append((y, label))\n        # multiple edges are allowed, so be careful\n        if x not in self.reverse_list[y]:\n            self.reverse_list[y].append(x)","427":"def list_domains(self):\n        \"\"\"Utility method to list all the domains in the jar.\"\"\"\n        domains = []\n        for cookie in iter(self):\n            if cookie.domain not in domains:\n                domains.append(cookie.domain)\n        return domains","428":"def list_paths(self):\n        \"\"\"Utility method to list all the paths in the jar.\"\"\"\n        paths = []\n        for cookie in iter(self):\n            if cookie.path not in paths:\n                paths.append(cookie.path)\n        return paths","429":"def revoke(self, role):\n        \"\"\"Remove a role from the entity.\n\n        :type role: str\n        :param role: The role to remove from the entity.\n        \"\"\"\n        if role in self.roles:\n            self.roles.remove(role)","430":"def ordered_union(l1, l2):\n  \"\"\"\n  Return the union of l1 and l2, with a deterministic ordering.\n  (Union of python sets does not necessarily have a consisten iteration\n  order)\n  :param l1: list of items\n  :param l2: list of items\n  :returns: list containing one copy of each item that is in l1 or in l2\n  \"\"\"\n  out = []\n  for e in l1 + l2:\n    if e not in out:\n      out.append(e)\n  return out","431":"def get_layer_params(self, layer_name):\n    \"\"\"\n    Provides access to the parameters of the given layer.\n    Works arounds the non-availability of graph collections in\n                eager mode.\n    :layer_name: name of the layer for which parameters are\n                required, must be one of the string in the\n                list layer_names\n    :return: list of parameters corresponding to the given\n                layer.\n    \"\"\"\n    assert layer_name in self.layer_names\n\n    out = []\n    layer = self.layers[layer_name]\n    layer_variables = layer.variables\n\n    # For each parameter in a layer.\n    for param in layer_variables:\n      if param not in out:\n        out.append(param)\n    return out","432":"def difference (b, a):\n    \"\"\" Returns the elements of B that are not in A.\n    \"\"\"\n    a = set(a)\n    result = []\n    for item in b:\n        if item not in a:\n            result.append(item)\n    return result","433":"def viable_source_types (target_type):\n    \"\"\" Helper rule, caches the result of '__viable_source_types_real'.\n    \"\"\"\n    assert isinstance(target_type, basestring)\n    if target_type not in __viable_source_types_cache:\n        __vst_cached_types.append(target_type)\n        __viable_source_types_cache [target_type] = __viable_source_types_real (target_type)\n    return __viable_source_types_cache [target_type]","434":"def viable_source_types_for_generator (generator):\n    \"\"\" Caches the result of 'viable_source_types_for_generator'.\n    \"\"\"\n    assert isinstance(generator, Generator)\n    if generator not in __viable_source_types_cache:\n        __vstg_cached_generators.append(generator)\n        __viable_source_types_cache[generator] = viable_source_types_for_generator_real (generator)\n\n    return __viable_source_types_cache[generator]","435":"def prune_feed_map(meta_graph, feed_map):\n  \"\"\"Function to prune the feedmap of nodes which no longer exist.\"\"\"\n  node_names = [x.name + \":0\" for x in meta_graph.graph_def.node]\n  keys_to_delete = []\n  for k, _ in feed_map.items():\n    if k not in node_names:\n      keys_to_delete.append(k)\n  for k in keys_to_delete:\n    del feed_map[k]","436":"def remove(self, tab_index):\r\n        \"\"\"Remove the widget at the corresponding tab_index.\"\"\"\r\n        _id = id(self.editor.tabs.widget(tab_index))\r\n        if _id in self.history:\r\n            self.history.remove(_id)","437":"def _add_pret_words(self, pret_embeddings):\n        \"\"\"Read pre-trained embedding file for extending vocabulary\n\n        Parameters\n        ----------\n        pret_embeddings : tuple\n            (embedding_name, source), used for gluonnlp.embedding.create(embedding_name, source)\n        \"\"\"\n        words_in_train_data = set(self._id2word)\n        pret_embeddings = gluonnlp.embedding.create(pret_embeddings[0], source=pret_embeddings[1])\n\n        for idx, token in enumerate(pret_embeddings.idx_to_token):\n            if token not in words_in_train_data:\n                self._id2word.append(token)","438":"def check_cache(self, template):\n        '''\n        Cache a file only once\n        '''\n        if template not in self.cached:\n            self.cache_file(template)\n            self.cached.append(template)","439":"def add(self, key):\n        \"\"\"\n        Add `key` as an item to this OrderedSet, then return its index.\n\n        If `key` is already in the OrderedSet, return the index it already\n        had.\n        \"\"\"\n        if key not in self.map:\n            self.map[key] = len(self.items)\n            self.items.append(key)\n        return self.map[key]","440":"def expand_classes_glob(classes, salt_data):\n    '''\n    Expand the list of `classes` to no longer include any globbing.\n\n    :param iterable(str) classes: Iterable of classes\n    :param dict salt_data: configuration data\n    :return: Expanded list of classes with resolved globbing\n    :rtype: list(str)\n    '''\n    all_classes = []\n    expanded_classes = []\n    saltclass_path = salt_data['path']\n\n    for _class in classes:\n        all_classes.extend(match_class_glob(_class, saltclass_path))\n\n    for _class in all_classes:\n        if _class not in expanded_classes:\n            expanded_classes.append(_class)\n\n    return expanded_classes"}}